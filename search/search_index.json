{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"skill-ml \u00b6 Open Skills Project - Machine Learning This is the library for the methods usable by the Open Skills API, including processing algorithms and utilities for computing our jobs and skills taxonomy. New to Skills-ML? Check out the Skills-ML Tour ! It will get you started with the concepts. You can also check out the notebook version of the tour which you can run on your own. Documentation \u00b6 Hosted on Github Pages Quick Start \u00b6 1. Virtualenv \u00b6 skills-ml depends on python3.6, so create a virtual environment using a python3.6 executable. virtualenv venv -p /usr/bin/python3.6 Activate your virtualenv source venv/bin/activate 2. Installation \u00b6 pip install skills-ml 3. Import skills_ml \u00b6 import skills_ml There are a couple of examples of specific uses of components to perform specific tasks in examples . Check out the descriptions of different algorithm types in algorithms/ and look at any individual directories that match what you'd like to do (e.g. skill extraction, job title normalization) skills-airflow is the open-source production system that uses skills-ml algorithms in an Airflow pipeline to generate open datasets Building the Documentation \u00b6 skills-ml uses a forked version of pydocmd, and a custom script to keep the pydocmd config file up to date. Here's how to keep the docs updated before you push: $ cd docs $ PYTHONPATH=\"../\" python update_docs.py # this will update docs/pydocmd.yml with the package/module structure and export the Skills-ML Tour notebook to the documentation directory $ pydocmd serve # will serve local documentation that you can check in your browser $ pydocmd gh-deploy # will update the gh-pages branch Structure \u00b6 algorithms/ - Core algorithmic module. Each submodule is meant to contain a different type of component, such as a job title normalizer or a skill tagger, with a common interface so different pipelines can try out different versions of the components. datasets/ - Wrappers for interfacing with different datasets, such as ONET, Urbanized Area. evaluation/ - Code for testing different components against each other. Contributors \u00b6 Kwame Porter Robinson - Github Eddie Lin - Github Tristan Crockett - Github Zoo Chai - Github License \u00b6 This project is licensed under the MIT License - see the LICENSE.md file for details.","title":"Home"},{"location":"#skill-ml","text":"Open Skills Project - Machine Learning This is the library for the methods usable by the Open Skills API, including processing algorithms and utilities for computing our jobs and skills taxonomy. New to Skills-ML? Check out the Skills-ML Tour ! It will get you started with the concepts. You can also check out the notebook version of the tour which you can run on your own.","title":"skill-ml"},{"location":"#documentation","text":"Hosted on Github Pages","title":"Documentation"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#1-virtualenv","text":"skills-ml depends on python3.6, so create a virtual environment using a python3.6 executable. virtualenv venv -p /usr/bin/python3.6 Activate your virtualenv source venv/bin/activate","title":"1. Virtualenv"},{"location":"#2-installation","text":"pip install skills-ml","title":"2. Installation"},{"location":"#3-import-skills_ml","text":"import skills_ml There are a couple of examples of specific uses of components to perform specific tasks in examples . Check out the descriptions of different algorithm types in algorithms/ and look at any individual directories that match what you'd like to do (e.g. skill extraction, job title normalization) skills-airflow is the open-source production system that uses skills-ml algorithms in an Airflow pipeline to generate open datasets","title":"3. Import skills_ml"},{"location":"#building-the-documentation","text":"skills-ml uses a forked version of pydocmd, and a custom script to keep the pydocmd config file up to date. Here's how to keep the docs updated before you push: $ cd docs $ PYTHONPATH=\"../\" python update_docs.py # this will update docs/pydocmd.yml with the package/module structure and export the Skills-ML Tour notebook to the documentation directory $ pydocmd serve # will serve local documentation that you can check in your browser $ pydocmd gh-deploy # will update the gh-pages branch","title":"Building the Documentation"},{"location":"#structure","text":"algorithms/ - Core algorithmic module. Each submodule is meant to contain a different type of component, such as a job title normalizer or a skill tagger, with a common interface so different pipelines can try out different versions of the components. datasets/ - Wrappers for interfacing with different datasets, such as ONET, Urbanized Area. evaluation/ - Code for testing different components against each other.","title":"Structure"},{"location":"#contributors","text":"Kwame Porter Robinson - Github Eddie Lin - Github Tristan Crockett - Github Zoo Chai - Github","title":"Contributors"},{"location":"#license","text":"This project is licensed under the MIT License - see the LICENSE.md file for details.","title":"License"},{"location":"Skills-ML Tour/","text":"%reload_ext autoreload %autoreload 2 import logging logging.basicConfig(level=logging.WARNING) Skills-ML Tour \u00b6 Skills-ML is an open source software library for applying NLP and ML to labor market data. It allows the user to perform tasks like skill extraction and occupation classification to collections of documents such as job postings, profiles, and course descriptions. Competency \u00b6 A competency is any expertise or talent that is useful for a job. Developed capacities (e.g. active listening), proficiency with tools or technology (e.g. lancets, Microsoft Word), innate abilities (e.g. originality), and academic knowledge (e.g. medicine) are all considered competencies. from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='dino_riding', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) Competency Relationships \u00b6 Competencies are often related to each other. Defining parent-child relationships is a standard building block of existing competency frameworks like ONET and ESCO. A parent-child relationship generally implies that the child is a \"type of\" the parent. from skills_ml.ontologies import Competency from skills_ml.ontologies.viz import display_nodes dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) train_surfing = Competency( identifier='12346', name='Train Surfing', description='Standing on the train while it goes' ) time_travel = Competency( identifier='23456', name='Time Travel', description='Traveling Through Time' ) advanced_science = Competency( identifier='2345', name='Advanced Science', ) extreme_transportation = Competency( identifier='123', name='Extreme Transportation', description='Comically dangerous forms of transportation' ) time_travel.add_parent(advanced_science) dinosaur_riding.add_parent(extreme_transportation) train_surfing.add_parent(extreme_transportation) display_nodes([dinosaur_riding, train_surfing, extreme_transportation, time_travel, advanced_science]) Occupation \u00b6 An occupation is a job or profession that a person can hold. Similar to competencies, these are also often defined hierarchically. from skills_ml.ontologies import Occupation extreme_postal_workers = Occupation(identifier='999', name='Extreme Postal Workers') dino_postal_worker = Occupation(identifier='9998', name='Deliverer of Items to the Past') train_yard_postal_worker = Occupation(identifier='9999', name='Deliverer of Items to Train Yards') dino_postal_worker.add_parent(extreme_postal_workers) train_yard_postal_worker.add_parent(extreme_postal_workers) display_nodes([extreme_postal_workers, dino_postal_worker, train_yard_postal_worker]) CompetencyOntology \u00b6 A CompetencyOntology is a model of the labor market, or some subset thereof, consisting of a collection of competencies, a collection of occupations, and all of the relationships between them. from skills_ml.ontologies import CompetencyOntology from skills_ml.ontologies.viz import display_ontology ontology = CompetencyOntology() ontology.add_competency(dinosaur_riding) ontology.add_competency(train_surfing) ontology.add_competency(extreme_transportation) ontology.add_competency(time_travel) ontology.add_competency(advanced_science) ontology.add_occupation(dino_postal_worker) ontology.add_occupation(train_yard_postal_worker) ontology.add_occupation(extreme_postal_workers) ontology.add_edge(occupation=dino_postal_worker, competency=dinosaur_riding) ontology.add_edge(occupation=dino_postal_worker, competency=time_travel) ontology.add_edge(occupation=train_yard_postal_worker, competency=train_surfing) display_ontology(ontology) Prebuilt Ontologies \u00b6 To move on we'll want to level up to a full ontology. The example we'll use is O*NET, built from survey data and maintained by the US Department of Labor. A CompetencyOntology subclass that downloads the source files from the O*NET web site is included in Skills-ML. from skills_ml.ontologies.onet import Onet onet = Onet() onet.print_summary_stats() Ontology summary statistics for onet Num competencies: 32030 Num occupations: 1133 Num competency-occupation edges: 107305 Median occupations per competency: 1 Median competencies per occupation: 89 Mean occupations per competency: 3.350245090386837 Mean competencies per occupation: 94.70873786407768 list(onet.competencies)[0:5] [Competency(identifier=41104007-Water sampling pumps, name=Water sampling pumps, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43232502-Distance learning software, name=Distance learning software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=46181509-Chemical-resistant suits, name=Chemical-resistant suits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43231603-Avalara AvaTax ST, name=Avalara AvaTax ST, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232603-Mainsaver Asset Management, name=Mainsaver Asset Management, categories=['Technology', 'O*NET T2'], {})] Filtering \u00b6 ~34000 competencies and ~1100 occupations is a lot. Let's explore the filtering functionality of the CompetencyOntology to zoom in on a more specific slice. filter_by filters using edges: the filtering function it expects takes in an edge (between a Competency and Occupation) and returns whether or not it should be in the result. The result takes the form of a new CompetencyOntology, so you can interact with it in the same way as you would the source ontology. nurse_practitioners = onet.filter_by(lambda edge: 'Nurse Practitioners' in edge.occupation.name) nurse_practitioners.competencies {Competency(identifier=1.A.1.a.1, name=Oral Comprehension, categories=['Abilities'], {'competencyText': 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'}), Competency(identifier=1.A.1.a.2, name=Written Comprehension, categories=['Abilities'], {'competencyText': 'The ability to read and understand information and ideas presented in writing.'}), Competency(identifier=1.A.1.a.3, name=Oral Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in speaking so others will understand.'}), Competency(identifier=1.A.1.a.4, name=Written Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in writing so others will understand.'}), Competency(identifier=1.A.1.b.1, name=Fluency of Ideas, categories=['Abilities'], {'competencyText': 'The ability to come up with a number of ideas about a topic (the number of ideas is important, not their quality, correctness, or creativity).'}), Competency(identifier=1.A.1.b.2, name=Originality, categories=['Abilities'], {'competencyText': 'The ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.'}), Competency(identifier=1.A.1.b.3, name=Problem Sensitivity, categories=['Abilities'], {'competencyText': 'The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.'}), Competency(identifier=1.A.1.b.4, name=Deductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to apply general rules to specific problems to produce answers that make sense.'}), Competency(identifier=1.A.1.b.5, name=Inductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events).'}), Competency(identifier=1.A.1.b.6, name=Information Ordering, categories=['Abilities'], {'competencyText': 'The ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations).'}), Competency(identifier=1.A.1.b.7, name=Category Flexibility, categories=['Abilities'], {'competencyText': 'The ability to generate or use different sets of rules for combining or grouping things in different ways.'}), Competency(identifier=1.A.1.c.1, name=Mathematical Reasoning, categories=['Abilities'], {'competencyText': 'The ability to choose the right mathematical methods or formulas to solve a problem.'}), Competency(identifier=1.A.1.d.1, name=Memorization, categories=['Abilities'], {'competencyText': 'The ability to remember information such as words, numbers, pictures, and procedures.'}), Competency(identifier=1.A.1.e.1, name=Speed of Closure, categories=['Abilities'], {'competencyText': 'The ability to quickly make sense of, combine, and organize information into meaningful patterns.'}), Competency(identifier=1.A.1.e.2, name=Flexibility of Closure, categories=['Abilities'], {'competencyText': 'The ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material.'}), Competency(identifier=1.A.1.e.3, name=Perceptual Speed, categories=['Abilities'], {'competencyText': 'The ability to quickly and accurately compare similarities and differences among sets of letters, numbers, objects, pictures, or patterns. The things to be compared may be presented at the same time or one after the other. This ability also includes comparing a presented object with a remembered object.'}), Competency(identifier=1.A.2.a.1, name=Arm-Hand Steadiness, categories=['Abilities'], {'competencyText': 'The ability to keep your hand and arm steady while moving your arm or while holding your arm and hand in one position.'}), Competency(identifier=1.A.2.a.3, name=Finger Dexterity, categories=['Abilities'], {'competencyText': 'The ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.'}), Competency(identifier=1.A.4.a.1, name=Near Vision, categories=['Abilities'], {'competencyText': 'The ability to see details at close range (within a few feet of the observer).'}), Competency(identifier=1.A.4.b.4, name=Speech Recognition, categories=['Abilities'], {'competencyText': 'The ability to identify and understand the speech of another person.'}), Competency(identifier=1.A.4.b.5, name=Speech Clarity, categories=['Abilities'], {'competencyText': 'The ability to speak clearly so others can understand you.'}), Competency(identifier=2.A.1.a, name=Reading Comprehension, categories=['Skills'], {'competencyText': 'Understanding written sentences and paragraphs in work related documents.'}), Competency(identifier=2.A.1.b, name=Active Listening, categories=['Skills'], {'competencyText': 'Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.'}), Competency(identifier=2.A.1.c, name=Writing, categories=['Skills'], {'competencyText': 'Communicating effectively in writing as appropriate for the needs of the audience.'}), Competency(identifier=2.A.1.d, name=Speaking, categories=['Skills'], {'competencyText': 'Talking to others to convey information effectively.'}), Competency(identifier=2.A.1.f, name=Science, categories=['Skills'], {'competencyText': 'Using scientific rules and methods to solve problems.'}), Competency(identifier=2.A.2.a, name=Critical Thinking, categories=['Skills'], {'competencyText': 'Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.'}), Competency(identifier=2.A.2.b, name=Active Learning, categories=['Skills'], {'competencyText': 'Understanding the implications of new information for both current and future problem-solving and decision-making.'}), Competency(identifier=2.A.2.c, name=Learning Strategies, categories=['Skills'], {'competencyText': 'Selecting and using training/instructional methods and procedures appropriate for the situation when learning or teaching new things.'}), Competency(identifier=2.A.2.d, name=Monitoring, categories=['Skills'], {'competencyText': 'Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.'}), Competency(identifier=2.B.1.a, name=Social Perceptiveness, categories=['Skills'], {'competencyText': \"Being aware of others' reactions and understanding why they react as they do.\"}), Competency(identifier=2.B.1.b, name=Coordination, categories=['Skills'], {'competencyText': \"Adjusting actions in relation to others' actions.\"}), Competency(identifier=2.B.1.c, name=Persuasion, categories=['Skills'], {'competencyText': 'Persuading others to change their minds or behavior.'}), Competency(identifier=2.B.1.d, name=Negotiation, categories=['Skills'], {'competencyText': 'Bringing others together and trying to reconcile differences.'}), Competency(identifier=2.B.1.e, name=Instructing, categories=['Skills'], {'competencyText': 'Teaching others how to do something.'}), Competency(identifier=2.B.1.f, name=Service Orientation, categories=['Skills'], {'competencyText': 'Actively looking for ways to help people.'}), Competency(identifier=2.B.2.i, name=Complex Problem Solving, categories=['Skills'], {'competencyText': 'Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.'}), Competency(identifier=2.B.3.a, name=Operations Analysis, categories=['Skills'], {'competencyText': 'Analyzing needs and product requirements to create a design.'}), Competency(identifier=2.B.4.e, name=Judgment and Decision Making, categories=['Skills'], {'competencyText': 'Considering the relative costs and benefits of potential actions to choose the most appropriate one.'}), Competency(identifier=2.B.4.g, name=Systems Analysis, categories=['Skills'], {'competencyText': 'Determining how a system should work and how changes in conditions, operations, and the environment will affect outcomes.'}), Competency(identifier=2.B.4.h, name=Systems Evaluation, categories=['Skills'], {'competencyText': 'Identifying measures or indicators of system performance and the actions needed to improve or correct performance, relative to the goals of the system.'}), Competency(identifier=2.B.5.a, name=Time Management, categories=['Skills'], {'competencyText': \"Managing one's own time and the time of others.\"}), Competency(identifier=2.B.5.d, name=Management of Personnel Resources, categories=['Skills'], {'competencyText': 'Motivating, developing, and directing people as they work, identifying the best people for the job.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=41103901-Microhematocrit centrifuges, name=Microhematocrit centrifuges, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104102-Lancets, name=Lancets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104104-Tourniquets, name=Tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104107-Evacuated blood collection tubes, name=Evacuated blood collection tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104118-Specimen collection containers, name=Specimen collection containers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104403-Tissue culture incubators, name=Tissue culture incubators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41111709-Binocular light compound microscopes, name=Binocular light compound microscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41115807-Hemoglobin analyzers, name=Hemoglobin analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116138-Urinalysis test strips, name=Urinalysis test strips, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116201-Glucometers, name=Glucometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131606-Protective face shields, name=Protective face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131612-Protective gowns, name=Protective gowns, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42132203-Medical examination protective gloves, name=Medical examination protective gloves, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42141807-Transcutaneous electric nerve stimulation TENS equipment, name=Transcutaneous electric nerve stimulation TENS equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Nasal suctioning equipment, name=Nasal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Oral suctioning equipment, name=Oral suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Tracheal suctioning equipment, name=Tracheal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142509-Epidural catheters, name=Epidural catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142532-Pericardiocentesis kits, name=Pericardiocentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142537-Thoracentesis kits, name=Thoracentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142609-Hypodermic syringes, name=Hypodermic syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142616-Blood drawing syringes, name=Blood drawing syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142715-Urinary catheters, name=Urinary catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42144102-Chest tubes, name=Chest tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171608-Head immobilization devices, name=Head immobilization devices, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171613-Spinal immobilization equipment, name=Spinal immobilization equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172101-Automated external defibrillators AED, name=Automated external defibrillators AED, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172102-Cardiopulmonary resuscitation CPR face shields, name=Cardiopulmonary resuscitation CPR face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Electronic blood pressure monitors, name=Electronic blood pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Pediatric blood pressure cuffs, name=Pediatric blood pressure cuffs, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Electrocardiography EKG machines, name=Electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Portable electrocardiography EKG machines, name=Portable electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181713-Holter monitors, name=Holter monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181801-Pulse oximeters, name=Pulse oximeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181902-Intracranial pressure monitors, name=Intracranial pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Cardiac monitors, name=Cardiac monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Hemodynamic monitors, name=Hemodynamic monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Ophthalmoscopes, name=Ophthalmoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Otoscopes, name=Otoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182013-Vaginal exam specula, name=Vaginal exam specula, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182103-Mechanical stethoscopes, name=Mechanical stethoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182201-Digital medical thermometers, name=Digital medical thermometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182302-Reflex hammers, name=Reflex hammers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182412-Diagnostic tuning forks, name=Diagnostic tuning forks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182416-Tympanometers, name=Tympanometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182805-Medical scales, name=Medical scales, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Snellen eye charts, name=Snellen eye charts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Visual acuity testing cards, name=Visual acuity testing cards, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42192401-Crash carts, name=Crash carts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42201701-Doppler ultrasound equipment, name=Doppler ultrasound equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Angiocaths, name=Angiocaths, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Pulmonary artery catheters, name=Pulmonary artery catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Pacemaker analyzers, name=Pacemaker analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Transcutaneous pacemakers, name=Transcutaneous pacemakers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42211502-Crutches, name=Crutches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221501-Arterial line catheters, name=Arterial line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221503-Multiple lumen central line catheters, name=Multiple lumen central line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221506-Umbilical catheters, name=Umbilical catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV administration sets, name=Intravenous IV administration sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV cutdown trays, name=Intravenous IV cutdown trays, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42222001-Intravenous IV infusion pumps, name=Intravenous IV infusion pumps, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42231701-Nasogastric tubes, name=Nasogastric tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241507-Orthopedic splinting equipment, name=Orthopedic splinting equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241705-Lower extremity braces, name=Lower extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241707-Walking braces, name=Walking braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241801-Upper extremity braces, name=Upper extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241802-Back braces, name=Back braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241803-Neck braces, name=Neck braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42242103-Halo traction equipment, name=Halo traction equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271501-Apnea monitors, name=Apnea monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271502-Arterial blood gas monitoring equipment, name=Arterial blood gas monitoring equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271602-Incentive spirometers, name=Incentive spirometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271604-Peak flowmeters, name=Peak flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271702-Oxygen concentrators, name=Oxygen concentrators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271707-Oxygen flowmeters, name=Oxygen flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271708-Oxygen delivery masks, name=Oxygen delivery masks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271710-Nasal catheters, name=Nasal catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271802-Handheld nebulizers, name=Handheld nebulizers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271903-Endotracheal ET tubes, name=Endotracheal ET tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271910-Tracheotomy sets, name=Tracheotomy sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272001-Fiberoptic laryngoscopes, name=Fiberoptic laryngoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272017-Intubation sets, name=Intubation sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272201-Mechanical intermittent positive pressure ventilators, name=Mechanical intermittent positive pressure ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272203-Bilevel positive airway pressure BiPAP ventilators, name=Bilevel positive airway pressure BiPAP ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291613-Surgical scalpels, name=Surgical scalpels, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291614-Straight surgical scissors, name=Straight surgical scissors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291704-Biopsy punches, name=Biopsy punches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291802-Mosquito hemostats, name=Mosquito hemostats, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294702-Intra-aortic balloon pumps IABP, name=Intra-aortic balloon pumps IABP, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294717-Ventricular assist devices VAD, name=Ventricular assist devices VAD, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Fiberoptic endoscopes, name=Fiberoptic endoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Flexible sigmoidoscopes, name=Flexible sigmoidoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295104-Electrosurgical cauterization machines, name=Electrosurgical cauterization machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Argon lasers, name=Argon lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Carbon dioxide CO2 lasers, name=Carbon dioxide CO2 lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Pulsed dye lasers, name=Pulsed dye lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295122-Pneumatic tourniquets, name=Pneumatic tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312008-Surgical staple removers, name=Surgical staple removers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312010-Skin staplers, name=Skin staplers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312202-Suturing kits, name=Suturing kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43191507-Multi-line telephone systems, name=Multi-line telephone systems, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211503-Laptop computers, name=Laptop computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211504-Personal digital assistants PDA, name=Personal digital assistants PDA, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211508-Personal computers, name=Personal computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211509-Tablet computers, name=Tablet computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43231507-Microsoft SharePoint, name=Microsoft SharePoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43231513-Microsoft Office, name=Microsoft Office, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232104-Microsoft Word, name=Microsoft Word, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232106-Microsoft PowerPoint, name=Microsoft PowerPoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232110-Microsoft Excel, name=Microsoft Excel, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232306-Microsoft Access, name=Microsoft Access, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Allscripts Professional EHR, name=Allscripts Professional EHR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Amkai AmkaiCharts, name=Amkai AmkaiCharts, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Bizmatics PrognoCIS EMR, name=Bizmatics PrognoCIS EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Cerner Millennium, name=Cerner Millennium, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-GE Healthcare Centricity EMR, name=GE Healthcare Centricity EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Healthcare common procedure coding system HCPCS, name=Healthcare common procedure coding system HCPCS, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MEDITECH software, name=MEDITECH software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical condition coding software, name=Medical condition coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical procedure coding software, name=Medical procedure coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medscribbler Enterprise, name=Medscribbler Enterprise, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MicroFour PracticeStudio.NET EMR, name=MicroFour PracticeStudio.NET EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-NextGen Healthcare Information Systems EMR, name=NextGen Healthcare Information Systems EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-PCC Pediatric Partner, name=PCC Pediatric Partner, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SOAPware EMR, name=SOAPware EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-StatCom Patient Flow Logistics Enterprise Suite, name=StatCom Patient Flow Logistics Enterprise Suite, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SynaMed EMR, name=SynaMed EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Texas Medical Software SpringCharts EMR, name=Texas Medical Software SpringCharts EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-e-MDs software, name=e-MDs software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-eClinicalWorks, name=eClinicalWorks, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Microsoft Internet Explorer, name=Microsoft Internet Explorer, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Web browser software, name=Web browser software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43233501-Microsoft Outlook, name=Microsoft Outlook, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=46181804-Safety goggles, name=Safety goggles, categories=['Tools', 'O*NET T2'], {})} That's a big list of competencies. Each competency has a list of categories, so let's get all of the different possible categories set(cat for competency in nurse_practitioners.competencies for cat in competency.categories) {'Abilities', 'Knowledge', 'O*NET T2', 'Skills', 'Technology', 'Tools'} So we can ask questions like: what type of Knowledge do nurse practitioners need? [competency for competency in nurse_practitioners.competencies if 'Knowledge' in competency.categories] [Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'})] There are many other questions we can ask of just an ontology, but the real value will come from combining the knowledge contained in the ontology with larger unstructured datasets. In this next section we will explore how Skills-ML helps the user deal with such datasets as job postings, profiles, or course descriptions. Spirit of Skills-ML \u00b6 Dataflow Programming : Skills-ML 's design philosophy builds on dataflow programming or so called data streaming to process very large datasets (larger than RAM; potentially infinite). One-pass algorithm : Data points are processed one at a time. Lazy evaluation : an evaluation strategy which delays the evaluation of an expression until its value is needed. In Skills-ML , most of the classes and functions here incorporates the concept of Iterable or Generator . We build the expression first and evaluate later. Creating Dataset \u00b6 Before we do anything with the context, we need dataset. Skills-ML makes use of schema.org\u2019s JobPosting standard. As it has been in use for a long time, some open sources are already using this standard, which is easy to import. Other job posting data sources are converted into the schema.org Schema and all work on job postings is done using this standard schema. In Skills-ML , job_postings module has all the functionalities to create the data we need for later usage. Common Schema \u00b6 We have an useful function to help create the data generator from s3. JobPostingCollectionFromS3 : Stream job posting from s3. JobPostingCollectionSample : Stream a finite number of job postings stored within the library. However, we are not restrcted to just JobPosting data. One can easily create whatever data generator such as ProfileGenerator or CourseGenerator . For example, we want to use the Vrigina Dataset which is an open data set of job postings. We just have to create a job posting generator with some transformation. from skills_ml.job_postings.raw.virginia import VirginiaTransformer from urllib.request import urlopen import json va_url = \"http://opendata.cs.vt.edu/dataset/ab0abac3-2293-4c9d-8d80-22d450254389/resource/074f7e44-9275-4bba-874e-4795e8f6830c/download/openjobs-jobpostings.may-2016.json\" class VAJobposting(object): def __init__(self, uri): self.uri = uri def __iter__(self): request = urlopen(self.uri) for line in request.readlines(): raw = json.loads(line) yield VirginiaTransformer(partner_id=\"va\")._transform(raw) jobpostings_va = VAJobposting(va_url) print(len(list(jobpostings_va))) 40098 Filtering \u00b6 To create a good dataset, we might want to have some criteria for choosing the proper job posting based on the task we want to perform, like job postings that have the label information, job postings that belong to certain occupation, or job postings that have rich enough information in the description field. JobPostingFilterer : Filter common schema job postings through a number of filtering functions. This function also follows lazy evaluation strategy. from skills_ml.job_postings.filtering import JobPostingFilterer def is_tech_jobs(job): if job['onet_soc_code'][:2] in ['15', '17', '19']: return True else: return False tech_jobs = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[is_tech_jobs] ) from skills_ml.ontologies.onet import majorgroupname from collections import Counter import pandas as pd import matplotlib.pyplot as plt import matplotlib import seaborn as sns sns.set(style=\"darkgrid\", font_scale=2) %matplotlib inline # major group distribution plotting function def plot_major_group_distribution(job_postings): c = Counter() for job in job_postings: c.update([job['onet_soc_code'][:2]]) s = pd.Series(c).sort_index() s.index = s.index.map(majorgroupname) ax = s.plot.bar(figsize=(20,10),rot=90) ax.set_xlabel('soc_major_group') ax.set_ylabel('number of job posting') ax.set_title(f\"total number: {s.sum()}\") return s plot_major_group_distribution(tech_jobs) Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 dtype: int64 What if we want to make sure that all the job postings have ONet SOC Code and it's not unknown(first 2 digit 99)? We can define filter functions like these which can be either generic function or lambda function. def filter_onet_soc_code(job): if job['onet_soc_code'] and job['onet_soc_code'][:2] != '99': return True else: return False has_soc = lambda x: x['onet_soc_code'] not_unknown_soc = lambda x: x['onet_soc_code'][:2] != '99' jobpostings_filtered = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[has_soc, not_unknown_soc] ) plot_major_group_distribution(jobpostings_filtered) Management 6506 Business and Financial Operations 3867 Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 Community and Social Service 282 Legal 94 Education, Training, and Library 679 Arts, Design, Entertainment, Sports, and Media 598 Healthcare Practitioners and Technical 3447 Healthcare Support 494 Protective Service 484 Food Preparation and Serving Related 792 Building and Grounds Cleaning and Maintenance 189 Personal Care and Service 97 Sales and Related 1415 Office and Administrative Support 2580 Construction and Extraction 196 Installation, Maintenance, and Repair 832 Production 442 Transportation and Material Moving 2722 Military Specific 2 dtype: int64 Random Sampling \u00b6 Even though we have a lot of data, most of time we don't need all of them to do the analysis. Or we can't even fit all the data into memory to do the analysis. What we need more importantly is a suitable sampled dataset. JobSampler : Sample job posting by (weighted) reservoir sampling. Random Sampling from Streaming Data - Reservoir Sampling \u00b6 \"Say you have a stream of items of large and unknown length that we can only iterate over once.\" It's memeory efficient and just one iteration There is a great overview of reservoir sampling in https://gregable.com/2007/10/reservoir-sampling.html. Let's say the original job postings dataset are too much for my Mac Yosemite to do any analysis and I want only 1000 job postings but still preserve the statistical characteristics of the original dataset. from skills_ml.job_postings.sample import JobSampler sampler = JobSampler( job_posting_generator=jobpostings_filtered, k=1000, ) plot_major_group_distribution(sampler) Management 188 Business and Financial Operations 119 Computer and Mathematical 158 Architecture and Engineering 61 Life, Physical, and Social Science 3 Community and Social Service 10 Legal 2 Education, Training, and Library 12 Arts, Design, Entertainment, Sports, and Media 12 Healthcare Practitioners and Technical 112 Healthcare Support 12 Protective Service 11 Food Preparation and Serving Related 30 Building and Grounds Cleaning and Maintenance 7 Personal Care and Service 4 Sales and Related 54 Office and Administrative Support 82 Construction and Extraction 8 Installation, Maintenance, and Repair 26 Production 16 Transportation and Material Moving 73 dtype: int64 Something wrong happened! We are missing Military Occupations ! Because military job postings are extremely rare in the original dataset, simple ramdom sampling might result in lack of classes. Weighted Reservoir Sampling \u00b6 How would you sample from a weighted distribution where each element has a given weight associated with it in the stream? For certain task, we need some curated sample. For example, if we want to build a occupation classifier, we want similar amounts of job posting for each occupation. Now we want to have a more uniform distributed sample across all major groups. Here we need to provide a weight dictionary in the JobSampler c = Counter() for job in jobpostings_filtered: c.update([job['onet_soc_code'][:2]]) weights = dict() for key, value in c.items(): weights[key] = max(c.values()) / value weights {'11': 1.0, '15': 1.2845014807502468, '17': 3.3588022715539494, '29': 1.8874383521903104, '41': 4.597879858657244, '43': 2.521705426356589, '13': 1.6824411688647531, '49': 7.819711538461538, '33': 13.442148760330578, '27': 10.879598662207357, '47': 33.19387755102041, '51': 14.71945701357466, '35': 8.214646464646465, '25': 9.581737849779087, '31': 13.17004048582996, '19': 33.885416666666664, '21': 23.070921985815602, '37': 34.423280423280424, '53': 2.3901542983100663, '39': 67.0721649484536, '23': 69.2127659574468, '55': 3253.0} sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=1000, key=lambda x: x['onet_soc_code'][:2], weights=weights) plot_major_group_distribution(sampler) Management 41 Business and Financial Operations 51 Computer and Mathematical 49 Architecture and Engineering 55 Life, Physical, and Social Science 49 Community and Social Service 42 Legal 39 Education, Training, and Library 56 Arts, Design, Entertainment, Sports, and Media 54 Healthcare Practitioners and Technical 55 Healthcare Support 45 Protective Service 57 Food Preparation and Serving Related 52 Building and Grounds Cleaning and Maintenance 44 Personal Care and Service 37 Sales and Related 48 Office and Administrative Support 48 Construction and Extraction 38 Installation, Maintenance, and Repair 38 Production 51 Transportation and Material Moving 49 Military Specific 2 dtype: int64 Skill Extraction \u00b6 A common task is extracting competencies from unstructured text. Sometimes this is ontology-based (finding concepts from a known ontology in text), but this is not necessarily true. Skills-ML unites these with a common interface in the SkillExtractor class. The common interface is that every SkillExtractor needs to be able to take in a collection of documents, and yield what we call CandidateSkill objects. What Is a CandidateSkill? \u00b6 A CandidateSkill is a possible occurrence of a skill/competency in context in some document. It consists of the following fields: skill_name - The text version of the skill as it appears in the document matched_skill_identifier - A reference to the skill in some ontology. This may be empty, if no ontology was used to search for skills. context - The text surrounding the skill in the document. The goal is for a human labeler to be able to use this to determine whether or not the occurrence represents a true skill. How much context is included is up to the algorithm. start_index - The start index of the skill occurrence within the document string. confidence - The confidence level the algorithm has in this candidate skill being a true occurrence of a skill. This may be empty, if the algorithm has now way of producing a confidence value. document_id - A unique identifier for the source document. document_type - The type of document (examples: Job Posting, Profile, Course Description) source_object - The entire source document. skill_extractor_name - The name of the skill extractor algorithm. Every SkillExtractor subclass defines a name property that is used to give processes downstream context about how their output data was produced. The idea behind the CandidateSkill object is to serve as a common interface between SkillExtractor objects, automatic evaluation methods, and manual evaluation methods. A labeling interface might intake CandidateSkill objects for humans to say yes/no to. Another type of labeling interface might involve the export of CandidateSkill objects based on what a human highlighted in the interface when shown the entire document Unsupervised evaluation metrics may take in one set of CandidateSkills to produce simple descriptive metrics Supervised evaluation metrics may take in one set of CandidateSkills from a SkillExtractor and another set of CandidateSkills from a human labeling interface and use the latter to evaluate the former We'll talk about some of these use cases in more detail later. But for now, let's start with a simple example that uses NLP rules and isn't tied to an ontology. Let's define a method for extracting skills as 'all noun phrases that end in the word skill or skills'. This is a simple method that realistically won't cover all possible occurrences of skills, but this is a start. from skills_ml.algorithms.skill_extractors import SkillEndingPatternExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample job_posting_generator = JobPostingCollectionSample() # instantiate the skill extractor. This class defaults to only considering lines that # start with a bullet, which doesn't work for this dataset. So we set this flag to False. skill_extractor = SkillEndingPatternExtractor(only_bulleted_lines=False) job_posting = next(iter(job_posting_generator)) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: communication skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: organizational skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: communication skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required skill name: organizational skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required The results for one job posting are modest. Two distinct skill names, each occurring two different times in the document. This is a start. Now let's try another skill extractor: matching with ONET data. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: self context: The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric. skill name: self context: We are proud to be an equal opportunity employer College degree preferred, 2-5 years experience in print and/or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients/prospects is required, Minimal non local travel a few times a year is required Yikes. What is this? As it turns out, 'Self' is a real programming language . But it's not applicable here. Simply searching for skill names has its limitations. To help with this, there is also the SocScopedExactMatchSkillExtractor. This does exact matching, but only for the occupation that the document is tagged with. This, of course, is only applicable if the document has one. And it needs a full CompetencyOntology to work. from skills_ml.algorithms.skill_extractors import SocScopedExactMatchSkillExtractor skill_extractor = SocScopedExactMatchSkillExtractor(onet) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') No results. This is expected: For an occupation that is not related to computer programming, the language 'Self' is likely irrelevant. Here's a list of all the other skill extractors available. FuzzyMatchSkillExtractor - Similar to the ExactMatchSkillExtractor, but using a configurable edit distance to find skill names that are very close to the targets. AbilityEndingPatternExtractor - Similar to the SkillEndingPatternExtractor, but finding noun phrases that end in 'ability' or 'abilities'. SectionExtractSkillExtractor - Attempts to divide the text into sections with headers, which is a common pattern found in job postings. Return each individual sentence found in sections with certain headers (Skills, Competencies, Qualifications). Evaluating Skill Extractors \u00b6 We want to be able to evaluate skill extractors. We may or may not have labeled skills but do want to be able to generate descriptive metrics either way. from skills_ml.evaluation.skill_extraction_metrics import TotalOccurrences, TotalVocabularySize, OntologyCompetencyRecall metrics = [ TotalOccurrences(), TotalVocabularySize(), OntologyCompetencyRecall(onet) ] exact_match_skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for metric in metrics: candidate_skills = [] for job_posting in job_posting_generator: candidate_skills += list(exact_match_skill_extractor.candidate_skills(job_posting)) print('metric:', metric.name, 'value:', metric.eval(candidate_skills, 50)) metric: total_candidate_skills value: 153 metric: total_vocabulary_size value: 40 metric: onet_ksat_competency_recall value: 0.001248868213181804 Embedding \u00b6 Labor market data tends to be large in scale, but represented as raw text. Consequently, an important early step for most tasks is to transform texts into a mathematical form that can be used in the downstream tasks. In the context of skills and jobs, an embedding model trained on large amount of job posting data is able to map a skill or a job title into a high dimensional space as well as preserving the contextual and semantic relationship. Ideally, a good embedding model will cluster similar skills and jobs. Embedding Models \u00b6 Many word embedding techniques have been developed since the most impactful embedding algorithm word2vec was published in 2013. Currently, Skills-ML includes word2vec, doc2vec and fastext and may include more in the future. Word2VecModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, a random vector will be assigned with the same dimension. Doc2VecModel is able to look up a word vector and infer a sentence/paragraph vector by gradient descending on the fly, so it is non-deterministic. It does not support online learning. FastTextModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, it sums all vectors of the unseen word\u2019s char-ngrams. If none of the char-ngrams of the unseen word is present, a random vector will be assigned with the same dimension. from skills_ml.algorithms.embedding.models import Word2VecModel, FastTextModel cbow = Word2VecModel(size=200, sg=0, window=7, iter=3, batch_words=1000) skip_gram = Word2VecModel(size=200, sg=1, window=7, iter=3, batch_words=1000) fasttext = FastTextModel(size=200, window=7, iter=3, batch_words=1000) Corpora \u00b6 Next, we need some text corpus to train embedding modelss. Skills-ML provides pre-defined classes to convert common schema job listings into a corpus in documnet level suitable for use by machine learning algorithms or specific tasks. Word2VecGensimCorpusCreator Doc2VecGensimCorpusCreator from skills_ml.job_postings.corpora import Word2VecGensimCorpusCreator, Doc2VecGensimCorpusCreator sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) w2v_corpus_generator = Word2VecGensimCorpusCreator(sampler) Preprocessing \u00b6 Or we can build our own corpus generator by using some preprocessing tools Function Compostition - ProcessingPipeline will compose processing functions together to become a callable object that takes in the input from the very first processing function and returns the output of the last processing function. - IterablePipeline will compose processing functions together to be passed to different stages(training/ prediction) from skills_ml.algorithms.preprocessing import IterablePipeline from skills_ml.algorithms import nlp from functools import partial document_schema_fields = ['description','experienceRequirements', 'qualifications', 'skills'] pipeline = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_html, nlp.clean_str, nlp.word_tokenize, ) corpus_generator = pipeline(sampler) Train Embedding \u00b6 The EmbeddingTrainer provides online batch learning for Word2VecModel and FastTextModel. from skills_ml.algorithms.embedding.train import EmbeddingTrainer trainer = EmbeddingTrainer(cbow, skip_gram, fasttext, batch_size=100) trainer.train(corpus_generator) WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles Storage \u00b6 Skills-ML has couple useful storage classes that could benefit both local or cloud. - S3Store : S3 storage engine - FSStore : File system storage engine - ModelStorage : Serialization model storage. from skills_ml.storage import FSStore, S3Store, ModelStorage fs = FSStore(path=\"tmp/model_cache/embedding/examples\") trainer.save_model(storage=fs) print(cbow.model_name) print(cbow.storage) word2vec_9d094b1f6b2610e530e14e5a71884be3.model FSStore(path=tmp/model_cache/embedding/examples) Examples \u00b6 for c, s in zip(cbow.wv.most_similar(['engineer']), skip_gram.wv.most_similar(['engineer'])): print(c, s) ('developer', 0.8033339977264404) ('sr', 0.623277485370636) ('sr', 0.7680195569992065) ('architect', 0.5734556317329407) ('analyst', 0.724011242389679) ('developer', 0.5714775919914246) ('designer', 0.7087661027908325) ('writer', 0.5478475093841553) ('architect', 0.6725302934646606) ('soc', 0.5414971709251404) ('mclean', 0.6457548141479492) ('herndon', 0.5357624292373657) ('embedded', 0.6317166090011597) ('tester', 0.532582700252533) ('inc', 0.6310484409332275) ('isso', 0.5319244861602783) ('scientist', 0.62971031665802) ('analyst', 0.5278465747833252) ('springfield', 0.6180558800697327) ('editor', 0.5252323746681213) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. if np.issubdtype(vec.dtype, np.int): for c, s in zip(cbow.wv.most_similar(['python']), skip_gram.wv.most_similar(['python'])): print(c, s) ('scripting', 0.9461991190910339) ('ruby', 0.9052647948265076) ('java', 0.9233536720275879) ('perl', 0.8700193762779236) ('ruby', 0.9213511943817139) ('js', 0.8653509616851807) ('unix', 0.9152072668075562) ('selenium', 0.8345634937286377) ('languages', 0.9030321836471558) ('git', 0.8201907277107239) ('javascript', 0.8950035572052002) ('subversion', 0.8199359774589539) ('linux', 0.8872419595718384) ('javascript', 0.8152703046798706) ('programming', 0.8679823875427246) ('scripting', 0.8112311363220215) ('sql', 0.8587027788162231) ('unix', 0.8111978769302368) ('perl', 0.8509498238563538) ('languages', 0.8040737509727478) for c, s in zip(cbow.wv.most_similar(['cnc']), skip_gram.wv.most_similar(['cnc'])): print(c, s) ('machine', 0.7754281759262085) ('hws', 0.7544339895248413) ('machines', 0.730404257774353) ('fabrication', 0.743915319442749) ('circuits', 0.7134151458740234) ('machining', 0.7412827014923096) ('hydraulic', 0.7047945857048035) ('controllers', 0.7408922910690308) ('machining', 0.7047876715660095) ('turbines', 0.734102725982666) ('pumps', 0.6980614066123962) ('overhauling', 0.7285677194595337) ('metal', 0.6945027112960815) ('rebuilding', 0.7274059057235718) ('electric', 0.6921502351760864) ('bearings', 0.7227500677108765) ('assemblies', 0.6852962970733643) ('turbine', 0.7216542959213257) ('machinery', 0.67722487449646) ('harpoon', 0.7176830172538757) Skills-ML also provides a function to visualize the embedding in tensorboard from skills_ml.algorithms.embedding.models import visualize_in_tensorboard visualize_in_tensorboard(cbow) Run `tensorboard --logdir=/home/ubuntu/skills-ml/word2vec_9d094b1f6b2610e530e14e5a71884be3 --host 127.0.0.1` to run visualize result on tensorboard Evaluation \u00b6 Although there is an emerging trend towards generating embeddings for structured and unstructured data, there is not yet any systematic suite for measuring the quality of embeddings. We generally follow one of the few works in embedding evaluation [Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts] to create metrics for evaluating embedding against the gold standard ontology dataset. The gold standard ontology is curated by domain experts like O*NET, so a good embedding should replicate the structure of the entities in the gold standard taxonomy. In other words, it is useful to see how an embedding reflects the clustering structure. One trivial clustering is Major Groups of occupations. A good embedding should cluster the occupations which belong to the same major groups. CategorizationMetric : The cosine similarity between the embedding of the concept and the mean vector of embeddings of all the entities within that concept cluster. This metric aligns a clustering of entities into different categories, reflecting how well the embedding of a concept cluster performs as the background concept of the entities typed by it. IntraClusterCohesion : The sum of squared error of the embedding of the centroid of the concept cluster and the embedding of each entities within that cluster. It measures how near the data points in a cluster are to the cluster centroid. MajorGroupRecall : For a major group, calculate the cosine similarity against all the occupations and find the top n closest occupations. The recall is defined as the number of true positives from top n closest occupations divided by the total number of occupation within the major group. MajorGroupPrecision : Similarly to MajorGroupRecall which is called Coherence Score in the paper, start by finding the top n closest occupations. The precision is defined as the number of true positives from top n closest occupations divided by n from skills_ml.ontologies.onet import Onet major_group_occupation_des_clustering = onet.major_group_occupation_description_clustering from skills_ml.evaluation.embedding_metrics import metrics_for_embedding, CategorizationMetric, IntraClusterCohesion, RecallTopN, PrecisionTopN from skills_ml.algorithms.preprocessing import ProcessingPipeline def vectorization(embedding): p = ProcessingPipeline( nlp.normalize, nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=embedding) ) return p categorization_metric = CategorizationMetric(major_group_occupation_des_clustering) intra_cohesion = IntraClusterCohesion(major_group_occupation_des_clustering) recall_top = RecallTopN(major_group_occupation_des_clustering, topn=10) precision_top = PrecisionTopN(major_group_occupation_des_clustering, topn=10) categorization_metric.eval(vectorization(fasttext)) {'Life, Physical, and Social Science': 0.544181802616692, 'Education, Training, and Library': 0.39010462667236234, 'Legal': 0.44299323639279864, 'Protective Service': 0.8338733121787928, 'Personal Care and Service': 0.5520270184970673, 'Military Specific': 0.8403838778749896, 'Arts, Design, Entertainment, Sports, and Media': 0.41337371196897577, 'Business and Financial Operations': 0.33068055201077906, 'Installation, Maintenance, and Repair': 0.18600172293592943, 'Community and Social Service': 0.3938874349835806, 'Food Preparation and Serving Related': 0.2854292068104062, 'Farming, Fishing, and Forestry': 0.49009970825435945, 'Management': 0.6667720141362156, 'Architecture and Engineering': 0.31587733273288865, 'Transportation and Material Moving': 0.3438685579518763, 'Healthcare Practitioners and Technical': 0.3313605878282969, 'Production': 0.7619768121213049, 'Computer and Mathematical': 0.2723307384580105, 'Sales and Related': 0.29548456080419716, 'Office and Administrative Support': 0.5946786302511617, 'Construction and Extraction': 0.46874949551031586, 'Healthcare Support': 0.9223879488399899, 'Building and Grounds Cleaning and Maintenance': 0.32908847148448117} import statistics import operator from collections import defaultdict # We define some helper functions to evaluate multiple embeddings def algorithm_name(emb): if emb.model_type == 'word2vec' or emb.model_type == 'fasttext': if getattr(emb, 'sg', None) == 1: return 'Skip-Gram' else: return 'Continuous Bag of Words' elif emb.model_type == 'doc2vec': if getattr(emb, 'dm', None) == 1: return 'Distributed Memory' else: return 'Distributed Bag of Words' def evaluate_multiple_embeddings(embeddings, vectorization, metric): result = defaultdict(dict) for emb in embeddings: c = metric.eval(vectorization(emb)) name = emb.model_name.split('.')[0] result[name]['mean'] = statistics.mean(list(c.values())) result[name]['variance'] = statistics.variance(list(c.values())) result[name]['std'] = statistics.stdev(list(c.values())) result[name]['max'] = max(c.items(), key=operator.itemgetter(1))[1] result[name]['max_cluster'] = max(c.items(), key=operator.itemgetter(1))[0] result[name]['min'] = min(c.items(), key=operator.itemgetter(1))[1] result[name]['min_cluster'] = min(c.items(), key=operator.itemgetter(1))[0] result[name]['type'] = emb.model_type result[name]['algorithm'] = algorithm_name(emb) result[name]['window'] = emb.window return pd.DataFrame(result) evaluate_multiple_embeddings([cbow, skip_gram, fasttext], vectorization, categorization_metric) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word2vec_9d094b1f6b2610e530e14e5a71884be3 word2vec_b6aebc90db420cbb071d7e78e3158e51 fasttext_7d92bf3d9781f057978d909bed7e7f04 algorithm Continuous Bag of Words Skip-Gram Continuous Bag of Words max 0.923396 0.64388 0.922388 max_cluster Healthcare Support Production Healthcare Support mean 0.49752 0.34426 0.478505 min 0.161117 0.181937 0.186002 min_cluster Installation, Maintenance, and Repair Food Preparation and Serving Related Installation, Maintenance, and Repair std 0.221347 0.122163 0.204855 type word2vec word2vec fasttext variance 0.0489945 0.0149237 0.0419656 window 7 7 7 Occupation Classification \u00b6 A common issue with job posting data is incomplete, incorrect, and inconsistent occupation classification. The majority of job postings in the US are using the O*NET SOC classification system, but many are either missing or poorly classified. This can be improved by using machine learning. SOC Codes \u00b6 Most of the job posting data collected are aligned with the O*NET SOC system. The occupations in the SOC are classified at four levels of aggregation: major group, minor group, broad occupation, and detailed occupation. Each lower level of detail identifies a more specific group of occupations. Each item in the SOC is designated by a six-digit code. The first two digits represent the major group, the third digit represents the minor group, the fourth and fifth digits represent the broad occupation, and the sixth digit represents the detailed occupation. - Major group codes end with 0000 (e.g., 29-0000 Healthcare Practitioners and Technical Occupations \u2014the exceptions are minor groups 15-1200 Computer Occupations, 31- 1100 Home Health and Personal Care Aides; and Nursing Assistants, Orderlies, and Psychiatric Aides, and 51-5100 Printing Workers, which end with 00). - Minor groups generally end with 000 (e.g., 29-1000 Health Diagnosing or Treating Practitioners). - Broad occupations end with 0 (e.g., 29-1020 Dentists). - Detailed occupations end with a number other than 0 (e.g., 29-1022 Oral and Maxillofacial Surgeons). Target Variable \u00b6 FullSOC SOCMajorGroup from skills_ml.algorithms.occupation_classifiers import FullSOC, SOCMajorGroup full_soc = FullSOC(onet_cache=onet) Design Matrix \u00b6 The classification task consists of inferring a SOC code from a job posting and is accomplished through several stages: preprocessing, filtering, training and testing. DesignMatrix helps users accomplish this task. import random from itertools import islice from skills_ml.utils import itershuffle from skills_ml.algorithms.occupation_classifiers import DesignMatrix sample = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) dataset = itershuffle(sample) train = islice(dataset, 0, 4000) test = islice(dataset, 4000) pipe_x = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=fasttext) ) pipe_y = IterablePipeline( full_soc.transformer ) matrix = DesignMatrix( train, full_soc, pipe_x, pipe_y, ) OccupationClassifierTrainer \u00b6 OccupationClassifierTrainer trains classifiers with cross validation and picks the best classifier with a grid search based on the metric. It takes in a dictionary for the grid search. from skills_ml.algorithms.occupation_classifiers.train import OccupationClassifierTrainer grid_config = { 'sklearn.ensemble.ExtraTreesClassifier': { 'n_estimators': [50, 100], 'criterion': ['entropy', 'gini'], 'max_depth': [20], 'max_features': ['log2'], 'min_samples_split': [10] }, 'sklearn.neural_network.MLPClassifier': { 'hidden_layer_sizes': [100, 500], 'activation': ['logistic', 'relu'], 'solver': ['adam'] }, } cls_trainer = OccupationClassifierTrainer( matrix=matrix, k_folds=3, grid_config=grid_config, storage=FSStore('tmp/model_cache/soc_classifiers/examples'), n_jobs=4) cls_trainer.train(save=False) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1d /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) cls_trainer.best_estimators [<ProxyObjectWithStorage at 0x7f2f1c1bfb48 for GridSearchCV at 0x7f2f19160278>, <ProxyObjectWithStorage at 0x7f2f1bfb2eb8 for GridSearchCV at 0x7f2f192ea0b8>] Evaluation \u00b6 Accuracy, recall, precision and f1 are the metrics taken into consideration. Since it is a multi-class classification problem, an overall performance is evaluated by looking at the micro-average and macro-average for the metrics. A macro-average will compute the metric independently for each class and then take the average, whereas a micro-average will aggregate the contributions of all classes and then computes the average. In other words, a macro-average is treating all classes equally. from skills_ml.algorithms.occupation_classifiers.test import OccupationClassifierTester from skills_ml.evaluation.occ_cls_evaluator import OnetOccupationClassificationEvaluator from skills_ml.algorithms.occupation_classifiers.classifiers import CombinedClassifier from skills_ml.algorithms.embedding.train import Reiterable steps = [ partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.normalize, nlp.clean_str, nlp.word_tokenize, ] evaluators = [] test_data = list(test) for cls in cls_trainer.best_estimators: tester = OccupationClassifierTester( test_data_generator=test_data, preprocessing=steps, classifier=CombinedClassifier(fasttext, cls) ) evaluators.append(OnetOccupationClassificationEvaluator(tester)) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'itershuffle' raised StopIteration for e, c in zip(evaluators, cls_trainer.best_estimators): print(c.best_estimator_) print('accuracy: ', e.accuracy) print('precision: ', e.precision) print('f1: ', e.f1) print('major group: ', e.accuracy_major_group) print('macro precision: ', e.macro_precision) print('micro precision: ', e.micro_precision) print('recall: ', e.recall) print('macro recall: ', e.macro_recall) print('micro recall: ', e.micro_recall) print('macro f1: ', e.macro_f1) print('micro f1: ', e.micro_f1) print('\\n') ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini', max_depth=20, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4, oob_score=False, random_state=None, verbose=0, warm_start=False) accuracy: 0.483 precision: [0.09090909 0. 0.07894737 ... 0. 0. 0. ] f1: [0.15384615 0. 0.12244898 ... 0. 0. 0. ] major group: 0.547 macro precision: 0.3822259316025384 micro precision: 0.483 recall: [0.5 0. 0.27272727 ... 0. 0. 0. ] macro recall: 0.3180345574223314 micro recall: 0.483 macro f1: 0.3258889572669588 micro f1: 0.483 MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=500, learning_rate='constant', learning_rate_init=0.001, max_iter=200, momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) accuracy: 0.51 precision: [0. 0. 0.1 ... 0. 0. 0. ] f1: [0. 0. 0.11538462 ... 0. 0. 0. ] major group: 0.587 macro precision: 0.3392587829678509 micro precision: 0.51 recall: [0. 0. 0.13636364 ... 0. 0. 0. ] macro recall: 0.3423782550302827 micro recall: 0.51 macro f1: 0.32266097925849724 micro f1: 0.51 /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for)","title":"Skills ML Tour"},{"location":"Skills-ML Tour/#skills-ml-tour","text":"Skills-ML is an open source software library for applying NLP and ML to labor market data. It allows the user to perform tasks like skill extraction and occupation classification to collections of documents such as job postings, profiles, and course descriptions.","title":"Skills-ML Tour"},{"location":"Skills-ML Tour/#competency","text":"A competency is any expertise or talent that is useful for a job. Developed capacities (e.g. active listening), proficiency with tools or technology (e.g. lancets, Microsoft Word), innate abilities (e.g. originality), and academic knowledge (e.g. medicine) are all considered competencies. from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='dino_riding', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' )","title":"Competency"},{"location":"Skills-ML Tour/#competency-relationships","text":"Competencies are often related to each other. Defining parent-child relationships is a standard building block of existing competency frameworks like ONET and ESCO. A parent-child relationship generally implies that the child is a \"type of\" the parent. from skills_ml.ontologies import Competency from skills_ml.ontologies.viz import display_nodes dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) train_surfing = Competency( identifier='12346', name='Train Surfing', description='Standing on the train while it goes' ) time_travel = Competency( identifier='23456', name='Time Travel', description='Traveling Through Time' ) advanced_science = Competency( identifier='2345', name='Advanced Science', ) extreme_transportation = Competency( identifier='123', name='Extreme Transportation', description='Comically dangerous forms of transportation' ) time_travel.add_parent(advanced_science) dinosaur_riding.add_parent(extreme_transportation) train_surfing.add_parent(extreme_transportation) display_nodes([dinosaur_riding, train_surfing, extreme_transportation, time_travel, advanced_science])","title":"Competency Relationships"},{"location":"Skills-ML Tour/#occupation","text":"An occupation is a job or profession that a person can hold. Similar to competencies, these are also often defined hierarchically. from skills_ml.ontologies import Occupation extreme_postal_workers = Occupation(identifier='999', name='Extreme Postal Workers') dino_postal_worker = Occupation(identifier='9998', name='Deliverer of Items to the Past') train_yard_postal_worker = Occupation(identifier='9999', name='Deliverer of Items to Train Yards') dino_postal_worker.add_parent(extreme_postal_workers) train_yard_postal_worker.add_parent(extreme_postal_workers) display_nodes([extreme_postal_workers, dino_postal_worker, train_yard_postal_worker])","title":"Occupation"},{"location":"Skills-ML Tour/#competencyontology","text":"A CompetencyOntology is a model of the labor market, or some subset thereof, consisting of a collection of competencies, a collection of occupations, and all of the relationships between them. from skills_ml.ontologies import CompetencyOntology from skills_ml.ontologies.viz import display_ontology ontology = CompetencyOntology() ontology.add_competency(dinosaur_riding) ontology.add_competency(train_surfing) ontology.add_competency(extreme_transportation) ontology.add_competency(time_travel) ontology.add_competency(advanced_science) ontology.add_occupation(dino_postal_worker) ontology.add_occupation(train_yard_postal_worker) ontology.add_occupation(extreme_postal_workers) ontology.add_edge(occupation=dino_postal_worker, competency=dinosaur_riding) ontology.add_edge(occupation=dino_postal_worker, competency=time_travel) ontology.add_edge(occupation=train_yard_postal_worker, competency=train_surfing) display_ontology(ontology)","title":"CompetencyOntology"},{"location":"Skills-ML Tour/#prebuilt-ontologies","text":"To move on we'll want to level up to a full ontology. The example we'll use is O*NET, built from survey data and maintained by the US Department of Labor. A CompetencyOntology subclass that downloads the source files from the O*NET web site is included in Skills-ML. from skills_ml.ontologies.onet import Onet onet = Onet() onet.print_summary_stats() Ontology summary statistics for onet Num competencies: 32030 Num occupations: 1133 Num competency-occupation edges: 107305 Median occupations per competency: 1 Median competencies per occupation: 89 Mean occupations per competency: 3.350245090386837 Mean competencies per occupation: 94.70873786407768 list(onet.competencies)[0:5] [Competency(identifier=41104007-Water sampling pumps, name=Water sampling pumps, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43232502-Distance learning software, name=Distance learning software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=46181509-Chemical-resistant suits, name=Chemical-resistant suits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43231603-Avalara AvaTax ST, name=Avalara AvaTax ST, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232603-Mainsaver Asset Management, name=Mainsaver Asset Management, categories=['Technology', 'O*NET T2'], {})]","title":"Prebuilt Ontologies"},{"location":"Skills-ML Tour/#filtering","text":"~34000 competencies and ~1100 occupations is a lot. Let's explore the filtering functionality of the CompetencyOntology to zoom in on a more specific slice. filter_by filters using edges: the filtering function it expects takes in an edge (between a Competency and Occupation) and returns whether or not it should be in the result. The result takes the form of a new CompetencyOntology, so you can interact with it in the same way as you would the source ontology. nurse_practitioners = onet.filter_by(lambda edge: 'Nurse Practitioners' in edge.occupation.name) nurse_practitioners.competencies {Competency(identifier=1.A.1.a.1, name=Oral Comprehension, categories=['Abilities'], {'competencyText': 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'}), Competency(identifier=1.A.1.a.2, name=Written Comprehension, categories=['Abilities'], {'competencyText': 'The ability to read and understand information and ideas presented in writing.'}), Competency(identifier=1.A.1.a.3, name=Oral Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in speaking so others will understand.'}), Competency(identifier=1.A.1.a.4, name=Written Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in writing so others will understand.'}), Competency(identifier=1.A.1.b.1, name=Fluency of Ideas, categories=['Abilities'], {'competencyText': 'The ability to come up with a number of ideas about a topic (the number of ideas is important, not their quality, correctness, or creativity).'}), Competency(identifier=1.A.1.b.2, name=Originality, categories=['Abilities'], {'competencyText': 'The ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.'}), Competency(identifier=1.A.1.b.3, name=Problem Sensitivity, categories=['Abilities'], {'competencyText': 'The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.'}), Competency(identifier=1.A.1.b.4, name=Deductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to apply general rules to specific problems to produce answers that make sense.'}), Competency(identifier=1.A.1.b.5, name=Inductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events).'}), Competency(identifier=1.A.1.b.6, name=Information Ordering, categories=['Abilities'], {'competencyText': 'The ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations).'}), Competency(identifier=1.A.1.b.7, name=Category Flexibility, categories=['Abilities'], {'competencyText': 'The ability to generate or use different sets of rules for combining or grouping things in different ways.'}), Competency(identifier=1.A.1.c.1, name=Mathematical Reasoning, categories=['Abilities'], {'competencyText': 'The ability to choose the right mathematical methods or formulas to solve a problem.'}), Competency(identifier=1.A.1.d.1, name=Memorization, categories=['Abilities'], {'competencyText': 'The ability to remember information such as words, numbers, pictures, and procedures.'}), Competency(identifier=1.A.1.e.1, name=Speed of Closure, categories=['Abilities'], {'competencyText': 'The ability to quickly make sense of, combine, and organize information into meaningful patterns.'}), Competency(identifier=1.A.1.e.2, name=Flexibility of Closure, categories=['Abilities'], {'competencyText': 'The ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material.'}), Competency(identifier=1.A.1.e.3, name=Perceptual Speed, categories=['Abilities'], {'competencyText': 'The ability to quickly and accurately compare similarities and differences among sets of letters, numbers, objects, pictures, or patterns. The things to be compared may be presented at the same time or one after the other. This ability also includes comparing a presented object with a remembered object.'}), Competency(identifier=1.A.2.a.1, name=Arm-Hand Steadiness, categories=['Abilities'], {'competencyText': 'The ability to keep your hand and arm steady while moving your arm or while holding your arm and hand in one position.'}), Competency(identifier=1.A.2.a.3, name=Finger Dexterity, categories=['Abilities'], {'competencyText': 'The ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.'}), Competency(identifier=1.A.4.a.1, name=Near Vision, categories=['Abilities'], {'competencyText': 'The ability to see details at close range (within a few feet of the observer).'}), Competency(identifier=1.A.4.b.4, name=Speech Recognition, categories=['Abilities'], {'competencyText': 'The ability to identify and understand the speech of another person.'}), Competency(identifier=1.A.4.b.5, name=Speech Clarity, categories=['Abilities'], {'competencyText': 'The ability to speak clearly so others can understand you.'}), Competency(identifier=2.A.1.a, name=Reading Comprehension, categories=['Skills'], {'competencyText': 'Understanding written sentences and paragraphs in work related documents.'}), Competency(identifier=2.A.1.b, name=Active Listening, categories=['Skills'], {'competencyText': 'Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.'}), Competency(identifier=2.A.1.c, name=Writing, categories=['Skills'], {'competencyText': 'Communicating effectively in writing as appropriate for the needs of the audience.'}), Competency(identifier=2.A.1.d, name=Speaking, categories=['Skills'], {'competencyText': 'Talking to others to convey information effectively.'}), Competency(identifier=2.A.1.f, name=Science, categories=['Skills'], {'competencyText': 'Using scientific rules and methods to solve problems.'}), Competency(identifier=2.A.2.a, name=Critical Thinking, categories=['Skills'], {'competencyText': 'Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.'}), Competency(identifier=2.A.2.b, name=Active Learning, categories=['Skills'], {'competencyText': 'Understanding the implications of new information for both current and future problem-solving and decision-making.'}), Competency(identifier=2.A.2.c, name=Learning Strategies, categories=['Skills'], {'competencyText': 'Selecting and using training/instructional methods and procedures appropriate for the situation when learning or teaching new things.'}), Competency(identifier=2.A.2.d, name=Monitoring, categories=['Skills'], {'competencyText': 'Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.'}), Competency(identifier=2.B.1.a, name=Social Perceptiveness, categories=['Skills'], {'competencyText': \"Being aware of others' reactions and understanding why they react as they do.\"}), Competency(identifier=2.B.1.b, name=Coordination, categories=['Skills'], {'competencyText': \"Adjusting actions in relation to others' actions.\"}), Competency(identifier=2.B.1.c, name=Persuasion, categories=['Skills'], {'competencyText': 'Persuading others to change their minds or behavior.'}), Competency(identifier=2.B.1.d, name=Negotiation, categories=['Skills'], {'competencyText': 'Bringing others together and trying to reconcile differences.'}), Competency(identifier=2.B.1.e, name=Instructing, categories=['Skills'], {'competencyText': 'Teaching others how to do something.'}), Competency(identifier=2.B.1.f, name=Service Orientation, categories=['Skills'], {'competencyText': 'Actively looking for ways to help people.'}), Competency(identifier=2.B.2.i, name=Complex Problem Solving, categories=['Skills'], {'competencyText': 'Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.'}), Competency(identifier=2.B.3.a, name=Operations Analysis, categories=['Skills'], {'competencyText': 'Analyzing needs and product requirements to create a design.'}), Competency(identifier=2.B.4.e, name=Judgment and Decision Making, categories=['Skills'], {'competencyText': 'Considering the relative costs and benefits of potential actions to choose the most appropriate one.'}), Competency(identifier=2.B.4.g, name=Systems Analysis, categories=['Skills'], {'competencyText': 'Determining how a system should work and how changes in conditions, operations, and the environment will affect outcomes.'}), Competency(identifier=2.B.4.h, name=Systems Evaluation, categories=['Skills'], {'competencyText': 'Identifying measures or indicators of system performance and the actions needed to improve or correct performance, relative to the goals of the system.'}), Competency(identifier=2.B.5.a, name=Time Management, categories=['Skills'], {'competencyText': \"Managing one's own time and the time of others.\"}), Competency(identifier=2.B.5.d, name=Management of Personnel Resources, categories=['Skills'], {'competencyText': 'Motivating, developing, and directing people as they work, identifying the best people for the job.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=41103901-Microhematocrit centrifuges, name=Microhematocrit centrifuges, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104102-Lancets, name=Lancets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104104-Tourniquets, name=Tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104107-Evacuated blood collection tubes, name=Evacuated blood collection tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104118-Specimen collection containers, name=Specimen collection containers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104403-Tissue culture incubators, name=Tissue culture incubators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41111709-Binocular light compound microscopes, name=Binocular light compound microscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41115807-Hemoglobin analyzers, name=Hemoglobin analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116138-Urinalysis test strips, name=Urinalysis test strips, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116201-Glucometers, name=Glucometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131606-Protective face shields, name=Protective face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131612-Protective gowns, name=Protective gowns, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42132203-Medical examination protective gloves, name=Medical examination protective gloves, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42141807-Transcutaneous electric nerve stimulation TENS equipment, name=Transcutaneous electric nerve stimulation TENS equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Nasal suctioning equipment, name=Nasal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Oral suctioning equipment, name=Oral suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Tracheal suctioning equipment, name=Tracheal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142509-Epidural catheters, name=Epidural catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142532-Pericardiocentesis kits, name=Pericardiocentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142537-Thoracentesis kits, name=Thoracentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142609-Hypodermic syringes, name=Hypodermic syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142616-Blood drawing syringes, name=Blood drawing syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142715-Urinary catheters, name=Urinary catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42144102-Chest tubes, name=Chest tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171608-Head immobilization devices, name=Head immobilization devices, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171613-Spinal immobilization equipment, name=Spinal immobilization equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172101-Automated external defibrillators AED, name=Automated external defibrillators AED, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172102-Cardiopulmonary resuscitation CPR face shields, name=Cardiopulmonary resuscitation CPR face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Electronic blood pressure monitors, name=Electronic blood pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Pediatric blood pressure cuffs, name=Pediatric blood pressure cuffs, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Electrocardiography EKG machines, name=Electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Portable electrocardiography EKG machines, name=Portable electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181713-Holter monitors, name=Holter monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181801-Pulse oximeters, name=Pulse oximeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181902-Intracranial pressure monitors, name=Intracranial pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Cardiac monitors, name=Cardiac monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Hemodynamic monitors, name=Hemodynamic monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Ophthalmoscopes, name=Ophthalmoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Otoscopes, name=Otoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182013-Vaginal exam specula, name=Vaginal exam specula, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182103-Mechanical stethoscopes, name=Mechanical stethoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182201-Digital medical thermometers, name=Digital medical thermometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182302-Reflex hammers, name=Reflex hammers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182412-Diagnostic tuning forks, name=Diagnostic tuning forks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182416-Tympanometers, name=Tympanometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182805-Medical scales, name=Medical scales, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Snellen eye charts, name=Snellen eye charts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Visual acuity testing cards, name=Visual acuity testing cards, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42192401-Crash carts, name=Crash carts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42201701-Doppler ultrasound equipment, name=Doppler ultrasound equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Angiocaths, name=Angiocaths, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Pulmonary artery catheters, name=Pulmonary artery catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Pacemaker analyzers, name=Pacemaker analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Transcutaneous pacemakers, name=Transcutaneous pacemakers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42211502-Crutches, name=Crutches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221501-Arterial line catheters, name=Arterial line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221503-Multiple lumen central line catheters, name=Multiple lumen central line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221506-Umbilical catheters, name=Umbilical catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV administration sets, name=Intravenous IV administration sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV cutdown trays, name=Intravenous IV cutdown trays, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42222001-Intravenous IV infusion pumps, name=Intravenous IV infusion pumps, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42231701-Nasogastric tubes, name=Nasogastric tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241507-Orthopedic splinting equipment, name=Orthopedic splinting equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241705-Lower extremity braces, name=Lower extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241707-Walking braces, name=Walking braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241801-Upper extremity braces, name=Upper extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241802-Back braces, name=Back braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241803-Neck braces, name=Neck braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42242103-Halo traction equipment, name=Halo traction equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271501-Apnea monitors, name=Apnea monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271502-Arterial blood gas monitoring equipment, name=Arterial blood gas monitoring equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271602-Incentive spirometers, name=Incentive spirometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271604-Peak flowmeters, name=Peak flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271702-Oxygen concentrators, name=Oxygen concentrators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271707-Oxygen flowmeters, name=Oxygen flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271708-Oxygen delivery masks, name=Oxygen delivery masks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271710-Nasal catheters, name=Nasal catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271802-Handheld nebulizers, name=Handheld nebulizers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271903-Endotracheal ET tubes, name=Endotracheal ET tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271910-Tracheotomy sets, name=Tracheotomy sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272001-Fiberoptic laryngoscopes, name=Fiberoptic laryngoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272017-Intubation sets, name=Intubation sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272201-Mechanical intermittent positive pressure ventilators, name=Mechanical intermittent positive pressure ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272203-Bilevel positive airway pressure BiPAP ventilators, name=Bilevel positive airway pressure BiPAP ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291613-Surgical scalpels, name=Surgical scalpels, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291614-Straight surgical scissors, name=Straight surgical scissors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291704-Biopsy punches, name=Biopsy punches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291802-Mosquito hemostats, name=Mosquito hemostats, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294702-Intra-aortic balloon pumps IABP, name=Intra-aortic balloon pumps IABP, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294717-Ventricular assist devices VAD, name=Ventricular assist devices VAD, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Fiberoptic endoscopes, name=Fiberoptic endoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Flexible sigmoidoscopes, name=Flexible sigmoidoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295104-Electrosurgical cauterization machines, name=Electrosurgical cauterization machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Argon lasers, name=Argon lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Carbon dioxide CO2 lasers, name=Carbon dioxide CO2 lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Pulsed dye lasers, name=Pulsed dye lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295122-Pneumatic tourniquets, name=Pneumatic tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312008-Surgical staple removers, name=Surgical staple removers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312010-Skin staplers, name=Skin staplers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312202-Suturing kits, name=Suturing kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43191507-Multi-line telephone systems, name=Multi-line telephone systems, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211503-Laptop computers, name=Laptop computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211504-Personal digital assistants PDA, name=Personal digital assistants PDA, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211508-Personal computers, name=Personal computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211509-Tablet computers, name=Tablet computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43231507-Microsoft SharePoint, name=Microsoft SharePoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43231513-Microsoft Office, name=Microsoft Office, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232104-Microsoft Word, name=Microsoft Word, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232106-Microsoft PowerPoint, name=Microsoft PowerPoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232110-Microsoft Excel, name=Microsoft Excel, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232306-Microsoft Access, name=Microsoft Access, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Allscripts Professional EHR, name=Allscripts Professional EHR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Amkai AmkaiCharts, name=Amkai AmkaiCharts, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Bizmatics PrognoCIS EMR, name=Bizmatics PrognoCIS EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Cerner Millennium, name=Cerner Millennium, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-GE Healthcare Centricity EMR, name=GE Healthcare Centricity EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Healthcare common procedure coding system HCPCS, name=Healthcare common procedure coding system HCPCS, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MEDITECH software, name=MEDITECH software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical condition coding software, name=Medical condition coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical procedure coding software, name=Medical procedure coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medscribbler Enterprise, name=Medscribbler Enterprise, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MicroFour PracticeStudio.NET EMR, name=MicroFour PracticeStudio.NET EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-NextGen Healthcare Information Systems EMR, name=NextGen Healthcare Information Systems EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-PCC Pediatric Partner, name=PCC Pediatric Partner, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SOAPware EMR, name=SOAPware EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-StatCom Patient Flow Logistics Enterprise Suite, name=StatCom Patient Flow Logistics Enterprise Suite, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SynaMed EMR, name=SynaMed EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Texas Medical Software SpringCharts EMR, name=Texas Medical Software SpringCharts EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-e-MDs software, name=e-MDs software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-eClinicalWorks, name=eClinicalWorks, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Microsoft Internet Explorer, name=Microsoft Internet Explorer, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Web browser software, name=Web browser software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43233501-Microsoft Outlook, name=Microsoft Outlook, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=46181804-Safety goggles, name=Safety goggles, categories=['Tools', 'O*NET T2'], {})} That's a big list of competencies. Each competency has a list of categories, so let's get all of the different possible categories set(cat for competency in nurse_practitioners.competencies for cat in competency.categories) {'Abilities', 'Knowledge', 'O*NET T2', 'Skills', 'Technology', 'Tools'} So we can ask questions like: what type of Knowledge do nurse practitioners need? [competency for competency in nurse_practitioners.competencies if 'Knowledge' in competency.categories] [Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'})] There are many other questions we can ask of just an ontology, but the real value will come from combining the knowledge contained in the ontology with larger unstructured datasets. In this next section we will explore how Skills-ML helps the user deal with such datasets as job postings, profiles, or course descriptions.","title":"Filtering"},{"location":"Skills-ML Tour/#spirit-of-skills-ml","text":"Dataflow Programming : Skills-ML 's design philosophy builds on dataflow programming or so called data streaming to process very large datasets (larger than RAM; potentially infinite). One-pass algorithm : Data points are processed one at a time. Lazy evaluation : an evaluation strategy which delays the evaluation of an expression until its value is needed. In Skills-ML , most of the classes and functions here incorporates the concept of Iterable or Generator . We build the expression first and evaluate later.","title":"Spirit of Skills-ML"},{"location":"Skills-ML Tour/#creating-dataset","text":"Before we do anything with the context, we need dataset. Skills-ML makes use of schema.org\u2019s JobPosting standard. As it has been in use for a long time, some open sources are already using this standard, which is easy to import. Other job posting data sources are converted into the schema.org Schema and all work on job postings is done using this standard schema. In Skills-ML , job_postings module has all the functionalities to create the data we need for later usage.","title":"Creating Dataset"},{"location":"Skills-ML Tour/#common-schema","text":"We have an useful function to help create the data generator from s3. JobPostingCollectionFromS3 : Stream job posting from s3. JobPostingCollectionSample : Stream a finite number of job postings stored within the library. However, we are not restrcted to just JobPosting data. One can easily create whatever data generator such as ProfileGenerator or CourseGenerator . For example, we want to use the Vrigina Dataset which is an open data set of job postings. We just have to create a job posting generator with some transformation. from skills_ml.job_postings.raw.virginia import VirginiaTransformer from urllib.request import urlopen import json va_url = \"http://opendata.cs.vt.edu/dataset/ab0abac3-2293-4c9d-8d80-22d450254389/resource/074f7e44-9275-4bba-874e-4795e8f6830c/download/openjobs-jobpostings.may-2016.json\" class VAJobposting(object): def __init__(self, uri): self.uri = uri def __iter__(self): request = urlopen(self.uri) for line in request.readlines(): raw = json.loads(line) yield VirginiaTransformer(partner_id=\"va\")._transform(raw) jobpostings_va = VAJobposting(va_url) print(len(list(jobpostings_va))) 40098","title":"Common Schema"},{"location":"Skills-ML Tour/#filtering_1","text":"To create a good dataset, we might want to have some criteria for choosing the proper job posting based on the task we want to perform, like job postings that have the label information, job postings that belong to certain occupation, or job postings that have rich enough information in the description field. JobPostingFilterer : Filter common schema job postings through a number of filtering functions. This function also follows lazy evaluation strategy. from skills_ml.job_postings.filtering import JobPostingFilterer def is_tech_jobs(job): if job['onet_soc_code'][:2] in ['15', '17', '19']: return True else: return False tech_jobs = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[is_tech_jobs] ) from skills_ml.ontologies.onet import majorgroupname from collections import Counter import pandas as pd import matplotlib.pyplot as plt import matplotlib import seaborn as sns sns.set(style=\"darkgrid\", font_scale=2) %matplotlib inline # major group distribution plotting function def plot_major_group_distribution(job_postings): c = Counter() for job in job_postings: c.update([job['onet_soc_code'][:2]]) s = pd.Series(c).sort_index() s.index = s.index.map(majorgroupname) ax = s.plot.bar(figsize=(20,10),rot=90) ax.set_xlabel('soc_major_group') ax.set_ylabel('number of job posting') ax.set_title(f\"total number: {s.sum()}\") return s plot_major_group_distribution(tech_jobs) Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 dtype: int64 What if we want to make sure that all the job postings have ONet SOC Code and it's not unknown(first 2 digit 99)? We can define filter functions like these which can be either generic function or lambda function. def filter_onet_soc_code(job): if job['onet_soc_code'] and job['onet_soc_code'][:2] != '99': return True else: return False has_soc = lambda x: x['onet_soc_code'] not_unknown_soc = lambda x: x['onet_soc_code'][:2] != '99' jobpostings_filtered = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[has_soc, not_unknown_soc] ) plot_major_group_distribution(jobpostings_filtered) Management 6506 Business and Financial Operations 3867 Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 Community and Social Service 282 Legal 94 Education, Training, and Library 679 Arts, Design, Entertainment, Sports, and Media 598 Healthcare Practitioners and Technical 3447 Healthcare Support 494 Protective Service 484 Food Preparation and Serving Related 792 Building and Grounds Cleaning and Maintenance 189 Personal Care and Service 97 Sales and Related 1415 Office and Administrative Support 2580 Construction and Extraction 196 Installation, Maintenance, and Repair 832 Production 442 Transportation and Material Moving 2722 Military Specific 2 dtype: int64","title":"Filtering"},{"location":"Skills-ML Tour/#random-sampling","text":"Even though we have a lot of data, most of time we don't need all of them to do the analysis. Or we can't even fit all the data into memory to do the analysis. What we need more importantly is a suitable sampled dataset. JobSampler : Sample job posting by (weighted) reservoir sampling.","title":"Random Sampling"},{"location":"Skills-ML Tour/#random-sampling-from-streaming-data-reservoir-sampling","text":"\"Say you have a stream of items of large and unknown length that we can only iterate over once.\" It's memeory efficient and just one iteration There is a great overview of reservoir sampling in https://gregable.com/2007/10/reservoir-sampling.html. Let's say the original job postings dataset are too much for my Mac Yosemite to do any analysis and I want only 1000 job postings but still preserve the statistical characteristics of the original dataset. from skills_ml.job_postings.sample import JobSampler sampler = JobSampler( job_posting_generator=jobpostings_filtered, k=1000, ) plot_major_group_distribution(sampler) Management 188 Business and Financial Operations 119 Computer and Mathematical 158 Architecture and Engineering 61 Life, Physical, and Social Science 3 Community and Social Service 10 Legal 2 Education, Training, and Library 12 Arts, Design, Entertainment, Sports, and Media 12 Healthcare Practitioners and Technical 112 Healthcare Support 12 Protective Service 11 Food Preparation and Serving Related 30 Building and Grounds Cleaning and Maintenance 7 Personal Care and Service 4 Sales and Related 54 Office and Administrative Support 82 Construction and Extraction 8 Installation, Maintenance, and Repair 26 Production 16 Transportation and Material Moving 73 dtype: int64 Something wrong happened! We are missing Military Occupations ! Because military job postings are extremely rare in the original dataset, simple ramdom sampling might result in lack of classes.","title":"Random Sampling from Streaming Data - Reservoir Sampling"},{"location":"Skills-ML Tour/#weighted-reservoir-sampling","text":"How would you sample from a weighted distribution where each element has a given weight associated with it in the stream? For certain task, we need some curated sample. For example, if we want to build a occupation classifier, we want similar amounts of job posting for each occupation. Now we want to have a more uniform distributed sample across all major groups. Here we need to provide a weight dictionary in the JobSampler c = Counter() for job in jobpostings_filtered: c.update([job['onet_soc_code'][:2]]) weights = dict() for key, value in c.items(): weights[key] = max(c.values()) / value weights {'11': 1.0, '15': 1.2845014807502468, '17': 3.3588022715539494, '29': 1.8874383521903104, '41': 4.597879858657244, '43': 2.521705426356589, '13': 1.6824411688647531, '49': 7.819711538461538, '33': 13.442148760330578, '27': 10.879598662207357, '47': 33.19387755102041, '51': 14.71945701357466, '35': 8.214646464646465, '25': 9.581737849779087, '31': 13.17004048582996, '19': 33.885416666666664, '21': 23.070921985815602, '37': 34.423280423280424, '53': 2.3901542983100663, '39': 67.0721649484536, '23': 69.2127659574468, '55': 3253.0} sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=1000, key=lambda x: x['onet_soc_code'][:2], weights=weights) plot_major_group_distribution(sampler) Management 41 Business and Financial Operations 51 Computer and Mathematical 49 Architecture and Engineering 55 Life, Physical, and Social Science 49 Community and Social Service 42 Legal 39 Education, Training, and Library 56 Arts, Design, Entertainment, Sports, and Media 54 Healthcare Practitioners and Technical 55 Healthcare Support 45 Protective Service 57 Food Preparation and Serving Related 52 Building and Grounds Cleaning and Maintenance 44 Personal Care and Service 37 Sales and Related 48 Office and Administrative Support 48 Construction and Extraction 38 Installation, Maintenance, and Repair 38 Production 51 Transportation and Material Moving 49 Military Specific 2 dtype: int64","title":"Weighted Reservoir Sampling"},{"location":"Skills-ML Tour/#skill-extraction","text":"A common task is extracting competencies from unstructured text. Sometimes this is ontology-based (finding concepts from a known ontology in text), but this is not necessarily true. Skills-ML unites these with a common interface in the SkillExtractor class. The common interface is that every SkillExtractor needs to be able to take in a collection of documents, and yield what we call CandidateSkill objects.","title":"Skill Extraction"},{"location":"Skills-ML Tour/#what-is-a-candidateskill","text":"A CandidateSkill is a possible occurrence of a skill/competency in context in some document. It consists of the following fields: skill_name - The text version of the skill as it appears in the document matched_skill_identifier - A reference to the skill in some ontology. This may be empty, if no ontology was used to search for skills. context - The text surrounding the skill in the document. The goal is for a human labeler to be able to use this to determine whether or not the occurrence represents a true skill. How much context is included is up to the algorithm. start_index - The start index of the skill occurrence within the document string. confidence - The confidence level the algorithm has in this candidate skill being a true occurrence of a skill. This may be empty, if the algorithm has now way of producing a confidence value. document_id - A unique identifier for the source document. document_type - The type of document (examples: Job Posting, Profile, Course Description) source_object - The entire source document. skill_extractor_name - The name of the skill extractor algorithm. Every SkillExtractor subclass defines a name property that is used to give processes downstream context about how their output data was produced. The idea behind the CandidateSkill object is to serve as a common interface between SkillExtractor objects, automatic evaluation methods, and manual evaluation methods. A labeling interface might intake CandidateSkill objects for humans to say yes/no to. Another type of labeling interface might involve the export of CandidateSkill objects based on what a human highlighted in the interface when shown the entire document Unsupervised evaluation metrics may take in one set of CandidateSkills to produce simple descriptive metrics Supervised evaluation metrics may take in one set of CandidateSkills from a SkillExtractor and another set of CandidateSkills from a human labeling interface and use the latter to evaluate the former We'll talk about some of these use cases in more detail later. But for now, let's start with a simple example that uses NLP rules and isn't tied to an ontology. Let's define a method for extracting skills as 'all noun phrases that end in the word skill or skills'. This is a simple method that realistically won't cover all possible occurrences of skills, but this is a start. from skills_ml.algorithms.skill_extractors import SkillEndingPatternExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample job_posting_generator = JobPostingCollectionSample() # instantiate the skill extractor. This class defaults to only considering lines that # start with a bullet, which doesn't work for this dataset. So we set this flag to False. skill_extractor = SkillEndingPatternExtractor(only_bulleted_lines=False) job_posting = next(iter(job_posting_generator)) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: communication skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: organizational skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: communication skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required skill name: organizational skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required The results for one job posting are modest. Two distinct skill names, each occurring two different times in the document. This is a start. Now let's try another skill extractor: matching with ONET data. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: self context: The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric. skill name: self context: We are proud to be an equal opportunity employer College degree preferred, 2-5 years experience in print and/or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients/prospects is required, Minimal non local travel a few times a year is required Yikes. What is this? As it turns out, 'Self' is a real programming language . But it's not applicable here. Simply searching for skill names has its limitations. To help with this, there is also the SocScopedExactMatchSkillExtractor. This does exact matching, but only for the occupation that the document is tagged with. This, of course, is only applicable if the document has one. And it needs a full CompetencyOntology to work. from skills_ml.algorithms.skill_extractors import SocScopedExactMatchSkillExtractor skill_extractor = SocScopedExactMatchSkillExtractor(onet) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') No results. This is expected: For an occupation that is not related to computer programming, the language 'Self' is likely irrelevant. Here's a list of all the other skill extractors available. FuzzyMatchSkillExtractor - Similar to the ExactMatchSkillExtractor, but using a configurable edit distance to find skill names that are very close to the targets. AbilityEndingPatternExtractor - Similar to the SkillEndingPatternExtractor, but finding noun phrases that end in 'ability' or 'abilities'. SectionExtractSkillExtractor - Attempts to divide the text into sections with headers, which is a common pattern found in job postings. Return each individual sentence found in sections with certain headers (Skills, Competencies, Qualifications).","title":"What Is a CandidateSkill?"},{"location":"Skills-ML Tour/#evaluating-skill-extractors","text":"We want to be able to evaluate skill extractors. We may or may not have labeled skills but do want to be able to generate descriptive metrics either way. from skills_ml.evaluation.skill_extraction_metrics import TotalOccurrences, TotalVocabularySize, OntologyCompetencyRecall metrics = [ TotalOccurrences(), TotalVocabularySize(), OntologyCompetencyRecall(onet) ] exact_match_skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for metric in metrics: candidate_skills = [] for job_posting in job_posting_generator: candidate_skills += list(exact_match_skill_extractor.candidate_skills(job_posting)) print('metric:', metric.name, 'value:', metric.eval(candidate_skills, 50)) metric: total_candidate_skills value: 153 metric: total_vocabulary_size value: 40 metric: onet_ksat_competency_recall value: 0.001248868213181804","title":"Evaluating Skill Extractors"},{"location":"Skills-ML Tour/#embedding","text":"Labor market data tends to be large in scale, but represented as raw text. Consequently, an important early step for most tasks is to transform texts into a mathematical form that can be used in the downstream tasks. In the context of skills and jobs, an embedding model trained on large amount of job posting data is able to map a skill or a job title into a high dimensional space as well as preserving the contextual and semantic relationship. Ideally, a good embedding model will cluster similar skills and jobs.","title":"Embedding"},{"location":"Skills-ML Tour/#embedding-models","text":"Many word embedding techniques have been developed since the most impactful embedding algorithm word2vec was published in 2013. Currently, Skills-ML includes word2vec, doc2vec and fastext and may include more in the future. Word2VecModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, a random vector will be assigned with the same dimension. Doc2VecModel is able to look up a word vector and infer a sentence/paragraph vector by gradient descending on the fly, so it is non-deterministic. It does not support online learning. FastTextModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, it sums all vectors of the unseen word\u2019s char-ngrams. If none of the char-ngrams of the unseen word is present, a random vector will be assigned with the same dimension. from skills_ml.algorithms.embedding.models import Word2VecModel, FastTextModel cbow = Word2VecModel(size=200, sg=0, window=7, iter=3, batch_words=1000) skip_gram = Word2VecModel(size=200, sg=1, window=7, iter=3, batch_words=1000) fasttext = FastTextModel(size=200, window=7, iter=3, batch_words=1000)","title":"Embedding Models"},{"location":"Skills-ML Tour/#corpora","text":"Next, we need some text corpus to train embedding modelss. Skills-ML provides pre-defined classes to convert common schema job listings into a corpus in documnet level suitable for use by machine learning algorithms or specific tasks. Word2VecGensimCorpusCreator Doc2VecGensimCorpusCreator from skills_ml.job_postings.corpora import Word2VecGensimCorpusCreator, Doc2VecGensimCorpusCreator sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) w2v_corpus_generator = Word2VecGensimCorpusCreator(sampler)","title":"Corpora"},{"location":"Skills-ML Tour/#preprocessing","text":"Or we can build our own corpus generator by using some preprocessing tools Function Compostition - ProcessingPipeline will compose processing functions together to become a callable object that takes in the input from the very first processing function and returns the output of the last processing function. - IterablePipeline will compose processing functions together to be passed to different stages(training/ prediction) from skills_ml.algorithms.preprocessing import IterablePipeline from skills_ml.algorithms import nlp from functools import partial document_schema_fields = ['description','experienceRequirements', 'qualifications', 'skills'] pipeline = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_html, nlp.clean_str, nlp.word_tokenize, ) corpus_generator = pipeline(sampler)","title":"Preprocessing"},{"location":"Skills-ML Tour/#train-embedding","text":"The EmbeddingTrainer provides online batch learning for Word2VecModel and FastTextModel. from skills_ml.algorithms.embedding.train import EmbeddingTrainer trainer = EmbeddingTrainer(cbow, skip_gram, fasttext, batch_size=100) trainer.train(corpus_generator) WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles","title":"Train Embedding"},{"location":"Skills-ML Tour/#storage","text":"Skills-ML has couple useful storage classes that could benefit both local or cloud. - S3Store : S3 storage engine - FSStore : File system storage engine - ModelStorage : Serialization model storage. from skills_ml.storage import FSStore, S3Store, ModelStorage fs = FSStore(path=\"tmp/model_cache/embedding/examples\") trainer.save_model(storage=fs) print(cbow.model_name) print(cbow.storage) word2vec_9d094b1f6b2610e530e14e5a71884be3.model FSStore(path=tmp/model_cache/embedding/examples)","title":"Storage"},{"location":"Skills-ML Tour/#examples","text":"for c, s in zip(cbow.wv.most_similar(['engineer']), skip_gram.wv.most_similar(['engineer'])): print(c, s) ('developer', 0.8033339977264404) ('sr', 0.623277485370636) ('sr', 0.7680195569992065) ('architect', 0.5734556317329407) ('analyst', 0.724011242389679) ('developer', 0.5714775919914246) ('designer', 0.7087661027908325) ('writer', 0.5478475093841553) ('architect', 0.6725302934646606) ('soc', 0.5414971709251404) ('mclean', 0.6457548141479492) ('herndon', 0.5357624292373657) ('embedded', 0.6317166090011597) ('tester', 0.532582700252533) ('inc', 0.6310484409332275) ('isso', 0.5319244861602783) ('scientist', 0.62971031665802) ('analyst', 0.5278465747833252) ('springfield', 0.6180558800697327) ('editor', 0.5252323746681213) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. if np.issubdtype(vec.dtype, np.int): for c, s in zip(cbow.wv.most_similar(['python']), skip_gram.wv.most_similar(['python'])): print(c, s) ('scripting', 0.9461991190910339) ('ruby', 0.9052647948265076) ('java', 0.9233536720275879) ('perl', 0.8700193762779236) ('ruby', 0.9213511943817139) ('js', 0.8653509616851807) ('unix', 0.9152072668075562) ('selenium', 0.8345634937286377) ('languages', 0.9030321836471558) ('git', 0.8201907277107239) ('javascript', 0.8950035572052002) ('subversion', 0.8199359774589539) ('linux', 0.8872419595718384) ('javascript', 0.8152703046798706) ('programming', 0.8679823875427246) ('scripting', 0.8112311363220215) ('sql', 0.8587027788162231) ('unix', 0.8111978769302368) ('perl', 0.8509498238563538) ('languages', 0.8040737509727478) for c, s in zip(cbow.wv.most_similar(['cnc']), skip_gram.wv.most_similar(['cnc'])): print(c, s) ('machine', 0.7754281759262085) ('hws', 0.7544339895248413) ('machines', 0.730404257774353) ('fabrication', 0.743915319442749) ('circuits', 0.7134151458740234) ('machining', 0.7412827014923096) ('hydraulic', 0.7047945857048035) ('controllers', 0.7408922910690308) ('machining', 0.7047876715660095) ('turbines', 0.734102725982666) ('pumps', 0.6980614066123962) ('overhauling', 0.7285677194595337) ('metal', 0.6945027112960815) ('rebuilding', 0.7274059057235718) ('electric', 0.6921502351760864) ('bearings', 0.7227500677108765) ('assemblies', 0.6852962970733643) ('turbine', 0.7216542959213257) ('machinery', 0.67722487449646) ('harpoon', 0.7176830172538757) Skills-ML also provides a function to visualize the embedding in tensorboard from skills_ml.algorithms.embedding.models import visualize_in_tensorboard visualize_in_tensorboard(cbow) Run `tensorboard --logdir=/home/ubuntu/skills-ml/word2vec_9d094b1f6b2610e530e14e5a71884be3 --host 127.0.0.1` to run visualize result on tensorboard","title":"Examples"},{"location":"Skills-ML Tour/#evaluation","text":"Although there is an emerging trend towards generating embeddings for structured and unstructured data, there is not yet any systematic suite for measuring the quality of embeddings. We generally follow one of the few works in embedding evaluation [Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts] to create metrics for evaluating embedding against the gold standard ontology dataset. The gold standard ontology is curated by domain experts like O*NET, so a good embedding should replicate the structure of the entities in the gold standard taxonomy. In other words, it is useful to see how an embedding reflects the clustering structure. One trivial clustering is Major Groups of occupations. A good embedding should cluster the occupations which belong to the same major groups. CategorizationMetric : The cosine similarity between the embedding of the concept and the mean vector of embeddings of all the entities within that concept cluster. This metric aligns a clustering of entities into different categories, reflecting how well the embedding of a concept cluster performs as the background concept of the entities typed by it. IntraClusterCohesion : The sum of squared error of the embedding of the centroid of the concept cluster and the embedding of each entities within that cluster. It measures how near the data points in a cluster are to the cluster centroid. MajorGroupRecall : For a major group, calculate the cosine similarity against all the occupations and find the top n closest occupations. The recall is defined as the number of true positives from top n closest occupations divided by the total number of occupation within the major group. MajorGroupPrecision : Similarly to MajorGroupRecall which is called Coherence Score in the paper, start by finding the top n closest occupations. The precision is defined as the number of true positives from top n closest occupations divided by n from skills_ml.ontologies.onet import Onet major_group_occupation_des_clustering = onet.major_group_occupation_description_clustering from skills_ml.evaluation.embedding_metrics import metrics_for_embedding, CategorizationMetric, IntraClusterCohesion, RecallTopN, PrecisionTopN from skills_ml.algorithms.preprocessing import ProcessingPipeline def vectorization(embedding): p = ProcessingPipeline( nlp.normalize, nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=embedding) ) return p categorization_metric = CategorizationMetric(major_group_occupation_des_clustering) intra_cohesion = IntraClusterCohesion(major_group_occupation_des_clustering) recall_top = RecallTopN(major_group_occupation_des_clustering, topn=10) precision_top = PrecisionTopN(major_group_occupation_des_clustering, topn=10) categorization_metric.eval(vectorization(fasttext)) {'Life, Physical, and Social Science': 0.544181802616692, 'Education, Training, and Library': 0.39010462667236234, 'Legal': 0.44299323639279864, 'Protective Service': 0.8338733121787928, 'Personal Care and Service': 0.5520270184970673, 'Military Specific': 0.8403838778749896, 'Arts, Design, Entertainment, Sports, and Media': 0.41337371196897577, 'Business and Financial Operations': 0.33068055201077906, 'Installation, Maintenance, and Repair': 0.18600172293592943, 'Community and Social Service': 0.3938874349835806, 'Food Preparation and Serving Related': 0.2854292068104062, 'Farming, Fishing, and Forestry': 0.49009970825435945, 'Management': 0.6667720141362156, 'Architecture and Engineering': 0.31587733273288865, 'Transportation and Material Moving': 0.3438685579518763, 'Healthcare Practitioners and Technical': 0.3313605878282969, 'Production': 0.7619768121213049, 'Computer and Mathematical': 0.2723307384580105, 'Sales and Related': 0.29548456080419716, 'Office and Administrative Support': 0.5946786302511617, 'Construction and Extraction': 0.46874949551031586, 'Healthcare Support': 0.9223879488399899, 'Building and Grounds Cleaning and Maintenance': 0.32908847148448117} import statistics import operator from collections import defaultdict # We define some helper functions to evaluate multiple embeddings def algorithm_name(emb): if emb.model_type == 'word2vec' or emb.model_type == 'fasttext': if getattr(emb, 'sg', None) == 1: return 'Skip-Gram' else: return 'Continuous Bag of Words' elif emb.model_type == 'doc2vec': if getattr(emb, 'dm', None) == 1: return 'Distributed Memory' else: return 'Distributed Bag of Words' def evaluate_multiple_embeddings(embeddings, vectorization, metric): result = defaultdict(dict) for emb in embeddings: c = metric.eval(vectorization(emb)) name = emb.model_name.split('.')[0] result[name]['mean'] = statistics.mean(list(c.values())) result[name]['variance'] = statistics.variance(list(c.values())) result[name]['std'] = statistics.stdev(list(c.values())) result[name]['max'] = max(c.items(), key=operator.itemgetter(1))[1] result[name]['max_cluster'] = max(c.items(), key=operator.itemgetter(1))[0] result[name]['min'] = min(c.items(), key=operator.itemgetter(1))[1] result[name]['min_cluster'] = min(c.items(), key=operator.itemgetter(1))[0] result[name]['type'] = emb.model_type result[name]['algorithm'] = algorithm_name(emb) result[name]['window'] = emb.window return pd.DataFrame(result) evaluate_multiple_embeddings([cbow, skip_gram, fasttext], vectorization, categorization_metric) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word2vec_9d094b1f6b2610e530e14e5a71884be3 word2vec_b6aebc90db420cbb071d7e78e3158e51 fasttext_7d92bf3d9781f057978d909bed7e7f04 algorithm Continuous Bag of Words Skip-Gram Continuous Bag of Words max 0.923396 0.64388 0.922388 max_cluster Healthcare Support Production Healthcare Support mean 0.49752 0.34426 0.478505 min 0.161117 0.181937 0.186002 min_cluster Installation, Maintenance, and Repair Food Preparation and Serving Related Installation, Maintenance, and Repair std 0.221347 0.122163 0.204855 type word2vec word2vec fasttext variance 0.0489945 0.0149237 0.0419656 window 7 7 7","title":"Evaluation"},{"location":"Skills-ML Tour/#occupation-classification","text":"A common issue with job posting data is incomplete, incorrect, and inconsistent occupation classification. The majority of job postings in the US are using the O*NET SOC classification system, but many are either missing or poorly classified. This can be improved by using machine learning.","title":"Occupation Classification"},{"location":"Skills-ML Tour/#soc-codes","text":"Most of the job posting data collected are aligned with the O*NET SOC system. The occupations in the SOC are classified at four levels of aggregation: major group, minor group, broad occupation, and detailed occupation. Each lower level of detail identifies a more specific group of occupations. Each item in the SOC is designated by a six-digit code. The first two digits represent the major group, the third digit represents the minor group, the fourth and fifth digits represent the broad occupation, and the sixth digit represents the detailed occupation. - Major group codes end with 0000 (e.g., 29-0000 Healthcare Practitioners and Technical Occupations \u2014the exceptions are minor groups 15-1200 Computer Occupations, 31- 1100 Home Health and Personal Care Aides; and Nursing Assistants, Orderlies, and Psychiatric Aides, and 51-5100 Printing Workers, which end with 00). - Minor groups generally end with 000 (e.g., 29-1000 Health Diagnosing or Treating Practitioners). - Broad occupations end with 0 (e.g., 29-1020 Dentists). - Detailed occupations end with a number other than 0 (e.g., 29-1022 Oral and Maxillofacial Surgeons).","title":"SOC Codes"},{"location":"Skills-ML Tour/#target-variable","text":"FullSOC SOCMajorGroup from skills_ml.algorithms.occupation_classifiers import FullSOC, SOCMajorGroup full_soc = FullSOC(onet_cache=onet)","title":"Target Variable"},{"location":"Skills-ML Tour/#design-matrix","text":"The classification task consists of inferring a SOC code from a job posting and is accomplished through several stages: preprocessing, filtering, training and testing. DesignMatrix helps users accomplish this task. import random from itertools import islice from skills_ml.utils import itershuffle from skills_ml.algorithms.occupation_classifiers import DesignMatrix sample = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) dataset = itershuffle(sample) train = islice(dataset, 0, 4000) test = islice(dataset, 4000) pipe_x = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=fasttext) ) pipe_y = IterablePipeline( full_soc.transformer ) matrix = DesignMatrix( train, full_soc, pipe_x, pipe_y, )","title":"Design Matrix"},{"location":"Skills-ML Tour/#occupationclassifiertrainer","text":"OccupationClassifierTrainer trains classifiers with cross validation and picks the best classifier with a grid search based on the metric. It takes in a dictionary for the grid search. from skills_ml.algorithms.occupation_classifiers.train import OccupationClassifierTrainer grid_config = { 'sklearn.ensemble.ExtraTreesClassifier': { 'n_estimators': [50, 100], 'criterion': ['entropy', 'gini'], 'max_depth': [20], 'max_features': ['log2'], 'min_samples_split': [10] }, 'sklearn.neural_network.MLPClassifier': { 'hidden_layer_sizes': [100, 500], 'activation': ['logistic', 'relu'], 'solver': ['adam'] }, } cls_trainer = OccupationClassifierTrainer( matrix=matrix, k_folds=3, grid_config=grid_config, storage=FSStore('tmp/model_cache/soc_classifiers/examples'), n_jobs=4) cls_trainer.train(save=False) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1d /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) cls_trainer.best_estimators [<ProxyObjectWithStorage at 0x7f2f1c1bfb48 for GridSearchCV at 0x7f2f19160278>, <ProxyObjectWithStorage at 0x7f2f1bfb2eb8 for GridSearchCV at 0x7f2f192ea0b8>]","title":"OccupationClassifierTrainer"},{"location":"Skills-ML Tour/#evaluation_1","text":"Accuracy, recall, precision and f1 are the metrics taken into consideration. Since it is a multi-class classification problem, an overall performance is evaluated by looking at the micro-average and macro-average for the metrics. A macro-average will compute the metric independently for each class and then take the average, whereas a micro-average will aggregate the contributions of all classes and then computes the average. In other words, a macro-average is treating all classes equally. from skills_ml.algorithms.occupation_classifiers.test import OccupationClassifierTester from skills_ml.evaluation.occ_cls_evaluator import OnetOccupationClassificationEvaluator from skills_ml.algorithms.occupation_classifiers.classifiers import CombinedClassifier from skills_ml.algorithms.embedding.train import Reiterable steps = [ partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.normalize, nlp.clean_str, nlp.word_tokenize, ] evaluators = [] test_data = list(test) for cls in cls_trainer.best_estimators: tester = OccupationClassifierTester( test_data_generator=test_data, preprocessing=steps, classifier=CombinedClassifier(fasttext, cls) ) evaluators.append(OnetOccupationClassificationEvaluator(tester)) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'itershuffle' raised StopIteration for e, c in zip(evaluators, cls_trainer.best_estimators): print(c.best_estimator_) print('accuracy: ', e.accuracy) print('precision: ', e.precision) print('f1: ', e.f1) print('major group: ', e.accuracy_major_group) print('macro precision: ', e.macro_precision) print('micro precision: ', e.micro_precision) print('recall: ', e.recall) print('macro recall: ', e.macro_recall) print('micro recall: ', e.micro_recall) print('macro f1: ', e.macro_f1) print('micro f1: ', e.micro_f1) print('\\n') ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini', max_depth=20, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4, oob_score=False, random_state=None, verbose=0, warm_start=False) accuracy: 0.483 precision: [0.09090909 0. 0.07894737 ... 0. 0. 0. ] f1: [0.15384615 0. 0.12244898 ... 0. 0. 0. ] major group: 0.547 macro precision: 0.3822259316025384 micro precision: 0.483 recall: [0.5 0. 0.27272727 ... 0. 0. 0. ] macro recall: 0.3180345574223314 micro recall: 0.483 macro f1: 0.3258889572669588 micro f1: 0.483 MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=500, learning_rate='constant', learning_rate_init=0.001, max_iter=200, momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) accuracy: 0.51 precision: [0. 0. 0.1 ... 0. 0. 0. ] f1: [0. 0. 0.11538462 ... 0. 0. 0. ] major group: 0.587 macro precision: 0.3392587829678509 micro precision: 0.51 recall: [0. 0. 0.13636364 ... 0. 0. 0. ] macro recall: 0.3423782550302827 micro recall: 0.51 macro f1: 0.32266097925849724 micro f1: 0.51 /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for)","title":"Evaluation"},{"location":"algorithms.aggregators/","text":"skills_ml.algorithms.aggregators SkillAggregator SkillAggregator(self, skill_extractor, corpus_creator, *args, **kwargs) Aggregates skills found in job postings Args: skill_extractor (.skill_extractors.FreetextSkillExtractor) an object that returns skill counts from unstructured text corpus creator (object) an object that returns a text corpus from a job posting OccupationScopedSkillAggregator OccupationScopedSkillAggregator(self, skill_extractor, corpus_creator, *args, **kwargs) Aggregates skills found in job postings using the job's occupation Args: skill_extractor (.skill_extractors.FreetextSkillExtractor) an object that returns skill counts from unstructured text corpus creator (object) an object that returns a text corpus from a job posting SocCodeAggregator SocCodeAggregator(self, occupation_classifier, corpus_creator, *args, **kwargs) Aggregates SOC codes inferred from job posting text Args: occupation_classifier (.occupation_classifiers.SocClassifier) An object that returns a classified SOC code and similarity score from unstructured text corpus creator (object) an object that returns unstructured text from a job posting GivenSocCodeAggregator GivenSocCodeAggregator(self, output_count=1, output_total=False) Aggregates SOC codes given as an input field Caution! We may or may not know where these came from, and the method for creating them may differ from record to record CountAggregator CountAggregator(self, output_count=1, output_total=False) Counts job postings","title":"Algorithms.aggregators"},{"location":"algorithms.corpus_creators/","text":"skills_ml.algorithms.corpus_creators","title":"Algorithms.corpus creators"},{"location":"algorithms/","text":"skills_ml.algorithms Core algorithmic components Each subdirectory is meant to contain a different type of component, such as a job title normalizer or a skill tagger, with a common interface so different pipelines can try out different versions of the components.","title":"Algorithms"},{"location":"algorithms.skill_extractors/","text":"skills_ml.algorithms.skill_extractors Test docstring skills_ml.algorithms.skill_extractors.base","title":"Algorithms.skill extractors"},{"location":"api/","text":"skills_ml.algorithms Core algorithmic components Each subdirectory is meant to contain a different type of component, such as a job title normalizer or a skill tagger, with a common interface so different pipelines can try out different versions of the components. skills_ml.datasets Functions and classes to interface with various datasets skills_ml.evaluation","title":"Api"},{"location":"common_schema/","text":"Job Posting Common Schema \u00b6 skills-ml makes heavy use of schema.org's job posting schema, generally stored in JSON format. Here is an example: { \"incentiveCompensation\": \"\", \"experienceRequirements\": \"Here are some experience and requirements\", \"baseSalary\": {\"maxValue\": 0.0, \"@type\": \"MonetaryAmount\", \"minValue\": 0.0}, \"description\": \"We are looking for a person to fill this job\", \"title\": \"Bilingual (Italian) Customer Service Rep (Work from Home)\", \"employmentType\": \"Full-Time\", \"industry\": \"Call Center / SSO / BPO, Consulting, Sales - Marketing\", \"occupationalCategory\": \"\", \"qualifications\": \"Here are some qualifications\", \"educationRequirements\": \"Not Specified\", \"skills\": \"Customer Service, Consultant, Entry Level\", \"validThrough\": \"2014-02-05T00:00:00\", \"jobLocation\": {\"@type\": \"Place\", \"address\": {\"addressLocality\": \"Salisbury\", \"addressRegion\": \"PA\", \"@type\": \"PostalAddress\"}}, \"@context\": \"http://schema.org\", \"alternateName\": \"Customer Service Representative\", \"datePosted\": \"2013-03-07\", \"@type\": \"JobPosting\" }","title":"Job Posting Common Schema"},{"location":"common_schema/#job-posting-common-schema","text":"skills-ml makes heavy use of schema.org's job posting schema, generally stored in JSON format. Here is an example: { \"incentiveCompensation\": \"\", \"experienceRequirements\": \"Here are some experience and requirements\", \"baseSalary\": {\"maxValue\": 0.0, \"@type\": \"MonetaryAmount\", \"minValue\": 0.0}, \"description\": \"We are looking for a person to fill this job\", \"title\": \"Bilingual (Italian) Customer Service Rep (Work from Home)\", \"employmentType\": \"Full-Time\", \"industry\": \"Call Center / SSO / BPO, Consulting, Sales - Marketing\", \"occupationalCategory\": \"\", \"qualifications\": \"Here are some qualifications\", \"educationRequirements\": \"Not Specified\", \"skills\": \"Customer Service, Consultant, Entry Level\", \"validThrough\": \"2014-02-05T00:00:00\", \"jobLocation\": {\"@type\": \"Place\", \"address\": {\"addressLocality\": \"Salisbury\", \"addressRegion\": \"PA\", \"@type\": \"PostalAddress\"}}, \"@context\": \"http://schema.org\", \"alternateName\": \"Customer Service Representative\", \"datePosted\": \"2013-03-07\", \"@type\": \"JobPosting\" }","title":"Job Posting Common Schema"},{"location":"datasets/","text":"skills_ml.datasets Functions and classes to interface with various datasets skills_ml.datasets.sba_city_county Retrieve county lookup tables from the SBA for each state county_lookup county_lookup() Retrieve county lookup tables if they are not already cached Returns: dict each key is a state, each value is a dict {city_name: (fips_county_code, county_name)} skills_ml.datasets.nber_county_cbsa Retrieve county->CBSA crosswalk file from the NBER cbsa_lookup cbsa_lookup() Construct a County->CBSA Lookup table from NBER data Returns: dict each key is a (State Code, County FIPS code) tuple each value is a (CBSA FIPS code, CBSA Name) tuple","title":"Datasets"},{"location":"evaluation/","text":"skills_ml.evaluation","title":"Evaluation"},{"location":"examples/","text":"Usage Examples \u00b6 Corpus Creator with Sampling and Filtering \u00b6 To showcase the corpus creator and its options, we generate a few different job postings corpora: a simple one from a single quarter's worth of data filtered on different fields like SOC code and base salary reservoir-sampled versions of each of the above Extracting Skills using Noun Phrase Endings \u00b6 To showcase the noun phrase skill extractor, we download open job postings from Virginia Tech's open data portal and run them through the skill extractor. In the end, we have the most commonly occurring noun phrases ending in 'skill' or 'skills'. Skill Extraction and Evaluation Loop \u00b6 To showcase how skill extraction algorithms can be tested, we run extraction several times with different parameters: Skill extraction algorithms (exact, fuzzy matching) Base ontologies, consisting of ONET subsetted to Abilities, Skills, Knowledge) Metrics (Total Vocabulary Size, Total Candidate Skills, Recall of Given Ontology) Train an Word2Vec Embedding Model using Quarterly Jobposting Data \u00b6 To showcase the interface of training a word2vec embedding model in an online batch learning fashion: A list of quarters for creating the corpus from job posting data A trainer object that specifies some parameters of source, s3 path, batch size, model type ...etc. The train method takes whatever arugments gensim.models.word2vec.Word2Vec or gensim.model.doc2vec.Doc2Vec has Compute and Aggregate Properties of Job Postings as a Tabular Dataset \u00b6 To show job posting property computation and aggregation, we calculate job posting counts by cleaned title, and upload the resulting CSV to S3. This is essentially a mini version of the Data@Work Research Hub. To enable this example to be run with as few dependencies as possible, we use: a fake local s3 instance a sample of the Virginia Tech open job postings dataset only title cleaning and job counting. Train an Occupation Classifier with Sample Jobposting Data \u00b6 To showcase how occupation classifier can be trained using grid search and cross-validation: A sample of the Virginia Tech open job postings dataset An embedding model that is trained already The pipeline objects that takes in all steps including filters, transformation, tokenization and vectorization A config dictionary for grid search A matrix object that specifies the data source, target variable, pipelines An occupation classifier trainer object that specifies input matrix, number of folds, grid search config, storage and number of workers","title":"Examples"},{"location":"examples/#usage-examples","text":"","title":"Usage Examples"},{"location":"examples/#corpus-creator-with-sampling-and-filtering","text":"To showcase the corpus creator and its options, we generate a few different job postings corpora: a simple one from a single quarter's worth of data filtered on different fields like SOC code and base salary reservoir-sampled versions of each of the above","title":"Corpus Creator with Sampling and Filtering"},{"location":"examples/#extracting-skills-using-noun-phrase-endings","text":"To showcase the noun phrase skill extractor, we download open job postings from Virginia Tech's open data portal and run them through the skill extractor. In the end, we have the most commonly occurring noun phrases ending in 'skill' or 'skills'.","title":"Extracting Skills using Noun Phrase Endings"},{"location":"examples/#skill-extraction-and-evaluation-loop","text":"To showcase how skill extraction algorithms can be tested, we run extraction several times with different parameters: Skill extraction algorithms (exact, fuzzy matching) Base ontologies, consisting of ONET subsetted to Abilities, Skills, Knowledge) Metrics (Total Vocabulary Size, Total Candidate Skills, Recall of Given Ontology)","title":"Skill Extraction and Evaluation Loop"},{"location":"examples/#train-an-word2vec-embedding-model-using-quarterly-jobposting-data","text":"To showcase the interface of training a word2vec embedding model in an online batch learning fashion: A list of quarters for creating the corpus from job posting data A trainer object that specifies some parameters of source, s3 path, batch size, model type ...etc. The train method takes whatever arugments gensim.models.word2vec.Word2Vec or gensim.model.doc2vec.Doc2Vec has","title":"Train an Word2Vec Embedding Model using Quarterly Jobposting Data"},{"location":"examples/#compute-and-aggregate-properties-of-job-postings-as-a-tabular-dataset","text":"To show job posting property computation and aggregation, we calculate job posting counts by cleaned title, and upload the resulting CSV to S3. This is essentially a mini version of the Data@Work Research Hub. To enable this example to be run with as few dependencies as possible, we use: a fake local s3 instance a sample of the Virginia Tech open job postings dataset only title cleaning and job counting.","title":"Compute and Aggregate Properties of Job Postings as a Tabular Dataset"},{"location":"examples/#train-an-occupation-classifier-with-sample-jobposting-data","text":"To showcase how occupation classifier can be trained using grid search and cross-validation: A sample of the Virginia Tech open job postings dataset An embedding model that is trained already The pipeline objects that takes in all steps including filters, transformation, tokenization and vectorization A config dictionary for grid search A matrix object that specifies the data source, target variable, pipelines An occupation classifier trainer object that specifies input matrix, number of folds, grid search config, storage and number of workers","title":"Train an Occupation Classifier with Sample Jobposting Data"},{"location":"ontologies/","text":"Working With Ontologies \u00b6 skills-ml is introducing the CompetencyOntology class, for a rich, flexible representation of competencies, occupations, and their relationships with each other. The CompetencyOntology class is backed by JSON-LD, and based on Credential Engine's CTDL-ASN format for Competencies . The goal is to be able to read in any CTDL-ASN framework and produce a CompetencyOntology object for use throughout the skills-ml library. Furthermore, skills-ml contains pre-mapped versions of open frameworks like ONET for use out of the box. Competency \u00b6 A competency, in the CTDL-ASN context, refers some knowledge, skill, or ability that a person can possess or learn. Each competency contains: A unique identifier within the ontology. If you're familiar with ONET, think the table of contents identifiers (e.g. '1.A.1.a.1') Some basic textual information: a name (e.g. Oral Comprehension) and/or description (e.g. 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'), , and maybe a general textual category (e.g. Ability) Associative information with other competencies. A basic example is a parent/child relationship, for instance ONET's definition of 'Oral Comprehension' as the child of another competency called 'Verbal Abilities'. CTDL-ASN encodes this using the 'hasChild' and 'isChildOf' properties, and this is used in skills-ml. There many other types of associations competencies can have with each other that the Competency class in skills-ml does not yet address, you can read more at the Credential Engine's definition ofCompetency . The Competency class tracks all of this. It can be created using either keyword arguments in the class' Constructor or through a class method that loads from JSON-LD. Basic Example \u00b6 Using Python Constructor from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) Using JSON-LD from skills_ml.ontologies import Competency dinosaur_riding = Competency.from_jsonld({ '@type': 'Competency', '@id': '12345', 'name': 'Dinosaur Riding', 'description': 'Using the back of a dinosaur for transportation' }) To aid in bi-directional searching, the Competency object is meant to include a parent/child relationshiop on both the parent and child objects. The add_parent and add_child methods modify both the parent and child objects to easily maintain this bi-directional relationship. Example parent/child relationship \u00b6 Using Python Constructor from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) extreme_transportation = Competency( identifier='123', name='Extreme Transportation', description='Comically dangerous forms of transportation' ) dinosaur_riding.add_parent(extreme_transportation) print(dinosaur_riding.parents) print(extreme_transportation.children) Using JSON-LD dinosaur_riding = Competency.from_jsonld({ '@type': 'Competency', '@id': '12345', 'name': 'Dinosaur Riding', 'description': 'Using the back of a dinosaur for transportation', 'isChildOf': [{'@type': 'Competency', '@id': '123'}] }) extreme_transportation = Competency.from_jsonld({ '@type': 'Competency', '@id': '123', 'name': 'Extreme Transportation', 'description': 'Comically dangerous forms of transportation', 'hasChild': [{'@type': 'Competency', '@id': '12345'}] Occupation \u00b6 An Occupation is a job or profession that a person can hold. CTDL-ASN does not define this, so skills-ml models the Occupation similarly to the Competency, albeit with far less detail. A unique identifier within the ontology. If you're familiar with ONET, think of an ONET SOC code (11-1011.00) Some basic textual information: a name (e.g. Civil Engineer), maybe a description. Associative information with other occupations. So far the only relationship modeled in skills-ml between occupations is a parent/child one, similarly to Competency. Going back to the ONET example, an occupation representing the major group (identifier 11) may be thought of as the parent of SOC code 11-1011.00. Basic Example \u00b6 Using Python Constructor from skills_ml.ontologies import Occupation dinosaur_rider = Occupation( identifier='9999', name='Dinosaur Rider', ) Using JSON-LD from skills_ml.ontologies import Occupation dinosaur_rider = Occupation.from_jsonld({ '@type': 'Occupation', '@id': '9999', 'name': 'Dinosaur Rider' }) CompetencyOccupationEdge \u00b6 A CompetencyOccupationEdge is simply a relationship between a Competency and an Occupation. Currently, tthere are no further properties defined on this edge, though this will likely change in the future. Basic Example \u00b6 Using Python Constructor from skills_ml.ontologies import CompetencyOccupationEdge CompetencyOccupationEdge( occupation=dinosaur_rider, competency=dinosaur_riding ) Using JSON-LD from skills_ml.ontologies import CompetencyOccupationEdge CompetencyOccupationEdge.from_jsonld({ '@type': 'CompetencyOccupationEdge', '@id': 'competency=12345;occupation=9999', 'competency': {'@type': 'Competency', '@id': '12345'}, 'occupation': {'@type': 'Occupation', '@id': '9999'} }) CompetencyFramework \u00b6 A CompetencyFramework represent a collection of competencies and some metadata about them. The identifiers for given Competencies are used to disambiguate between them. The metadata exists so any code that uses the CompetencyFramework object can pass on useful knowledge about the framework to its output. The metadata has only two pieces of data: - name: A machine-readable name. Should be in snake case (e.g. onet_ksat ) - description: A human-readable description. Basic Example \u00b6 Using Python Constructor from skills_ml.ontologies import Competency, CompetencyFramework framework = CompetencyFramework( name='Sample Framework', description='A few basic competencies', competencies=[ Competency(identifier='a', name='Organization'), Competency(identifier='b', name='Communication Skills'), Competency(identifier='c', name='Cooking') ] ) CompetencyOntology \u00b6 An ontology represents a collection of competencies, a collection of occupations, and a collection of all relationships between competencies and occupations. The CompetencyOntology class represents each of these three collections using a set object. The identifiers for all of those objects are used to disambiguate between items in each of these sets. The JSON-LD representation of the ontology mirrors this internal structure. Below is an example of the objects defined above arranged into a CompetencyOntology. For brevity, the descriptions are omitted. Note in the Python example that importing the CompetencyOccupationEdge class is not necessary when using the Ontology; the add_edge method of Ontology can simply take a competency and occupation directly. Basic Example \u00b6 Using Python Constructor from skills_ml.ontologies import Competency, Occupation, CompetencyOntology ontology = CompetencyOntology( competency_name='caveman_games', competency_description='Competencies Useful to Characters in NES title Caveman Games' ) dinosaur_riding = Competency(identifier='12345', name='Dinosaur Riding') extreme_transportation = Competency(identifier='123', name='Extreme Transportation') dinosaur_riding.add_parent(extreme_transportation) dinosaur_rider = Occupation(identifier='9999', name='Dinosaur Rider') ontology.add_competency(dinosaur_riding) ontology.add_competency(extreme_transportation) ontology.add_occupation(dinosaur_rider) ontology.add_edge(occupation=dinosaur_rider, competency=dinosaur_riding) Using JSON-LD from skills_ml.ontologies import CompetencyOntology jsonld_string = \"\"\" 'name': 'test_ontology', 'competencies': [{ '@type': 'Competency', '@id': '12345', 'name': 'Dinosaur Riding', 'description': 'Using the back of a dinosaur for transportation', 'isChildOf': [{'@type': 'Competency', '@id': '123'}] }, { '@type': 'Competency', '@id': '123', 'name': 'Extreme Transportation', 'description': 'Comically dangerous forms of transportation', 'hasChild': [{'@type': 'Competency', '@id': '12345'}] }], 'occupations': [{ '@type': 'Occupation', '@id': '9999', 'name': 'Dinosaur Rider' }], 'edges': [{ '@type': 'CompetencyOccupationEdge', '@id': 'competency=12345;occupation=9999', 'competency': {'@type': 'Competency', '@id': '12345'}, 'occupation': {'@type': 'Occupation', '@id': '9999'} }] }\"\"\" ontology = CompetencyOntology(jsonld_string=jsonld_string) Using URL If you have access to a URL that contains compatible JSON-LD, you can send this URL right to the constructor. ontology = CompetencyOntology(url='https://myhost.com/ontology.json') Using Research Hub The Data@Work Research Hub hosts a number of ontologies publicly. The Research Hub ontology base URL is bundled into Skills-ML, so you can also just initialize one with the saved name on the Research Hub. You can view the list of available ontologies at the Research Hub ontology = CompetencyOntology(research_hub_name='esco') Creating CompetencyOntology from CandidateSkills \u00b6 To evaluate a method of skill extraction, it can be useful to format the output (a collection of CandidateSkill objects) as a CompetencyOntology. Importing skills_ml.ontologies.from_candidate_skills.ontology_from_candidate_skills enables this conversion. At present, the ontology_from_candidate_skills simply adds each of the found competencies to a bare ontology, and optionally associates them with the source object's occupation if tagged with one. Included Ontologies \u00b6 ONET \u00b6 The skills_ml.ontologies.onet module contains a Onet class inherited from CompetencyOntology. This class can be built either from a hosted JSON-LD file on the Research Hub, or by default will build the ontology during the instantiation from a variety of files on the ONET site, using at the time of writing the latest version of onet (db_v22_3): Content Model Reference.txt Knowledge.txt Skills.txt Abilities.txt Tools and Technology.txt Occupation Data.txt from skills.ml.ontologies.onet import Onet ONET = Onet() # this will take a while as it downloads the relatively large files and processes them ONET.filter_by(lambda edge: 'forklift' in edge.competency.name) If you pass in an ONET cache object, the raw ONET files can be cached on your filesystem so that building it the second time will be faster. from skills_ml.storage import FSStore from skills_ml.datasets.onet_cache import OnetSiteCache from skills_ml.ontologies.onet import Onet ONET = Onet(OnetSiteCache(FSStore('onet_cache'))) To build from the research hub, pass manual_build=False . This may be slightly quicker, and will be resilient to potential changes to the ONET site format. ONET = Onet(manual_build=False) ESCO \u00b6 The skills_ml.ontologies.esco module contains an Esco class inherited from CompetencyOntology that implements the European Skills and Competences and Occupations site. This class by default is built from a hosted JSON-LD file on the Research Hub, but you can also have it build right from the ESCO site. Building right from the ESCO site involves thousands of API calls, so we recommend building it from the Research Hub JSON-LD from skills_ml.ontologies.esco import Esco ESCO = Esco() # will build from premade Research Hub JSON-LD ESCO = Esco(manual_build=True) # will build from ESCO site, may take hours Uses of Ontologies \u00b6 Filtering \u00b6 You can filter the graph to produce subsets based on the list of edges. This will return another CompetencyOntology object, so any code that takes an ontology as input will work on the subsetted graph. You can optionally supply competency_name and competency_description keyword arguments to apply to the CompetencyFramework in the returned ontology object. This is necessary if you wish to send the CompetencyFramework object in the resulting ontology to algorithms in skills-ml. # Return an ontology that consists only of competencies with 'python' in the name, along with their related occupations ontology.filter_by(lambda edge: 'python' in edge.competency.name.lower()) # Return an ontology that consists only of occupations with 'software' in the name, along with their associated competencies ontology.filter_by(lambda edge: 'software' in edge.competency.name.lower()) # Return an ontology that is the intersection of 'python' competencies and 'software' occupations ontology.filter_by(lambda edge: 'software' in edge.occupation.name.lower() and 'python' in edge.competency.name.lower()) # Return only competencies who have a parent competency containing 'software' ontology.filter_by(lambda edge: any('software' in parent.name.lower() for parent in edge.parents) # Return an ontology with only 'python' competencies, and set a name/description for the resulting CompetencyFramework ontology.filter_by(lambda edge: 'python' in edge.competency.name.lower(), competency_name='python', competency_description='Python-related competencies') Skill Extraction: competencies-only \u00b6 Many list-based skill extraction require a CompetencyFramework as input. This can be retrieved directory from the CompetencyOntology object. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample skill_extractor = ExactMatchSkillExtractor(ontology.competency_framework) for candidate_skill in skill_extractor.candidate_skills(JobPostingCollectionSample()): print(candidate_skill) Skill Extraction: filtered competencies \u00b6 If you wish to filter a CompetencyOntology and then use it for skill extraction, you must make sure it has a name and description, either through the optional filter_by keyword argument or through modifying the CompetencyFramework instance directly. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample # Option 1: Using filter_by keyword arguments (recommended) competency_framework = ontology.filter_by( lambda edge: 'python' in edge.competency.name.lower(), competency_name='python', competency_description='Python-related competencies' ).competency_framework # Option 2: Modifying competency_framework afterwards competency_framework = ontology.filter_by(lambda edge: 'python' in edge.competency.name.lower()).competency_framework competency_framework.name = 'python' competency_framework.description = 'Python-related competencies' skill_extractor = ExactMatchSkillExtractor(competency_framework) for candidate_skill in skill_extractor.candidate_skills(JobPostingCollectionSample()): print(candidate_skill) Skill Extraction: full ontology \u00b6 The SocScopedExactMatchSkillExtractor requires both occupation and competency data, so it takes in the entire CompetencyOntology as input. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample skill_extractor = SocScopedExactMatchSkillExtractor(ontology) for candidate_skill in skill_extractor.candidate_skills(JobPostingCollectionSample()): print(candidate_skill) Exporting as JSON-LD \u00b6 You can export an ontology as a JSON-LD object for storage that you can later import import json with open('out.json', 'w') as f: json.dump(ontology.jsonld, f)","title":"Ontology Class"},{"location":"ontologies/#working-with-ontologies","text":"skills-ml is introducing the CompetencyOntology class, for a rich, flexible representation of competencies, occupations, and their relationships with each other. The CompetencyOntology class is backed by JSON-LD, and based on Credential Engine's CTDL-ASN format for Competencies . The goal is to be able to read in any CTDL-ASN framework and produce a CompetencyOntology object for use throughout the skills-ml library. Furthermore, skills-ml contains pre-mapped versions of open frameworks like ONET for use out of the box.","title":"Working With Ontologies"},{"location":"ontologies/#competency","text":"A competency, in the CTDL-ASN context, refers some knowledge, skill, or ability that a person can possess or learn. Each competency contains: A unique identifier within the ontology. If you're familiar with ONET, think the table of contents identifiers (e.g. '1.A.1.a.1') Some basic textual information: a name (e.g. Oral Comprehension) and/or description (e.g. 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'), , and maybe a general textual category (e.g. Ability) Associative information with other competencies. A basic example is a parent/child relationship, for instance ONET's definition of 'Oral Comprehension' as the child of another competency called 'Verbal Abilities'. CTDL-ASN encodes this using the 'hasChild' and 'isChildOf' properties, and this is used in skills-ml. There many other types of associations competencies can have with each other that the Competency class in skills-ml does not yet address, you can read more at the Credential Engine's definition ofCompetency . The Competency class tracks all of this. It can be created using either keyword arguments in the class' Constructor or through a class method that loads from JSON-LD.","title":"Competency"},{"location":"ontologies/#basic-example","text":"Using Python Constructor from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) Using JSON-LD from skills_ml.ontologies import Competency dinosaur_riding = Competency.from_jsonld({ '@type': 'Competency', '@id': '12345', 'name': 'Dinosaur Riding', 'description': 'Using the back of a dinosaur for transportation' }) To aid in bi-directional searching, the Competency object is meant to include a parent/child relationshiop on both the parent and child objects. The add_parent and add_child methods modify both the parent and child objects to easily maintain this bi-directional relationship.","title":"Basic Example"},{"location":"ontologies/#example-parentchild-relationship","text":"Using Python Constructor from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) extreme_transportation = Competency( identifier='123', name='Extreme Transportation', description='Comically dangerous forms of transportation' ) dinosaur_riding.add_parent(extreme_transportation) print(dinosaur_riding.parents) print(extreme_transportation.children) Using JSON-LD dinosaur_riding = Competency.from_jsonld({ '@type': 'Competency', '@id': '12345', 'name': 'Dinosaur Riding', 'description': 'Using the back of a dinosaur for transportation', 'isChildOf': [{'@type': 'Competency', '@id': '123'}] }) extreme_transportation = Competency.from_jsonld({ '@type': 'Competency', '@id': '123', 'name': 'Extreme Transportation', 'description': 'Comically dangerous forms of transportation', 'hasChild': [{'@type': 'Competency', '@id': '12345'}]","title":"Example parent/child relationship"},{"location":"ontologies/#occupation","text":"An Occupation is a job or profession that a person can hold. CTDL-ASN does not define this, so skills-ml models the Occupation similarly to the Competency, albeit with far less detail. A unique identifier within the ontology. If you're familiar with ONET, think of an ONET SOC code (11-1011.00) Some basic textual information: a name (e.g. Civil Engineer), maybe a description. Associative information with other occupations. So far the only relationship modeled in skills-ml between occupations is a parent/child one, similarly to Competency. Going back to the ONET example, an occupation representing the major group (identifier 11) may be thought of as the parent of SOC code 11-1011.00.","title":"Occupation"},{"location":"ontologies/#basic-example_1","text":"Using Python Constructor from skills_ml.ontologies import Occupation dinosaur_rider = Occupation( identifier='9999', name='Dinosaur Rider', ) Using JSON-LD from skills_ml.ontologies import Occupation dinosaur_rider = Occupation.from_jsonld({ '@type': 'Occupation', '@id': '9999', 'name': 'Dinosaur Rider' })","title":"Basic Example"},{"location":"ontologies/#competencyoccupationedge","text":"A CompetencyOccupationEdge is simply a relationship between a Competency and an Occupation. Currently, tthere are no further properties defined on this edge, though this will likely change in the future.","title":"CompetencyOccupationEdge"},{"location":"ontologies/#basic-example_2","text":"Using Python Constructor from skills_ml.ontologies import CompetencyOccupationEdge CompetencyOccupationEdge( occupation=dinosaur_rider, competency=dinosaur_riding ) Using JSON-LD from skills_ml.ontologies import CompetencyOccupationEdge CompetencyOccupationEdge.from_jsonld({ '@type': 'CompetencyOccupationEdge', '@id': 'competency=12345;occupation=9999', 'competency': {'@type': 'Competency', '@id': '12345'}, 'occupation': {'@type': 'Occupation', '@id': '9999'} })","title":"Basic Example"},{"location":"ontologies/#competencyframework","text":"A CompetencyFramework represent a collection of competencies and some metadata about them. The identifiers for given Competencies are used to disambiguate between them. The metadata exists so any code that uses the CompetencyFramework object can pass on useful knowledge about the framework to its output. The metadata has only two pieces of data: - name: A machine-readable name. Should be in snake case (e.g. onet_ksat ) - description: A human-readable description.","title":"CompetencyFramework"},{"location":"ontologies/#basic-example_3","text":"Using Python Constructor from skills_ml.ontologies import Competency, CompetencyFramework framework = CompetencyFramework( name='Sample Framework', description='A few basic competencies', competencies=[ Competency(identifier='a', name='Organization'), Competency(identifier='b', name='Communication Skills'), Competency(identifier='c', name='Cooking') ] )","title":"Basic Example"},{"location":"ontologies/#competencyontology","text":"An ontology represents a collection of competencies, a collection of occupations, and a collection of all relationships between competencies and occupations. The CompetencyOntology class represents each of these three collections using a set object. The identifiers for all of those objects are used to disambiguate between items in each of these sets. The JSON-LD representation of the ontology mirrors this internal structure. Below is an example of the objects defined above arranged into a CompetencyOntology. For brevity, the descriptions are omitted. Note in the Python example that importing the CompetencyOccupationEdge class is not necessary when using the Ontology; the add_edge method of Ontology can simply take a competency and occupation directly.","title":"CompetencyOntology"},{"location":"ontologies/#basic-example_4","text":"Using Python Constructor from skills_ml.ontologies import Competency, Occupation, CompetencyOntology ontology = CompetencyOntology( competency_name='caveman_games', competency_description='Competencies Useful to Characters in NES title Caveman Games' ) dinosaur_riding = Competency(identifier='12345', name='Dinosaur Riding') extreme_transportation = Competency(identifier='123', name='Extreme Transportation') dinosaur_riding.add_parent(extreme_transportation) dinosaur_rider = Occupation(identifier='9999', name='Dinosaur Rider') ontology.add_competency(dinosaur_riding) ontology.add_competency(extreme_transportation) ontology.add_occupation(dinosaur_rider) ontology.add_edge(occupation=dinosaur_rider, competency=dinosaur_riding) Using JSON-LD from skills_ml.ontologies import CompetencyOntology jsonld_string = \"\"\" 'name': 'test_ontology', 'competencies': [{ '@type': 'Competency', '@id': '12345', 'name': 'Dinosaur Riding', 'description': 'Using the back of a dinosaur for transportation', 'isChildOf': [{'@type': 'Competency', '@id': '123'}] }, { '@type': 'Competency', '@id': '123', 'name': 'Extreme Transportation', 'description': 'Comically dangerous forms of transportation', 'hasChild': [{'@type': 'Competency', '@id': '12345'}] }], 'occupations': [{ '@type': 'Occupation', '@id': '9999', 'name': 'Dinosaur Rider' }], 'edges': [{ '@type': 'CompetencyOccupationEdge', '@id': 'competency=12345;occupation=9999', 'competency': {'@type': 'Competency', '@id': '12345'}, 'occupation': {'@type': 'Occupation', '@id': '9999'} }] }\"\"\" ontology = CompetencyOntology(jsonld_string=jsonld_string) Using URL If you have access to a URL that contains compatible JSON-LD, you can send this URL right to the constructor. ontology = CompetencyOntology(url='https://myhost.com/ontology.json') Using Research Hub The Data@Work Research Hub hosts a number of ontologies publicly. The Research Hub ontology base URL is bundled into Skills-ML, so you can also just initialize one with the saved name on the Research Hub. You can view the list of available ontologies at the Research Hub ontology = CompetencyOntology(research_hub_name='esco')","title":"Basic Example"},{"location":"ontologies/#creating-competencyontology-from-candidateskills","text":"To evaluate a method of skill extraction, it can be useful to format the output (a collection of CandidateSkill objects) as a CompetencyOntology. Importing skills_ml.ontologies.from_candidate_skills.ontology_from_candidate_skills enables this conversion. At present, the ontology_from_candidate_skills simply adds each of the found competencies to a bare ontology, and optionally associates them with the source object's occupation if tagged with one.","title":"Creating CompetencyOntology from CandidateSkills"},{"location":"ontologies/#included-ontologies","text":"","title":"Included Ontologies"},{"location":"ontologies/#onet","text":"The skills_ml.ontologies.onet module contains a Onet class inherited from CompetencyOntology. This class can be built either from a hosted JSON-LD file on the Research Hub, or by default will build the ontology during the instantiation from a variety of files on the ONET site, using at the time of writing the latest version of onet (db_v22_3): Content Model Reference.txt Knowledge.txt Skills.txt Abilities.txt Tools and Technology.txt Occupation Data.txt from skills.ml.ontologies.onet import Onet ONET = Onet() # this will take a while as it downloads the relatively large files and processes them ONET.filter_by(lambda edge: 'forklift' in edge.competency.name) If you pass in an ONET cache object, the raw ONET files can be cached on your filesystem so that building it the second time will be faster. from skills_ml.storage import FSStore from skills_ml.datasets.onet_cache import OnetSiteCache from skills_ml.ontologies.onet import Onet ONET = Onet(OnetSiteCache(FSStore('onet_cache'))) To build from the research hub, pass manual_build=False . This may be slightly quicker, and will be resilient to potential changes to the ONET site format. ONET = Onet(manual_build=False)","title":"ONET"},{"location":"ontologies/#esco","text":"The skills_ml.ontologies.esco module contains an Esco class inherited from CompetencyOntology that implements the European Skills and Competences and Occupations site. This class by default is built from a hosted JSON-LD file on the Research Hub, but you can also have it build right from the ESCO site. Building right from the ESCO site involves thousands of API calls, so we recommend building it from the Research Hub JSON-LD from skills_ml.ontologies.esco import Esco ESCO = Esco() # will build from premade Research Hub JSON-LD ESCO = Esco(manual_build=True) # will build from ESCO site, may take hours","title":"ESCO"},{"location":"ontologies/#uses-of-ontologies","text":"","title":"Uses of Ontologies"},{"location":"ontologies/#filtering","text":"You can filter the graph to produce subsets based on the list of edges. This will return another CompetencyOntology object, so any code that takes an ontology as input will work on the subsetted graph. You can optionally supply competency_name and competency_description keyword arguments to apply to the CompetencyFramework in the returned ontology object. This is necessary if you wish to send the CompetencyFramework object in the resulting ontology to algorithms in skills-ml. # Return an ontology that consists only of competencies with 'python' in the name, along with their related occupations ontology.filter_by(lambda edge: 'python' in edge.competency.name.lower()) # Return an ontology that consists only of occupations with 'software' in the name, along with their associated competencies ontology.filter_by(lambda edge: 'software' in edge.competency.name.lower()) # Return an ontology that is the intersection of 'python' competencies and 'software' occupations ontology.filter_by(lambda edge: 'software' in edge.occupation.name.lower() and 'python' in edge.competency.name.lower()) # Return only competencies who have a parent competency containing 'software' ontology.filter_by(lambda edge: any('software' in parent.name.lower() for parent in edge.parents) # Return an ontology with only 'python' competencies, and set a name/description for the resulting CompetencyFramework ontology.filter_by(lambda edge: 'python' in edge.competency.name.lower(), competency_name='python', competency_description='Python-related competencies')","title":"Filtering"},{"location":"ontologies/#skill-extraction-competencies-only","text":"Many list-based skill extraction require a CompetencyFramework as input. This can be retrieved directory from the CompetencyOntology object. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample skill_extractor = ExactMatchSkillExtractor(ontology.competency_framework) for candidate_skill in skill_extractor.candidate_skills(JobPostingCollectionSample()): print(candidate_skill)","title":"Skill Extraction: competencies-only"},{"location":"ontologies/#skill-extraction-filtered-competencies","text":"If you wish to filter a CompetencyOntology and then use it for skill extraction, you must make sure it has a name and description, either through the optional filter_by keyword argument or through modifying the CompetencyFramework instance directly. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample # Option 1: Using filter_by keyword arguments (recommended) competency_framework = ontology.filter_by( lambda edge: 'python' in edge.competency.name.lower(), competency_name='python', competency_description='Python-related competencies' ).competency_framework # Option 2: Modifying competency_framework afterwards competency_framework = ontology.filter_by(lambda edge: 'python' in edge.competency.name.lower()).competency_framework competency_framework.name = 'python' competency_framework.description = 'Python-related competencies' skill_extractor = ExactMatchSkillExtractor(competency_framework) for candidate_skill in skill_extractor.candidate_skills(JobPostingCollectionSample()): print(candidate_skill)","title":"Skill Extraction: filtered competencies"},{"location":"ontologies/#skill-extraction-full-ontology","text":"The SocScopedExactMatchSkillExtractor requires both occupation and competency data, so it takes in the entire CompetencyOntology as input. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample skill_extractor = SocScopedExactMatchSkillExtractor(ontology) for candidate_skill in skill_extractor.candidate_skills(JobPostingCollectionSample()): print(candidate_skill)","title":"Skill Extraction: full ontology"},{"location":"ontologies/#exporting-as-json-ld","text":"You can export an ontology as a JSON-LD object for storage that you can later import import json with open('out.json', 'w') as f: json.dump(ontology.jsonld, f)","title":"Exporting as JSON-LD"},{"location":"skills_ml.algorithms.aggregators/","text":"Source: skills-ml/skills_ml/algorithms/aggregators/ init .py#L0 JobAggregator \u00b6 JobAggregator. __init__ \u00b6 __init__(self, output_count=1, output_total=False) Initialize self. See help(type(self)) for accurate signature. JobAggregator.accumulate \u00b6 accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to JobAggregator.group_outputs \u00b6 group_outputs(self, full_key) JobAggregator.initialize_counts \u00b6 initialize_counts(self) JobAggregator.output_header_row \u00b6 output_header_row(self, prefix) JobAggregator.rollup_outputs \u00b6 rollup_outputs(self, job_key) JobAggregator.value \u00b6 value(self, job_posting) CountAggregator \u00b6 Counts job postings CountAggregator. __init__ \u00b6 __init__(self, output_count=1, output_total=False) Initialize self. See help(type(self)) for accurate signature. CountAggregator.accumulate \u00b6 accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to CountAggregator.group_outputs \u00b6 group_outputs(self, full_key) CountAggregator.initialize_counts \u00b6 initialize_counts(self) CountAggregator.output_header_row \u00b6 output_header_row(self, prefix) CountAggregator.rollup_outputs \u00b6 rollup_outputs(self, job_key) CountAggregator.value \u00b6 value(self, job_posting) SkillAggregator \u00b6 Aggregates skills found in job postings Args: skill_extractor (.skill_extractors.FreetextSkillExtractor) an object that returns skill counts from unstructured text corpus creator (object) an object that returns a text corpus from a job posting SkillAggregator. __init__ \u00b6 __init__(self, skill_extractor, corpus_creator, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature. SkillAggregator.accumulate \u00b6 accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to SkillAggregator.group_outputs \u00b6 group_outputs(self, full_key) SkillAggregator.initialize_counts \u00b6 initialize_counts(self) SkillAggregator.output_header_row \u00b6 output_header_row(self, prefix) SkillAggregator.rollup_outputs \u00b6 rollup_outputs(self, job_key) SkillAggregator.value \u00b6 value(self, job_posting) OccupationScopedSkillAggregator \u00b6 Aggregates skills found in job postings using the job's occupation Args: skill_extractor (.skill_extractors.FreetextSkillExtractor) an object that returns skill counts from unstructured text corpus creator (object) an object that returns a text corpus from a job posting OccupationScopedSkillAggregator. __init__ \u00b6 __init__(self, skill_extractor, corpus_creator, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature. OccupationScopedSkillAggregator.accumulate \u00b6 accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to OccupationScopedSkillAggregator.group_outputs \u00b6 group_outputs(self, full_key) OccupationScopedSkillAggregator.initialize_counts \u00b6 initialize_counts(self) OccupationScopedSkillAggregator.output_header_row \u00b6 output_header_row(self, prefix) OccupationScopedSkillAggregator.rollup_outputs \u00b6 rollup_outputs(self, job_key) OccupationScopedSkillAggregator.value \u00b6 value(self, job_posting) SocCodeAggregator \u00b6 Aggregates SOC codes inferred from job posting text Args: occupation_classifier (.occupation_classifiers.SocClassifier) An object that returns a classified SOC code and similarity score from unstructured text corpus creator (object) an object that returns unstructured text from a job posting SocCodeAggregator. __init__ \u00b6 __init__(self, occupation_classifier, corpus_creator, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature. SocCodeAggregator.accumulate \u00b6 accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to SocCodeAggregator.group_outputs \u00b6 group_outputs(self, full_key) SocCodeAggregator.initialize_counts \u00b6 initialize_counts(self) SocCodeAggregator.output_header_row \u00b6 output_header_row(self, prefix) SocCodeAggregator.rollup_outputs \u00b6 rollup_outputs(self, job_key) SocCodeAggregator.value \u00b6 value(self, job_posting) GivenSocCodeAggregator \u00b6 Aggregates SOC codes given as an input field Caution! We may or may not know where these came from, and the method for creating them may differ from record to record GivenSocCodeAggregator. __init__ \u00b6 __init__(self, output_count=1, output_total=False) Initialize self. See help(type(self)) for accurate signature. GivenSocCodeAggregator.accumulate \u00b6 accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to GivenSocCodeAggregator.group_outputs \u00b6 group_outputs(self, full_key) GivenSocCodeAggregator.initialize_counts \u00b6 initialize_counts(self) GivenSocCodeAggregator.output_header_row \u00b6 output_header_row(self, prefix) GivenSocCodeAggregator.rollup_outputs \u00b6 rollup_outputs(self, job_key) GivenSocCodeAggregator.value \u00b6 value(self, job_posting)","title":"Skills ml.algorithms.aggregators"},{"location":"skills_ml.algorithms.aggregators/#jobaggregator","text":"","title":"JobAggregator"},{"location":"skills_ml.algorithms.aggregators/#jobaggregator__init__","text":"__init__(self, output_count=1, output_total=False) Initialize self. See help(type(self)) for accurate signature.","title":"JobAggregator.__init__"},{"location":"skills_ml.algorithms.aggregators/#jobaggregatoraccumulate","text":"accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to","title":"JobAggregator.accumulate"},{"location":"skills_ml.algorithms.aggregators/#jobaggregatorgroup_outputs","text":"group_outputs(self, full_key)","title":"JobAggregator.group_outputs"},{"location":"skills_ml.algorithms.aggregators/#jobaggregatorinitialize_counts","text":"initialize_counts(self)","title":"JobAggregator.initialize_counts"},{"location":"skills_ml.algorithms.aggregators/#jobaggregatoroutput_header_row","text":"output_header_row(self, prefix)","title":"JobAggregator.output_header_row"},{"location":"skills_ml.algorithms.aggregators/#jobaggregatorrollup_outputs","text":"rollup_outputs(self, job_key)","title":"JobAggregator.rollup_outputs"},{"location":"skills_ml.algorithms.aggregators/#jobaggregatorvalue","text":"value(self, job_posting)","title":"JobAggregator.value"},{"location":"skills_ml.algorithms.aggregators/#countaggregator","text":"Counts job postings","title":"CountAggregator"},{"location":"skills_ml.algorithms.aggregators/#countaggregator__init__","text":"__init__(self, output_count=1, output_total=False) Initialize self. See help(type(self)) for accurate signature.","title":"CountAggregator.__init__"},{"location":"skills_ml.algorithms.aggregators/#countaggregatoraccumulate","text":"accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to","title":"CountAggregator.accumulate"},{"location":"skills_ml.algorithms.aggregators/#countaggregatorgroup_outputs","text":"group_outputs(self, full_key)","title":"CountAggregator.group_outputs"},{"location":"skills_ml.algorithms.aggregators/#countaggregatorinitialize_counts","text":"initialize_counts(self)","title":"CountAggregator.initialize_counts"},{"location":"skills_ml.algorithms.aggregators/#countaggregatoroutput_header_row","text":"output_header_row(self, prefix)","title":"CountAggregator.output_header_row"},{"location":"skills_ml.algorithms.aggregators/#countaggregatorrollup_outputs","text":"rollup_outputs(self, job_key)","title":"CountAggregator.rollup_outputs"},{"location":"skills_ml.algorithms.aggregators/#countaggregatorvalue","text":"value(self, job_posting)","title":"CountAggregator.value"},{"location":"skills_ml.algorithms.aggregators/#skillaggregator","text":"Aggregates skills found in job postings Args: skill_extractor (.skill_extractors.FreetextSkillExtractor) an object that returns skill counts from unstructured text corpus creator (object) an object that returns a text corpus from a job posting","title":"SkillAggregator"},{"location":"skills_ml.algorithms.aggregators/#skillaggregator__init__","text":"__init__(self, skill_extractor, corpus_creator, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature.","title":"SkillAggregator.__init__"},{"location":"skills_ml.algorithms.aggregators/#skillaggregatoraccumulate","text":"accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to","title":"SkillAggregator.accumulate"},{"location":"skills_ml.algorithms.aggregators/#skillaggregatorgroup_outputs","text":"group_outputs(self, full_key)","title":"SkillAggregator.group_outputs"},{"location":"skills_ml.algorithms.aggregators/#skillaggregatorinitialize_counts","text":"initialize_counts(self)","title":"SkillAggregator.initialize_counts"},{"location":"skills_ml.algorithms.aggregators/#skillaggregatoroutput_header_row","text":"output_header_row(self, prefix)","title":"SkillAggregator.output_header_row"},{"location":"skills_ml.algorithms.aggregators/#skillaggregatorrollup_outputs","text":"rollup_outputs(self, job_key)","title":"SkillAggregator.rollup_outputs"},{"location":"skills_ml.algorithms.aggregators/#skillaggregatorvalue","text":"value(self, job_posting)","title":"SkillAggregator.value"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregator","text":"Aggregates skills found in job postings using the job's occupation Args: skill_extractor (.skill_extractors.FreetextSkillExtractor) an object that returns skill counts from unstructured text corpus creator (object) an object that returns a text corpus from a job posting","title":"OccupationScopedSkillAggregator"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregator__init__","text":"__init__(self, skill_extractor, corpus_creator, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature.","title":"OccupationScopedSkillAggregator.__init__"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregatoraccumulate","text":"accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to","title":"OccupationScopedSkillAggregator.accumulate"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregatorgroup_outputs","text":"group_outputs(self, full_key)","title":"OccupationScopedSkillAggregator.group_outputs"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregatorinitialize_counts","text":"initialize_counts(self)","title":"OccupationScopedSkillAggregator.initialize_counts"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregatoroutput_header_row","text":"output_header_row(self, prefix)","title":"OccupationScopedSkillAggregator.output_header_row"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregatorrollup_outputs","text":"rollup_outputs(self, job_key)","title":"OccupationScopedSkillAggregator.rollup_outputs"},{"location":"skills_ml.algorithms.aggregators/#occupationscopedskillaggregatorvalue","text":"value(self, job_posting)","title":"OccupationScopedSkillAggregator.value"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregator","text":"Aggregates SOC codes inferred from job posting text Args: occupation_classifier (.occupation_classifiers.SocClassifier) An object that returns a classified SOC code and similarity score from unstructured text corpus creator (object) an object that returns unstructured text from a job posting","title":"SocCodeAggregator"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregator__init__","text":"__init__(self, occupation_classifier, corpus_creator, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature.","title":"SocCodeAggregator.__init__"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregatoraccumulate","text":"accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to","title":"SocCodeAggregator.accumulate"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregatorgroup_outputs","text":"group_outputs(self, full_key)","title":"SocCodeAggregator.group_outputs"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregatorinitialize_counts","text":"initialize_counts(self)","title":"SocCodeAggregator.initialize_counts"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregatoroutput_header_row","text":"output_header_row(self, prefix)","title":"SocCodeAggregator.output_header_row"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregatorrollup_outputs","text":"rollup_outputs(self, job_key)","title":"SocCodeAggregator.rollup_outputs"},{"location":"skills_ml.algorithms.aggregators/#soccodeaggregatorvalue","text":"value(self, job_posting)","title":"SocCodeAggregator.value"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregator","text":"Aggregates SOC codes given as an input field Caution! We may or may not know where these came from, and the method for creating them may differ from record to record","title":"GivenSocCodeAggregator"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregator__init__","text":"__init__(self, output_count=1, output_total=False) Initialize self. See help(type(self)) for accurate signature.","title":"GivenSocCodeAggregator.__init__"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregatoraccumulate","text":"accumulate(self, job_posting, job_key, groups) Incorporates the data from a single job posting Args: job_posting (dict) a job postings in common schema format job_key (str) an element of the job posting (say, a title) that is being used for aggregation groups (iterable) the aggregatable groups (say, CBSAs) that this job posting belongs to","title":"GivenSocCodeAggregator.accumulate"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregatorgroup_outputs","text":"group_outputs(self, full_key)","title":"GivenSocCodeAggregator.group_outputs"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregatorinitialize_counts","text":"initialize_counts(self)","title":"GivenSocCodeAggregator.initialize_counts"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregatoroutput_header_row","text":"output_header_row(self, prefix)","title":"GivenSocCodeAggregator.output_header_row"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregatorrollup_outputs","text":"rollup_outputs(self, job_key)","title":"GivenSocCodeAggregator.rollup_outputs"},{"location":"skills_ml.algorithms.aggregators/#givensoccodeaggregatorvalue","text":"value(self, job_posting)","title":"GivenSocCodeAggregator.value"},{"location":"skills_ml.algorithms.corpus_creators/","text":"Source: skills-ml/skills_ml/algorithms/corpus_creators/ init .py Global Variables basic","title":"Skills ml.algorithms.corpus creators"},{"location":"skills_ml.algorithms.elasticsearch_indexers/","text":"Source: skills-ml/skills_ml/algorithms/elasticsearch_indexers/ init .py","title":"Skills ml.algorithms.elasticsearch indexers"},{"location":"skills_ml.algorithms.geocoders/","text":"Source: skills-ml/skills_ml/algorithms/geocoders/ init .py#L0 Global Variables - STATE_NAME_LOOKUP \u00b6 job_posting_search_strings \u00b6 job_posting_search_strings(job_posting) Convert a job posting to a geocode-ready search string Includes city and state if present, or just city Args: job_posting (string) A job posting in schema.org/JobPosting json form Returns: (string) A geocode-ready search string S3CachedGeocoder \u00b6 Geocoder that uses S3 as a cache. Args: s3_conn (boto.s3.connection) an s3 connection cache_s3_path (string) path (including bucket) to the json cache on s3 geocode_func (function) a function that geocodes a given search string defaults to the OSM geocoder provided by the geocode library sleep_time (int) The time, in seconds, between geocode calls S3CachedGeocoder.all_cached_geocodes \u00b6 Return the contents of the geocoding cache Returns: (dict) search strings mapping to their (dict) geocoded results S3CachedGeocoder. __init__ \u00b6 __init__(self, s3_conn, cache_s3_path, geocode_func=<function osm at 0x7fd512527b70>, \\ sleep_time=1) Initialize self. See help(type(self)) for accurate signature. S3CachedGeocoder.geocode \u00b6 geocode(self, search_string) Geocodes a single search string First checks in cache to see if the search string has been geocoded If the geocoding function is called, the process will sleep afterwards Warning: Not to be used in parallel! To respect throttling limits, it would be dangerous to try and run more then one of these against a geocoder service. As a result, the code assumes that when it comes time to save, there have been no external changes to the file on S3 worth keeping. Args: search_string (string) A search query to send to the geocoder Returns: (string) The geocoding result S3CachedGeocoder.geocode_job_postings_and_save \u00b6 geocode_job_postings_and_save(self, job_postings, save_every=100000) Geocode job postings and save the results to S3 Args: job_postings (iterable) Job postings in common schema format save_every (int) How frequently to defensively save the cache Defaults to every 100000 job postings S3CachedGeocoder.retrieve_from_cache \u00b6 retrieve_from_cache(self, job_posting) Retrieve a saved geocode result from the cache if it exists Usable in parallel, since it will not perform geocoding on its own. This means that you should make code that calls this dependent on the geocoding for the job posting being completed Args: job_posting (string) A job posting in schema.org/JobPosting json form Returns: (string) The geocoding result, or None if none is available S3CachedGeocoder.save \u00b6 save(self) Save the geocoding cache to S3","title":"Skills ml.algorithms.geocoders"},{"location":"skills_ml.algorithms.geocoders/#-state_name_lookup","text":"","title":"- STATE_NAME_LOOKUP"},{"location":"skills_ml.algorithms.geocoders/#job_posting_search_strings","text":"job_posting_search_strings(job_posting) Convert a job posting to a geocode-ready search string Includes city and state if present, or just city Args: job_posting (string) A job posting in schema.org/JobPosting json form Returns: (string) A geocode-ready search string","title":"job_posting_search_strings"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocoder","text":"Geocoder that uses S3 as a cache. Args: s3_conn (boto.s3.connection) an s3 connection cache_s3_path (string) path (including bucket) to the json cache on s3 geocode_func (function) a function that geocodes a given search string defaults to the OSM geocoder provided by the geocode library sleep_time (int) The time, in seconds, between geocode calls","title":"S3CachedGeocoder"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocoderall_cached_geocodes","text":"Return the contents of the geocoding cache Returns: (dict) search strings mapping to their (dict) geocoded results","title":"S3CachedGeocoder.all_cached_geocodes"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocoder__init__","text":"__init__(self, s3_conn, cache_s3_path, geocode_func=<function osm at 0x7fd512527b70>, \\ sleep_time=1) Initialize self. See help(type(self)) for accurate signature.","title":"S3CachedGeocoder.__init__"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocodergeocode","text":"geocode(self, search_string) Geocodes a single search string First checks in cache to see if the search string has been geocoded If the geocoding function is called, the process will sleep afterwards Warning: Not to be used in parallel! To respect throttling limits, it would be dangerous to try and run more then one of these against a geocoder service. As a result, the code assumes that when it comes time to save, there have been no external changes to the file on S3 worth keeping. Args: search_string (string) A search query to send to the geocoder Returns: (string) The geocoding result","title":"S3CachedGeocoder.geocode"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocodergeocode_job_postings_and_save","text":"geocode_job_postings_and_save(self, job_postings, save_every=100000) Geocode job postings and save the results to S3 Args: job_postings (iterable) Job postings in common schema format save_every (int) How frequently to defensively save the cache Defaults to every 100000 job postings","title":"S3CachedGeocoder.geocode_job_postings_and_save"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocoderretrieve_from_cache","text":"retrieve_from_cache(self, job_posting) Retrieve a saved geocode result from the cache if it exists Usable in parallel, since it will not perform geocoding on its own. This means that you should make code that calls this dependent on the geocoding for the job posting being completed Args: job_posting (string) A job posting in schema.org/JobPosting json form Returns: (string) The geocoding result, or None if none is available","title":"S3CachedGeocoder.retrieve_from_cache"},{"location":"skills_ml.algorithms.geocoders/#s3cachedgeocodersave","text":"save(self) Save the geocoding cache to S3","title":"S3CachedGeocoder.save"},{"location":"skills_ml.algorithms.job_geography_queriers/","text":"Source: skills-ml/skills_ml/algorithms/job_geography_queriers/ init .py#L0 Global Variables cbsa_from_geocode cbsa","title":"Skills ml.algorithms.job geography queriers"},{"location":"skills_ml.algorithms.job_labelers/","text":"Source: skills-ml/skills_ml/algorithms/job_labelers/ init .py","title":"Skills ml.algorithms.job labelers"},{"location":"skills_ml.algorithms.job_normalizers/","text":"Source: skills-ml/skills_ml/algorithms/job_normalizers/ init .py","title":"Skills ml.algorithms.job normalizers"},{"location":"skills_ml.algorithms.job_vectorizers/","text":"Source: skills-ml/skills_ml/algorithms/job_vectorizers/ init .py","title":"Skills ml.algorithms.job vectorizers"},{"location":"skills_ml.algorithms.jobtitle_cleaner/","text":"Source: skills-ml/skills_ml/algorithms/jobtitle_cleaner/ init .py","title":"Skills ml.algorithms.jobtitle cleaner"},{"location":"skills_ml.algorithms/","text":"[TOC] skills_ml.algorithms.embedding skills_ml.algorithms.embedding.base skills_ml.algorithms.embedding.models Embedding model class inherited the interface from gensim Word2VecModel Word2VecModel(self, model_name=None, storage=None, *args, **kwargs) The Word2VecModel inherited from gensim's Word2Vec model ( https://radimrehurek.com/gensim/models/word2vec.html) for training, using and evaluating word embedding with extension methods. Example from skills_ml.algorithms.embedding.models import Word2VecModel word2vec_model = Word2VecModel() Doc2VecModel Doc2VecModel(self, model_name=None, storage=None, *args, **kwargs) The Doc2VecModel inherited from gensim's Doc2Vec model ( https://radimrehurek.com/gensim/models/doc2vec) for training, using and evaluating word embedding with extension methods. Example from skills_ml.algorithms.embedding.models import Doc2VecModel doc2vec_model = Doc2VecModel() FastTextModel FastTextModel(self, model_name=None, storage=None, *args, **kwargs) The FastTextModel inhereited from gensim's FastText model ( https://radimrehurek.com/gensim/models/fasttext.html) for training, using and evaluating word embedding with extension methods. Example ``` from skills_ml.algorithms.embedding.models import import FastTextModel fasttext = FastTextModel() ``` skills_ml.algorithms.embedding.train EmbeddingTrainer EmbeddingTrainer(self, *models, model_storage=None, batch_size=2000) An embedding learning class. Example from skills_ml.algorithms.occupation_classifiers.train import EmbeddingTrainer from skills_ml.job_postings.common_schema import JobPostingCollectionSample from skills_ml.job_postings.corpora.basic import Doc2VecGensimCorpusCreator, Word2VecGensimCorpusCreator from skills_ml.storage import FSStore model = Word2VecModel(size=size, min_count=min_count, iter=iter, window=window, workers=workers, **kwargs) s3_conn = S3Hook().get_conn() job_postings_generator = JobPostingGenerator(s3_conn, quarters, s3_path, source=\"all\") corpus_generator = Word2VecGensimCorpusCreator(job_postings_generator) w2v = Word2VecModel(storage=FSStore(path='/tmp'), size=10, min_count=3, iter=4, window=6, workers=3) trainer = EmbeddingTrainer(w2v) trainer.train(corpus_generator) trainer.save_model() skills_ml.algorithms.geocoders Geocoders, with caching and throttling CachedGeocoder CachedGeocoder(self, cache_storage, cache_fname, geocode_func=<function osm at 0x7f010819a510>, sleep_time=1, autosave=True) Geocoder that uses specified storage as a cache. Args cache_storage (object) FSStore() or S3Store object to store the cache cache_fname (string) cache file name geocode_func (function) a function that geocodes a given search string defaults to the OSM geocoder provided by the geocode library sleep_time (int) The time, in seconds, between geocode calls skills_ml.algorithms.geocoders.cbsa Given geocode results, find matching Core-Based Statistical Areas. Match Match(self, /, *args, **kwargs) Match(index, area) CachedCBSAFinder CachedCBSAFinder(self, cache_storage, cache_fname, shapefile_name=None, cache_dir=None) Find CBSAs associated with geocode results and save them to the specified storage Geocode results are expected in the json format provided by the python geocoder module, with a 'bbox' The highest-level interface is the 'find_all_cbsas_and_save' method, which provides storage caching. A minimal call looks like cache_storage = S3Store('some-bucket') cache_fname = 'cbsas.json' cbsa_finder = CachedCBSAFinder(cache_storage=cache_storage, cache_fname=cache_fname) cbsa_finder.find_all_cbsas_and_save({ \"Flushing, NY\": { 'bbox': ['southwest': [..., ...], 'northeast': [...,...] } \"Houston, TX\": { 'bbox': ['southwest': [..., ...], 'northeast': [...,...] } }) # This usage of 'bbox' is what you can retrieve from a `geocoder` call, such as: geocoder.osm('Flushing, NY').json() The keys in the resulting cache will be the original search strings. Warning: The caching is not parallel-safe! It is recommended you should run only one copy of find_all_cbsas_and_save at a time to avoid overwriting the cache file. Args cache_storage (object) FSStore() or S3Store object to store the cache cache_fname (string) cache file name shapefile_name (string) local path to a CBSA shapefile to use optional, will download TIGER 2015 shapefile if absent cache_dir (string) local path to a cache directory to use if the shapefile needs to be downloaded optional, will use 'tmp' in working directory if absent skills_ml.algorithms.job_normalizers Algorithms to normalize a job title to a smaller space skills_ml.algorithms.job_normalizers.elasticsearch Indexes job postings for job title normalization NormalizeTopNIndexer NormalizeTopNIndexer(self, quarter, job_postings_generator, job_titles_index, alias_name, **kwargs) Creates an index that stores data for job title normalization. Depends on a previously created index with job titles and occupations. Queries the job title/occupation index for 1. job titles or occupations that match the job description 2. Occupation matches The top three results are indexed. Args quarter (string) the quarter from which to retrieve job postings job_postings_generator (iterable) an iterable of job postings job_title_index (string) The name of an already existing job title/occupation index skills_ml.algorithms.job_normalizers.esa_jobtitle_normalizer Normalize a job title through Explicit Semantic Analysis Originally written by Kwame Porter Robinson ESANormalizer ESANormalizer(self, onet_source=<class 'skills_ml.datasets.onet_source.OnetToDiskDownloader'>) Normalize a job title to ONET occupation titles using explicit semantic analysis. Uses ONET occupation titles and descriptions. skills_ml.algorithms.jobtitle_cleaner Clean job titles skills_ml.algorithms.jobtitle_cleaner.clean Clean job titles by utilizing a list of stopwords clean_by_rules clean_by_rules(jobtitle) Remove numbers and normalize spaces Args jobtitle (string) A string Returns : (string) the string with numbers removes and spaces normalized clean_by_neg_dic clean_by_neg_dic(jobtitle, negative_list, positive_list) Remove words from the negative dictionary Args jobtitle (string) A job title string negative_list (collection) A list of stop words positive_list (collection) A list of positive words to override stop words Returns : (string) The cleaned job title aggregate aggregate(df_jobtitles, groupby_keys) Args df_jobtitles : job titles in pandas DataFrame groupby_keys : a list of keys to be grouped by. should be something like ['title', 'geo'] Returns agg_cleaned_jobtitles : a aggregated verison of job title in pandas DataFrame JobTitleStringClean JobTitleStringClean(self) Clean job titles by stripping numbers, and removing place/state names (unless they are also ONET jobs) skills_ml.algorithms.nlp String transformations for cleaning for unicodedata, see http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values deep deep(func) A decorator that will apply a function to a nested list recursively Args func (function) : a function to be applied to a nested list Returns function : The wrapped function normalize normalize(text:str) -> str Args text (str) : A unicode string Returns str : The text, lowercased and in NFKD normal form lowercase_strip_punc lowercase_strip_punc(text:str, punct:Set[str]=None) -> str Args text (str) : A unicode string punct ( :obj: set , optional) Returns str : The text, lowercased, sans punctuation and in NFKD normal form title_phase_one title_phase_one(text:str, punct:Set[str]=None) -> str Args text (str) : A unicode string punct ( :obj: set , optional) Returns str : The text, lowercased, sans punctuation, whitespace normalized clean_str clean_str(text:str) -> str Args text : A unicode string Returns str : lowercased, sans punctuation, non-English letters sentence_tokenize sentence_tokenize(text:str) -> List[str] Args text (str) : a unicode string Returns list : tokenized sentence word_tokenize word_tokenize(text:str, punctuation=True) -> List[str] Args text (str) : a unicode string Returns list : tokenized words fields_join fields_join(document:Dict, document_schema_fields:List[str]=None) -> str Args document (dict) : a document dictionary document_schema_fields ( :obj: list , optional): a list of keys Returns str : a text joined with selected fields. vectorize vectorize(tokenized_text:List[str], embedding_model) Args tokenized_text : a tokenized list of word tokens embedding_model : the embedding model implements .infer_vector() method Returns np.ndarray : a word embedding vector section_extract section_extract(section_regex:Pattern[~AnyStr], document:str) -> List Only return the contents of the configured section heading Defines a 'heading' as the text of a sentence that - does not itself start with a bullet character - either has between 1 and 3 words or ends in a colon For a heading that matches the given pattern, returns each sentence between it and the next heading. Heavily relies on the fact that sentence_tokenize does line splitting as well as standard sentence tokenization. In this way, it should work both for text strings that have newlines and for text strings that don't. In addition, this function splits each sentence by bullet characters as often bullets denote what we want to call 'sentences', but authors often take advantage of the bullet characters to make the contents of each 'sentence' into small sentence fragments, which makes standard sentence tokenization insufficient if the newlines have been taken out. split_by_bullets split_by_bullets(sentence:str) -> List Split sentence by bullet characters strip_bullets_from_line strip_bullets_from_line(line:str) -> str Remove bullets from beginning of line skills_ml.algorithms.occupation_classifiers SOCMajorGroup SOCMajorGroup(self, filters=None) FullSOC FullSOC(self, filters=None, onet_cache=None) skills_ml.algorithms.occupation_classifiers.classifiers SocClassifier SocClassifier(self, classifier) Interface of SOC Code Classifier for computer class to use. KNNDoc2VecClassifier KNNDoc2VecClassifier(self, embedding_model, k=1, indexer=None, model_name=None, model_storage=None, **kwargs) Nearest neightbors model to classify the jobposting data into soc code. If the indexer is passed, then NearestNeighbors will use approximate nearest neighbor approach which is much faster than the built-in knn in gensim. Attributes embedding_model ( :job: skills_ml.algorithms.embedding.models.Doc2VecModel ): Doc2Vec embedding model k (int) : number of nearest neighbor. If k = 1, look for the soc code from single nearest neighbor. If k > 1, classify the soc code by the majority vote of nearest k neighbors. indexer ( :obj: gensim.similarities.index ): any kind of gensim compatible indexer skills_ml.algorithms.occupation_classifiers.test skills_ml.algorithms.occupation_classifiers.train OccupationClassifierTrainer OccupationClassifierTrainer(self, matrix, k_folds, grid_config=None, storage=None, random_state_for_split=None, scoring=['accuracy'], n_jobs=3) Trains a series of classifiers using the same training set Args matrix (skills_ml.algorithms.train.matrix) : a matrix object holds X, y and other training data information storage (skills_ml.storage) : a skills_ml storage object specified the store method k_folds (int) : number of folds for cross validation random_state_for_split(int) : random state n_jobs (int) : umber of jobs to run in parallel scores skills_ml.algorithms.preprocessing ProcessingPipeline ProcessingPipeline(self, *functions:Callable) A simple callable processing pipeline for imperative execution runtime. This class will compose processing functions together to become a callable object that takes in the input from the very first processing function and returns the output of the last processing function. Example This class can be used to create a callable vectorization object which will transform a string into a vector and also preserve the preprocessing functions for being reused later. ```python jp = JobPostingCollectionSample() vectorization = ProcessingPipeline( normalize, clean_html, clean_str, word_tokenize, partial(vectorize, embedding_model=w2v) ) vector = vecotrization(\"Why so serious?\") ``` Attributes functions (generator) : a series of functions IterablePipeline IterablePipeline(self, *functions:Callable) A simple iterable processing pipeline. This class will compose processing functions together to be passed to different stages(training/prediction) to assert the same processing procedrues. Example jp = JobPostingCollectionSample() pipe = IterablePipeline( partial(fields_join, document_schema_fields=['description']), clean_html, sentence_tokenize, clean_str, word_tokenize ) preprocessed_generator = pipe(jp) Attributes functions (generator) : a series of generator functions that takes another generator as input func2gen func2gen(func:Callable) -> Callable A wrapper that change a document-transforming function that takes only one document the input into a function that takes a generator/iterator as the input. When it instantiates, it will become a generator. Example @func2gen def do_something(doc) return do_something_to_the_doc(doc) Args func (function) : a function only take one document as the first argument input. Returns func (function) : a function that takes a generator as the first argument input. skills_ml.algorithms.sampling Generate and store samples of datasets skills_ml.algorithms.sampling.methods Generic sampling methods reservoir reservoir(it, k) Reservoir sampling with Random Sort from a job posting iterator Randomly choosing a sample of k items from a streaming iterator. Using random sort to implement the algorithm. Basically, it's assigning random number as keys to each item and maintain k items with minimum value for keys, which equals to assigning a random number to each item as key and sort items using these keys and take top k items. Args it (iterator) : Job posting iterator to sample from k (int) : Sample size Returns generator : The result sample of k items. reservoir_weighted reservoir_weighted(it, k, weights, key) Weighted reservoir Sampling from job posting iterator Randomly choosing a sample of k items from a streaming iterator based on the weights. Args it (iterator) : Job posting iterator to sample from. The format should be (job_posting, label) k (int) : Sample size weights (dict) : a dictionary that has key-value pairs as label-weighting pairs. It expects every label in the iterator to be present as a key in the weights dictionary For example, weights = {'11' : 2, '13', 1}. In this case, the label/key is the occupation major group and the value is the weight you want to sample with. Returns generator : The result sample of k items from weighted reservori sampling. skills_ml.algorithms.skill_extractors Extract skills from text corpora, such as job postings skills_ml.algorithms.skill_extractors.base Base classes for skill extraction CandidateSkill CandidateSkill(self, /, *args, **kwargs) CandidateSkill(skill_name, matched_skill_identifier, context, start_index, confidence, document_id, document_type, source_object, skill_extractor_name) Trie Trie(self) Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern. The corresponding Regex should match much faster than a simple Regex union. SkillExtractor SkillExtractor(self, transform_func:Callable=None) Abstract class for all skill extractors. All subclasses must implement candidate_skills. All subclasses must define properties 'method' (a short machine readable property) 'description' (a text description of how the extractor does its work) Args transform_func (callable, optional) Function that transforms a structured object into text Defaults to SimpleCorpusCreator's _join, which takes common text fields in common schema job postings and concatenates them together. For non-job postings another transform function may be needed. ListBasedSkillExtractor ListBasedSkillExtractor(self, competency_framework, *args, **kwargs) Extract skills by comparing with a known lookup/list. Subclasses must implement _skills_lookup and _document_skills_in_lookup Args skill_lookup_name (string, optional) An identifier for the skill lookup type. Defaults to onet_ksat skill_lookup_description (string, optional) A human-readable description of the skill lookup. skills_ml.algorithms.skill_extractors.exact_match Use exact matching with a source list to find skills ExactMatchSkillExtractor ExactMatchSkillExtractor(self, *args, **kwargs) Extract skills from unstructured text Builds a lookup based on the 'name' attribute of all competencies in the given framework Originally written by Kwame Porter Robinson skills_ml.algorithms.skill_extractors.fuzzy_match Use fuzzy matching with a source list to extract skills from unstructured text FuzzyMatchSkillExtractor FuzzyMatchSkillExtractor(self, *args, **kwargs) Extract skills from unstructured text using fuzzy matching skills_ml.algorithms.skill_extractors.grammar Use sentence grammar to extract phrases that may be skills sentences_words_pos sentences_words_pos(document) Chops raw text into part-of-speech (POS)-tagged words in sentences Args document (string) A document in text format Returns : (list) of sentences, each being a list of word/POS pair Example sentences_words_pos( '* Develop and maintain relationship with key members of ' + 'ESPN\u2019s Spanish speaking editorial team' ) [ # list of sentences [ # list of word/POS pairs ('*', 'NN'), ('Develop', 'NNP'), ('and', 'CC'), ('maintain', 'VB'), ('relationship', 'NN'), ('with', 'IN'), ('key', 'JJ'), ('members', 'NNS'), ('of', 'IN'), ('ESPN', 'NNP'), ('\u2019', 'NNP'), ('s', 'VBD'), ('Spanish', 'JJ'), ('speaking', 'NN'), ('editorial', 'NN'), ('team', 'NN') ] ] phrases_in_line_with_context phrases_in_line_with_context(line, parser, target_labels) Generate phrases in the given line of text Args text (string) : A line of raw text Yields tuples, each with two strings - a noun phrase - the context of the noun phrase (currently defined as the surrounding sentence) is_bulleted is_bulleted(string) Whether or not a given string begins a 'bullet' character A bullet character is understood to indicate list membership. Differeing common bullet characters are checked. Args string (string) : Any string Returns : (bool) whether or not the string begins with one of the characters in a predefined list of common bullets clean_beginning clean_beginning(string) Clean the beginning of a string of common undesired formatting substrings Args string (string) : Any string Returns : The string with beginning formatting substrings removed NPEndPatternExtractor NPEndPatternExtractor(self, endings, stop_phrases, only_bulleted_lines=True, confidence=95, *args, **kwargs) Identify noun phrases with certain ending words (e.g 'skills', 'abilities') as skills Args endings (list) : Single words that should identify the ending of a noun phrase as being a skill stop_phrases (list) : Noun phrases that should not be considered skills only_bulleted_lines (bool, default True) : Whether or not to only consider lines that look like they are items in a list SkillEndingPatternExtractor SkillEndingPatternExtractor(self, *args, **kwargs) Identify noun phrases ending with 'skill' or 'skills' as skills AbilityEndingPatternExtractor AbilityEndingPatternExtractor(self, *args, **kwargs) Identify noun phrases ending in 'ability' or 'abilities' as skills skills_ml.algorithms.skill_extractors.noun_phrase_ending Use noun phrases with specific endings to extract skills from job postings sentences_words_pos sentences_words_pos(document) Chops raw text into part-of-speech (POS)-tagged words in sentences Args document (string) A document in text format Returns : (list) of sentences, each being a list of word/POS pair Example sentences_words_pos( '* Develop and maintain relationship with key members of ' + 'ESPN\u2019s Spanish speaking editorial team' ) [ # list of sentences [ # list of word/POS pairs ('*', 'NN'), ('Develop', 'NNP'), ('and', 'CC'), ('maintain', 'VB'), ('relationship', 'NN'), ('with', 'IN'), ('key', 'JJ'), ('members', 'NNS'), ('of', 'IN'), ('ESPN', 'NNP'), ('\u2019', 'NNP'), ('s', 'VBD'), ('Spanish', 'JJ'), ('speaking', 'NN'), ('editorial', 'NN'), ('team', 'NN') ] ] noun_phrases_in_line_with_context noun_phrases_in_line_with_context(line) Generate noun phrases in the given line of text Args text (string) : A line of raw text Yields tuples, each with two strings - a noun phrase - the context of the noun phrase (currently defined as the surrounding sentence) is_bulleted is_bulleted(string) Whether or not a given string begins a 'bullet' character A bullet character is understood to indicate list membership. Differeing common bullet characters are checked. Args string (string) : Any string Returns : (bool) whether or not the string begins with one of the characters in a predefined list of common bullets clean_beginning clean_beginning(string) Clean the beginning of a string of common undesired formatting substrings Args string (string) : Any string Returns : The string with beginning formatting substrings removed NPEndPatternExtractor NPEndPatternExtractor(self, endings, stop_phrases, only_bulleted_lines=True, confidence=95, *args, **kwargs) Identify noun phrases with certain ending words (e.g 'skills', 'abilities') as skills Args endings (list) : Single words that should identify the ending of a noun phrase as being a skill stop_phrases (list) : Noun phrases that should not be considered skills only_bulleted_lines (bool, default True) : Whether or not to only consider lines that look like they are items in a list SkillEndingPatternExtractor SkillEndingPatternExtractor(self, *args, **kwargs) Identify noun phrases ending with 'skill' or 'skills' as skills AbilityEndingPatternExtractor AbilityEndingPatternExtractor(self, *args, **kwargs) Identify noun phrases ending in 'ability' or 'abilities' as skills skills_ml.algorithms.skill_extractors.section_extract SectionExtractSkillExtractor SectionExtractSkillExtractor(self, section_regex=None, *args, **kwargs) Extract skills from text by extracting sentences from matching 'sections'. Heavily utilizes skills_ml.algorithms.nlp.section_extract. For more detail on how to define 'sections', refer to its docstring. skills_ml.algorithms.skill_extractors.soc_exact SocScopedExactMatchSkillExtractor SocScopedExactMatchSkillExtractor(self, competency_ontology, *args, **kwargs) Extract skills from unstructured text, but only return matches that agree with a known taxonomy skills_ml.algorithms.skill_extractors.symspell SymSpell SymSpell(self, max_dictionary_edit_distance=2, prefix_length=7, count_threshold=1) SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent. Opposite to other algorithms only deletes are required, no transposes + replaces + inserts. Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term. Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters! SymSpell supports compound splitting / decompounding of multi-word input strings with three cases mistakenly inserted space into a correct word led to two incorrect terms mistakenly omitted space between two correct words led to one incorrect combined term multiple independent input terms with/without spelling errors See https://github.com/wolfgarbe/SymSpell for details. Args max_dictionary_edit_distance (int, optional) : Maximum distance used to generate index. Also acts as an upper bound for max_edit_distance parameter in lookup() method. Defaults to 2. prefix_length (int, optional) : Prefix length. Should not be changed normally. Defaults to 7. count_threshold (int, optional) : Threshold corpus-count value for words to be considered correct. Defaults to 1, values below zero are also mapped to 1. Consider setting a higher value if your corpus contains mistakes. skills_ml.algorithms.skill_feature_creator SequenceFeatureCreator SequenceFeatureCreator(self, job_posting_generator, sentence_tokenizer=None, word_tokenizer=None, features=None, embedding_model=None) Sequence Feature Creator helps users to instantiate different types of feature at once and combine them together into a sentence(sequence) feature array for sequence modeling. It's a generator that outputs a sentence array at a time. A sentence array is composed of word vectors. Example from skills_ml.algorithms.skill_feature_creator import FeatureCreator feature_vector_generator = FeatureCreator(job_posting_generator) feature_vector_generator = FeatureCreator(job_posting_generator, features=[\"StructuralFeature\", \"EmbeddingFeature\"]) Args job_posting_generator (generator) : job posting generator. sentence_tokenizer (func) : sentence tokenization function word_tokenizer (func) : word tokenization function features (list) : list of feature types ones want to include. If it's None or by default, it includes all the feature types. Yield sentence_array (numpy.array): an array of word vectors represents the words and punctuations in the sentence. The dimension is (# of words)*(dimension of the concat word vector) StructuralFeature StructuralFeature(self, sentence_tokenizer=None, word_tokenizer=None, **kwargs) Sturctural features ContextualFeature ContextualFeature(self, sentence_tokenizer=None, word_tokenizer=None, **kwargs) Contextual features EmbeddingFeature EmbeddingFeature(self, sentence_tokenizer=None, word_tokenizer=None, **kwargs) Embedding Feature skills_ml.algorithms.skill_feature_creator.contextual_features skills_ml.algorithms.skill_feature_creator.posTags skills_ml.algorithms.skill_feature_creator.structure_features","title":"Algorithms"},{"location":"skills_ml.algorithms.occupation_classifiers/","text":"Source: skills-ml/skills_ml/algorithms/occupation_classifiers/ init .py","title":"Skills ml.algorithms.occupation classifiers"},{"location":"skills_ml.algorithms.representativeness_calculators/","text":"Source: skills-ml/skills_ml/algorithms/representativeness_calculators/ init .py","title":"Skills ml.algorithms.representativeness calculators"},{"location":"skills_ml.algorithms.sampling/","text":"Source: skills-ml/skills_ml/algorithms/sampling/ init .py#L0 Sample \u00b6 Sample. __init__ \u00b6 __init__(self, base_path, sample_name) Initialize self. See help(type(self)) for accurate signature.","title":"Skills ml.algorithms.sampling"},{"location":"skills_ml.algorithms.sampling/#sample","text":"","title":"Sample"},{"location":"skills_ml.algorithms.sampling/#sample__init__","text":"__init__(self, base_path, sample_name) Initialize self. See help(type(self)) for accurate signature.","title":"Sample.__init__"},{"location":"skills_ml.algorithms.skill_extractors/","text":"Source: skills-ml/skills_ml/algorithms/skill_extractors/ init .py#L0 Global Variables fuzzy_match base soc_exact exact_match upload_candidates_from_job_posting_json \u00b6 upload_candidates_from_job_posting_json(candidates_path, skill_extractor, job_posting_json, \\ sample_name)","title":"Skills ml.algorithms.skill extractors"},{"location":"skills_ml.algorithms.skill_extractors/#upload_candidates_from_job_posting_json","text":"upload_candidates_from_job_posting_json(candidates_path, skill_extractor, job_posting_json, \\ sample_name)","title":"upload_candidates_from_job_posting_json"},{"location":"skills_ml.algorithms.skill_importance_extractors/","text":"Source: skills-ml/skills_ml/algorithms/skill_importance_extractors/ init .py","title":"Skills ml.algorithms.skill importance extractors"},{"location":"skills_ml.algorithms.skill_list_processors/","text":"Source: skills-ml/skills_ml/algorithms/skill_list_processors/ init .py#L0 Global Variables onet_ksat","title":"Skills ml.algorithms.skill list processors"},{"location":"skills_ml.algorithms.skill_taggers/","text":"Source: skills-ml/skills_ml/algorithms/skill_taggers/ init .py","title":"Skills ml.algorithms.skill taggers"},{"location":"skills_ml.algorithms.string_cleaners/","text":"Source: skills-ml/skills_ml/algorithms/string_cleaners/ init .py#L0 Global Variables nlp","title":"Skills ml.algorithms.string cleaners"},{"location":"skills_ml.algorithms.title_extractors/","text":"Source: skills-ml/skills_ml/algorithms/title_extractors/ init .py","title":"Skills ml.algorithms.title extractors"},{"location":"skills_ml.datasets.cbsa_shapefile/","text":"skills_ml.datasets.cbsa_shapefile download_shapefile download_shapefile(cache_dir) Download Tiger 2015 CBSA Shapefile Downloads the zip archive and unzips the contents Args: cache_dir (string) local path to download files to Returns: (string) Path to the extracted shapefile","title":"Skills ml.datasets.cbsa shapefile"},{"location":"skills_ml.datasets.cousub_ua/","text":"cousub_ua cousub_ua(city_cleaner) Construct a County Subdivision->UA Lookup table from Census data Returns: dict { StateCode: { CountySubdivisionName: UA Code } }","title":"Skills ml.datasets.cousub ua"},{"location":"skills_ml.datasets.job_postings/","text":"job_postings job_postings(s3_conn, quarter, s3_path, source='all') Stream all job listings from s3 for a given quarter Args: s3_conn: a boto s3 connection quarter: a string representing a quarter (2015Q1) s3_path: path to the job listings. source: should be a string or a subset of \"nlx\", \"va\", \"cb\" or \"all\" Yields: string in json format representing the next job listing Refer to sample_job_listing.json for example structure","title":"Skills ml.datasets.job postings"},{"location":"skills_ml.datasets/","text":"[TOC] skills_ml.datasets.cbsa_shapefile Use the Census CBSA shapefile download_shapefile download_shapefile(cache_dir) Download Tiger 2015 CBSA Shapefile Downloads the zip archive and unzips the contents Args cache_dir (string) local path to download files to Returns : (string) Path to the extracted shapefile skills_ml.datasets.cousub_ua Retrieve County Subdivision->Urbanized Area crosswalk cousub_ua cousub_ua(city_cleaner) Construct a County Subdivision->UA Lookup table from Census data Returns: dict { StateCode: { CountySubdivisionName: UA Code } } skills_ml.datasets.job_titles Process lists of job titles into a common format skills_ml.datasets.job_titles.elasticsearch Index job title/occupation pairs in Elasticsearch. JobTitlesMasterIndexer JobTitlesMasterIndexer(self, job_title_generator, alias_name, **kwargs) Args: job_title_generator (iterable). Each record is expected to be a dict with keys 'Title' for the job title and 'Original Title' for the occupation skills_ml.datasets.job_titles.onet Process ONET job titles into a common format Onet_Title Onet_Title(self, onet_cache) An object representing job title data from different ONET files Originally written by Kwame Porter Robinson OnetTitleExtractor OnetTitleExtractor(self, output_filename, onet_source, hash_function) An object that creates a job titles CSV based on ONET data skills_ml.datasets.nber_county_cbsa Retrieve county->CBSA crosswalk file from the NBER cbsa_lookup cbsa_lookup() Construct a County->CBSA Lookup table from NBER data Returns: dict each key is a (State Code, County FIPS code) tuple each value is a (CBSA FIPS code, CBSA Name) tuple skills_ml.datasets.negative_positive_dict negative_positive_dict negative_positive_dict() Construct a dictionary of terms that are considered not to be in job title, including states, states abv, cities Returns: dictionary of set skills_ml.datasets.onet_cache OnetCache OnetCache(self, s3_conn, s3_path, cache_dir) An object that downloads and caches ONET files from S3 OnetSiteCache OnetSiteCache(self, storage=None) An object that downloads files from the ONET site skills_ml.datasets.onet_source Download ONET files from their site OnetToMemoryDownloader OnetToMemoryDownloader(self, /, *args, **kwargs) Downloads newest version of ONET as of time of writing and returns it as text OnetToDiskDownloader OnetToDiskDownloader(self, /, *args, **kwargs) skills_ml.datasets.partner_updaters Update raw job postings from external partners skills_ml.datasets.partner_updaters.usa_jobs Update raw job postings from the USAJobs API USAJobsUpdater USAJobsUpdater(self, auth_key, key_email, session=None) skills_ml.datasets.place_ua Retrieve Census Place->Urbanized Area crosswalk place_ua place_ua(city_cleaner) Construct a Place->UA Lookup table from Census data Returns: dict { StateCode: { PlaceName: UA Code } } skills_ml.datasets.sba_city_county Retrieve county lookup tables from the SBA for each state county_lookup county_lookup() Retrieve county lookup tables if they are not already cached Returns: (dict) each key is a state, each value is a dict {city_name: (fips_county_code, county_name)} skills_ml.datasets.skill_importances Process lists of occupation skill importances into a common format skills_ml.datasets.skill_importances.onet Process ONET data to create a dataset with occupations and their skill importances OnetSkillImportanceExtractor OnetSkillImportanceExtractor(self, storage, output_dataset_name, hash_function=None) An object that creates a skills importance CSV based on ONET data Originally written by Kwame Porter Robinson skills_ml.datasets.skills Process lists of skills into a common format skills_ml.datasets.skills.ceasn_from_onet skills_ml.datasets.skills.onet_ksat Process ONET skill lists of various types into a common format OnetSkillListProcessor OnetSkillListProcessor(self, onet_source, output_filename, hash_function, ksa_types=None) An object that creates a skills CSV based on ONET data Originally written by Kwame Porter Robinson skills_ml.datasets.ua_cbsa Retrieve Urbanized Area->CBSA crosswalk ua_cbsa ua_cbsa() Construct a UA->CBSA Lookup table from Census data Returns: dict { UA Fips: [(CBSA FIPS, CBSA Name)] }","title":"External Dataset Processors"},{"location":"skills_ml.datasets.nber_county_cbsa/","text":"skills_ml.datasets.nber_county_cbsa Retrieve county->CBSA crosswalk file from the NBER cbsa_lookup cbsa_lookup() Construct a County->CBSA Lookup table from NBER data Returns: dict each key is a (State Code, County FIPS code) tuple each value is a (CBSA FIPS code, CBSA Name) tuple","title":"Skills ml.datasets.nber county cbsa"},{"location":"skills_ml.datasets.negative_positive_dict/","text":"negative_positive_dict negative_positive_dict() Construct a dictionary of terms that are considered not to be in job title, including states, states abv, cities Returns: dictionary of set","title":"Skills ml.datasets.negative positive dict"},{"location":"skills_ml.datasets.onet_cache/","text":"skills_ml.datasets.onet_cache OnetCache OnetCache(self, s3_conn, s3_path, cache_dir) An object that downloads and caches ONET files from S3","title":"Skills ml.datasets.onet cache"},{"location":"skills_ml.datasets.onet_source/","text":"skills_ml.datasets.onet_source","title":"Skills ml.datasets.onet source"},{"location":"skills_ml.datasets.partner_updaters/","text":"skills_ml.datasets.partner_updaters","title":"Skills ml.datasets.partner updaters"},{"location":"skills_ml.datasets.place_ua/","text":"place_ua place_ua(city_cleaner) Construct a Place->UA Lookup table from Census data Returns: dict { StateCode: { PlaceName: UA Code } }","title":"Skills ml.datasets.place ua"},{"location":"skills_ml.datasets.raw_job_postings/","text":"skills_ml.datasets.raw_job_postings skills_ml.datasets.raw_job_postings.usajobs Import USAJobs postings into the Open Skills common schema","title":"Skills ml.datasets.raw job postings"},{"location":"skills_ml.datasets.sba_city_county/","text":"skills_ml.datasets.sba_city_county Retrieve county lookup tables from the SBA for each state county_lookup county_lookup() Retrieve county lookup tables if they are not already cached Returns: dict each key is a state, each value is a dict {city_name: (fips_county_code, county_name)}","title":"Skills ml.datasets.sba city county"},{"location":"skills_ml.datasets.ua_cbsa/","text":"ua_cbsa ua_cbsa() Construct a UA->CBSA Lookup table from Census data Returns: dict { UA Fips: [(CBSA FIPS, CBSA Name)] }","title":"Skills ml.datasets.ua cbsa"},{"location":"skills_ml.evaluation/","text":"[TOC] skills_ml.evaluation.annotators BratExperiment BratExperiment(self, experiment_name, brat_s3_path) Manage a BRAT experiment. Handles The creation of BRAT config for a specific sample of job postings Adding users to the installation and allocating them semi-hidden job postings The parsing of the annotation results at the end of the experiment Syncs data to an experiment directory on S3. BRAT installations are expected to sync this data down regularly. Keeps track of a metadata file, available as a dictionary at self.metadata, with the following structure these first five keys are just storage of user input to either \u00b6 the constructor or start() \u00b6 view relevant docstrings for definitions \u00b6 sample_base_path sample_name entities_with_shortcuts minimum_annotations_per_posting max_postings_per_allocation units and allocations are far more important when reading results of an experiment \u00b6 units: { # canonical list of 'unit' (bundle of job postings) names, # along with a list of tuples of job posting keys (only unique within unit) and globally unique job posting ids 'unit_1': [ (posting_key_1, job_posting_id_1), (posting_key_2, job_posting_id_2), ], 'unit_2': [ (posting_key_1, job_posting_id_3), (posting_key_2, job_posting_id_4), ] } allocations: { # canonical list of unit assignments to users 'user_1': ['unit_1', 'unit_2'], 'user_2': ['unit_2'] } skills_ml.evaluation.embedding_metrics CategorizationMetric CategorizationMetric(self, clustering:skills_ml.ontologies.clustering.Clustering) cosine similarity between the clustering concept and the mean vector of all entities within that concept cluster. IntraClusterCohesion IntraClusterCohesion(self, clustering:skills_ml.ontologies.clustering.Clustering) sum of squared error of the centroid of the concept cluster and each entities within the concept cluster. RecallTopN RecallTopN(self, clustering:skills_ml.ontologies.clustering.Clustering, topn=20) For a given concept cluster and a given number n, find top n similar entities from the whole entity pool based on cosin similarity, and then calculate the top n recall: number of the true positive from top n closest entities divided by the total number of the concept cluster. PrecisionTopN PrecisionTopN(self, clustering:skills_ml.ontologies.clustering.Clustering, topn=10) For a given concept cluster and a given number n, find top n similar entities from the whole entity pool based on cosin similarity, and then calculate the top n precision: number of the true positive from top n closest entities divided by n. skills_ml.evaluation.job_title_normalizers Test job normalizers Requires 'interesting_job_titles.csv' to be populated, of format input job title description of job ONET code Each task will output two CSV files, one with the normalizer's ranks and one without ranks. The latter is for sending to people to fill out and the former is for testing those results against the normalizer's Originally written by Kwame Porter Robinson InputSchema InputSchema(self, /, *args, **kwargs) An enumeration listing the data elements and indices taken from source data InterimSchema InterimSchema(self, /, *args, **kwargs) An enumeration listing the data elements and indices after normalization NormalizerResponse NormalizerResponse(self, name=None, access=None, num_examples=3) Abstract interface for enforcing common iteration, access patterns to a variety of possible normalizers. Args name (string) : A name for the normalizer access (filename or file object) : A tab-delimited CSV with column order {job_title, description, soc_code} num_examples (int, optional) : Number of top responses to include Normalizers should return a list of results, ordered by relevance, with 'title' and optional 'relevance_score' keys MiniNormalizer MiniNormalizer(self, name, access, normalize_class) Access normalizer classes which can be instantiated and implement 'normalize_job_title(job_title)' DataAtWorkNormalizer DataAtWorkNormalizer(self, name=None, access=None, num_examples=3) skills_ml.evaluation.occ_cls_evaluator ClassificationEvaluator ClassificationEvaluator(self, result_generator) OnetOccupationClassificationEvaluator OnetOccupationClassificationEvaluator(self, result_generator) skills_ml.evaluation.representativeness_calculators Calculate representativeness of a dataset, such as job postings skills_ml.evaluation.representativeness_calculators.geo_occupation Computes geographic representativeness of job postings based on ONET SOC Code GeoOccupationRepresentativenessCalculator GeoOccupationRepresentativenessCalculator(self, geo_querier=None, normalizer=None) Calculates geographic representativeness of SOC Codes. If a job normalizer is given, will attempt to compute SOC codes of jobs that have missing SOC codes Args geo_querier (skills_ml.job_postings.geography_queriers) An object that can return a CBSA from a job posting normalizer (skills_ml.algorithms.occupation_classifiers) An object that can return the SOC code from a job posting skills_ml.evaluation.skill_extraction_metrics OntologyCompetencyRecall OntologyCompetencyRecall(self, ontology:skills_ml.ontologies.base.CompetencyOntology) The percentage of competencies in an ontology which are present in the candidate skills OntologyOccupationRecall OntologyOccupationRecall(self, ontology:skills_ml.ontologies.base.CompetencyOntology) The percentage of occupations in the ontology that are present in the candidate skills MedianSkillsPerDocument MedianSkillsPerDocument(self, /, *args, **kwargs) The median number of distinct skills present in each document SkillsPerDocumentHistogram SkillsPerDocumentHistogram(self, bins=10, *args, **kwargs) The PercentageNoSkillDocuments PercentageNoSkillDocuments(self, /, *args, **kwargs) The percentage of documents that contained zero skills TotalVocabularySize TotalVocabularySize(self, /, *args, **kwargs) The total number of skills represented TotalOccurrences TotalOccurrences(self, /, *args, **kwargs) The total number of candidate skill occurrences EvaluationSetPrecision EvaluationSetPrecision(self, candidate_skills:Generator[skills_ml.algorithms.skill_extractors.base.CandidateSkill, NoneType, NoneType], evaluation_set_name:str, strict:bool=True) Find the precision evaluated against an evaluation set of candidate skills. Args candidate_skills (CandidateSkillYielder) : A collection of candidate skills to evaluate against evaluation_set_name (str) : A name for the evaluation set of candidate skills. Used in the name of the metric so results from multiple evaluation sets can be compared side-by-side. strict (bool, default True) : Whether or not to enforce the exact location of the match, versus just matching between sets on the same skill name and document. Setting this to False will guard against 1. labelers who don't mark every instance of a skill once they found one instance 2. discrepancies in start_index values caused by errant transformation methods However, this could also produce false matches, so use with care. EvaluationSetRecall EvaluationSetRecall(self, candidate_skills, evaluation_set_name, strict=True) Find the recall evaluated against an evaluation set of candidate skills. Args candidate_skills (CandidateSkillYielder) : A collection of candidate skills to evaluate against evaluation_set_name (str) : A name for the evaluation set of candidate skills. Used in the name of the metric so results from multiple evaluation sets can be compared side-by-side. strict (bool, default True) : Whether or not to enforce the exact location of the match, versus just matching between sets on the same skill name and document. Setting this to False will guard against 1. labelers who don't mark every instance of a skill once they found one instance 2. discrepancies in start_index values caused by errant transformation methods However, this could also produce false matches, so use with care. skills_ml.evaluation.skill_extractors","title":"Evaluation Tools"},{"location":"skills_ml.evaluation/#these-first-five-keys-are-just-storage-of-user-input-to-either","text":"","title":"these first five keys are just storage of user input to either"},{"location":"skills_ml.evaluation/#the-constructor-or-start","text":"","title":"the constructor or start()"},{"location":"skills_ml.evaluation/#view-relevant-docstrings-for-definitions","text":"sample_base_path sample_name entities_with_shortcuts minimum_annotations_per_posting max_postings_per_allocation","title":"view relevant docstrings for definitions"},{"location":"skills_ml.evaluation/#units-and-allocations-are-far-more-important-when-reading-results-of-an-experiment","text":"units: { # canonical list of 'unit' (bundle of job postings) names, # along with a list of tuples of job posting keys (only unique within unit) and globally unique job posting ids 'unit_1': [ (posting_key_1, job_posting_id_1), (posting_key_2, job_posting_id_2), ], 'unit_2': [ (posting_key_1, job_posting_id_3), (posting_key_2, job_posting_id_4), ] } allocations: { # canonical list of unit assignments to users 'user_1': ['unit_1', 'unit_2'], 'user_2': ['unit_2'] }","title":"units and allocations are far more important when reading results of an experiment"},{"location":"skills_ml.evaluation.query/","text":"skills_ml.evaluation.query InterimSchema InterimSchema(self, /, *args, **kwargs) An enumeration. NormalizerResponse NormalizerResponse(self, name=None, access=None, num_examples=3) Abstract interface for enforcing common iteration, access patterns to a variety of possible normalizers. access should be a CSV with column order: job_title, description, soc_code Normalizers should return a list of results, ordered by relevance, with 'title' and optional 'relevance_score' keys MiniNormalizer MiniNormalizer(self, name, access, normalize_class) Access normalizer classes which can be instantiated and implement 'normalize_job_title(job_title)' InputSchema InputSchema(self, /, *args, **kwargs) An enumeration.","title":"Skills ml.evaluation.query"},{"location":"skills_ml.job_postings/","text":"[TOC] skills_ml.job_postings.aggregate skills_ml.job_postings.aggregate.dataset_transform Track stats of job listing datasets, before and after transformation into the common schema. DatasetStatsCounter DatasetStatsCounter(self, dataset_id, quarter) Accumulate data Dataset ETL statistics for a quarter to show presence and absence of different fields, and the total count of rows Args dataset_id (string) A dataset id quarter (string) The quarter being analyzed DatasetStatsAggregator DatasetStatsAggregator(self, dataset_id, s3_conn) Aggregate data Dataset ETL statistics up to the dataset level Args dataset_id (string) A dataset id s3_conn (boto.Connection) an s3 connection GlobalStatsAggregator GlobalStatsAggregator(self, s3_conn) Aggregate Dataset ETL statistics up to the global level Args s3_conn (boto.Connection) an s3 connection skills_ml.job_postings.aggregate.field_values Track field value distribution of common schema job postings FieldValueCounter FieldValueCounter(self, quarter, field_values) Accumulate field distribution statistics for common schema job postings Args quarter (string) The quarter being analyzed field_values (list) each entry should be either 1. a field key 2. a tuple, first value field key, second value function to fetch value or values from document skills_ml.job_postings.aggregate.pandas Aggregation functions that can be used with pandas dataframes listy_n_most_common listy_n_most_common(*params, **kwparams) Expects each item to be iterable, each sub-item to be addable AggregateFunction AggregateFunction(self, returns) Wrap a function with an attribute that indicates the return type name skills_ml.job_postings.common_schema A variety of common-schema job posting collections. Each class in this module should implement a generator that yields job postings (in the common schema, as a JSON string), and has a 'metadata' attribute so any users of the job postings can inspect meaningful metadata about the postings. JobPostingCollectionFromS3 JobPostingCollectionFromS3(self, s3_conn, s3_paths, extra_metadata=None) Stream job posting from s3. Expects that each will be stored in JSON format, one job posting per line. The s3_path given will be iterated through as a prefix, so job postings may be partitioned under that prefix however you choose. It will look in every file under that prefix. Example import json from airflow.hooks import S3Hook from skills_ml.job_postings.common_schema import JobPostingGenerator s3_conn = S3Hook().get_conn() job_postings_generator = JobPostingCollectionFromS3(s3_conn, s3_path='my-bucket/job_postings_common_schema') for job_posting in job_postings_generator: print(job_posting['title']) Attributes s3_conn : a boto s3 connection s3_path : path to the job listings. there may be multiple JobPostingCollectionSample JobPostingCollectionSample(self, num_records:int=50) Stream a finite number of job postings stored within the library. Example import json job_postings = JobPostingCollectionSample() for job_posting in job_postings: print(json.loads(job_posting)['title']) Meant to provide a dependency-less example of common schema job postings for introduction to the library Args: num_records (int): The maximum number of records to return. Defaults to 50 (all postings available) <h2 id=\"skills_ml.job_postings.common_schema.generate_job_postings_from_s3\">generate_job_postings_from_s3</h2> ```python generate_job_postings_from_s3(s3_conn, s3_prefix:str) -> Generator[Dict[str, Any], NoneType, NoneType] Stream all job listings from s3 Args s3_conn : a boto s3 connection s3_prefix : path to the job listings. Yields string in json format representing the next job listing Refer to sample_job_listing.json for example structure generate_job_postings_from_s3_multiple_prefixes generate_job_postings_from_s3_multiple_prefixes(s3_conn, s3_prefixes:str) -> Generator[Dict[str, Any], NoneType, NoneType] Chain the generators of a list of multiple quarters Args s3_conn : a boto s3 connection s3_prefixes : paths to job listings Return a generator that all generators are chained together into batches_generator batches_generator(iterable, batch_size) Batch generator Args iterable : an iterable batch_size : batch size get_onet_occupation get_onet_occupation(job_posting) Retrieve the occupation from the job posting First checks the custom 'onet_soc_code' key, then the standard 'occupationalCategory' key, and falls back to the unknown occupation skills_ml.job_postings.computed_properties Encapsulates the computation of some piece of data for job postings, to make aggregation and tabular datasets easy to produce JobPostingComputedProperty JobPostingComputedProperty(self, storage, partition_func=None) Base class for computers of job posting properties. Using this class, expensive computations can be performed once, stored on S3 per job posting in partitions, and reused in different aggregations. The base class takes care of all of the serialization and partitioning, leaving subclasses to implement a function for computing the property of a single posting and metadata describing the output of this function. Subclasses must implement - _compute_func_on_one to produce a callable that takes in a single job posting and returns JSON-serializable output representing the computation target. This function can produce objects that are kept in scope and reused, so properties that require a large object (e.g. a trained classifier) to do their computation work can be downloaded from S3 here without requiring the I/O work to be done over and over. (See .computers.SOCClassifyProperty for illustration) - property_name attribute (string) that is used when saving the computed properties - property_columns attribute (list) of ComputedPropertyColumns that map to the column names output by `_compute_func_on_one` Args storage (skills_ml.storage.Store) A storage object in which to store the cached properties. partition_func (callable, optional) A function that takes a job posting and outputs a string that should be used as a partition key. Must be deterministic. Defaults to the 'datePosted' value The caches will be namespaced by the property name and partition function ComputedPropertyColumn ComputedPropertyColumn(self, name, description, compatible_aggregate_function_paths=None) Metadata about a specific output column of a computed property Args name (string) The name of the column description (string) A description of the column and how it was populated. compatible_aggregate_function_paths (dict, optional) : If this property is meant to be used in aggregations, map string function paths to descriptions of what the function is computing for this column. All function paths should be compatible with pandas.agg (one argument, an iterable), though multi-argument functions can be used in conjunction with functools.partial skills_ml.job_postings.computed_properties.aggregators Aggregate job posting computed properties into tabular datasets df_for_properties_and_keys df_for_properties_and_keys(computed_properties, keys) Assemble a dataframe with the raw data from many computed properties and keys Args computed_properties (list of JobPostingComputedProperty) keys (list of strs) Returns : pandas.DataFrame expand_array_col_to_many_cols expand_array_col_to_many_cols(base_col, func, aggregation) Expand an array column created as the result of an .aggregate call into many columns Args base_col (string) The name of the base column (before .aggregate) func (function) The base function that was aggregated on aggregation (pandas.DataFrame) The post-aggregation dataframe Returns : pandas.DataFrame, minus the array column and plus columns for each array value base_func base_func(aggregate_function) Deals with the possibility of functools.partial being applied to a given function. Allows access to the decorated 'return' attribute whether or not it is also a partial function Args aggregate_function (callable) Either a raw function or a functools.partial object Returns : callable aggregation_for_properties_and_keys aggregation_for_properties_and_keys(grouping_properties, aggregate_properties, aggregate_functions, keys) Assemble an aggregation dataframe for given partition keys Args grouping_properties (list of JobPostingComputedProperty) Properties to form the primary key of the aggregation aggregate_properties (list of JobPostingComputedProperty) Properties to be aggregated over the primary key aggregate_functions (dict) A lookup of aggregate functions to be applied for each aggregate column keys (list of str) The desired partition keys for the aggregation to cover Returns : pandas.DataFrame indexed on the grouping properties, covering all data from the given keys aggregate_properties aggregate_properties(out_filename, grouping_properties, aggregate_properties, aggregate_functions, storage, aggregation_name) Aggregate computed properties and stores the resulting CSV Args out_filename (string) The desired filename (without path) for the .csv grouping_properties (list of JobPostingComputedProperty) Properties to form the primary key of the aggregation aggregate_properties (list of JobPostingComputedProperty) Properties to be aggregated over the primary key aggregate_functions (dict) A lookup of aggregate functions to be applied for each aggregate column aggregations_path (string) The base s3 path to store aggregations aggregation_name (string) The name of this particular aggregation Returns : nothing skills_ml.job_postings.computed_properties.computers Various computers of job posting properties. Each class is generally a generic algorithm (such as skill extraction or occupation classification) paired with enough configuration to run on its own TitleCleanPhaseOne TitleCleanPhaseOne(self, storage, partition_func=None) Perform one phase of job title cleaning: lowercase/remove punctuation TitleCleanPhaseTwo TitleCleanPhaseTwo(self, storage, partition_func=None) Perform two phases of job title cleaning lowercase/remove punctuation Remove geography information Geography Geography(self, geo_querier, *args, **kwargs) Produce a geography by querying a given JobGeographyQuerier Args geo_querier SOCClassifyProperty SOCClassifyProperty(self, classifier_obj, *args, **kwargs) Classify the SOC code from a trained classifier Args classifier_obj (object, optional) An object to use as a classifier. If not sent one will be downloaded from s3 GivenSOC GivenSOC(self, storage, partition_func=None) Assign the SOC code given by the partner HourlyPay HourlyPay(self, storage, partition_func=None) The pay given in the baseSalary field if salaryFrequency is hourly YearlyPay YearlyPay(self, storage, partition_func=None) The pay given in the baseSalary field if salaryFrequency is yearly SkillCounts SkillCounts(self, skill_extractor, *args, **kwargs) Adding top skill counts from a skill extractor Args: (skills_ml.algorithms.skill_extractors.base.SkillExtractorBase) A skill extractor object PostingIdPresent PostingIdPresent(self, storage, partition_func=None) Records job posting ids. Used for counting job postings skills_ml.job_postings.corpora CorpusCreator CorpusCreator(self, job_posting_generator=None, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], raw=False) A base class for objects that convert common schema job listings into a corpus in documnet level suitable for use by machine learning algorithms or specific tasks. Example from skills_ml.job_postings.common_schema import JobPostingCollectionSample from skills_ml.job_postings.corpora.basic import CorpusCreator job_postings_generator = JobPostingCollectionSample() # Default will include all the cleaned job postings corpus = CorpusCreator(job_postings_generator) # For getting a the raw job postings without any cleaning corpus = CorpusCreator(job_postings_generator, raw=True) Attributes job_posting_generator (generator) : an iterable that generates JSON strings. Each string is expected to represent a job listing conforming to the common schema See sample_job_listing.json for an example of this schema document_schema_fields (list) : an list of schema fields to be included raw (bool) : a flag whether to return the raw documents or transformed documents Yield (dict): a dictinary only with selected fields as keys and corresponding raw/cleaned value SimpleCorpusCreator SimpleCorpusCreator(self, job_posting_generator=None, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], raw=False) An object that transforms job listing documents by picking important schema fields and returns them as one large lowercased string Doc2VecGensimCorpusCreator Doc2VecGensimCorpusCreator(self, job_posting_generator, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], *args, **kwargs) Corpus for training Gensim Doc2Vec An object that transforms job listing documents by picking important schema fields and yields them as one large cleaned array of words Example from skills_ml.job_postings.common_schema import JobPostingCollectionSample from skills_ml.job_postings.corpora.basic import Doc2VecGensimCorpusCreator job_postings_generator = JobPostingCollectionSample() corpus = Doc2VecGensimCorpusCreator(job_postings_generator) Attributes: job_posting_generator (generator): a job posting generator document_schema_fields (list): an list of schema fields to be included <h2 id=\"skills_ml.job_postings.corpora.Word2VecGensimCorpusCreator\">Word2VecGensimCorpusCreator</h2> ```python Word2VecGensimCorpusCreator(self, job_posting_generator, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], *args, **kwargs) An object that transforms job listing documents by picking important schema fields and yields them as one large cleaned array of words JobCategoryCorpusCreator JobCategoryCorpusCreator(self, job_posting_generator=None, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], raw=False) An object that extract the label of each job listing document which could be onet soc code or occupationalCategory and yields them as a lowercased string SectionExtractWord2VecCorpusCreator SectionExtractWord2VecCorpusCreator(self, section_regex, *args, **kwargs) Only return the contents of the configured section headers. Heavily utilizes skills_ml.algorithms.nlp.section_extract. For more detail on how to define 'sections', refer to its docstring. RawCorpusCreator RawCorpusCreator(self, job_posting_generator, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills']) An object that yields the joined raw string of job posting skills_ml.job_postings.filtering Filtering streamed job postings soc_major_group_filter soc_major_group_filter(major_groups:List) -> Callable Return a function that checks the ONET Soc Code of a job posting (if it is present) against the configured major groups. JobPostingFilterer JobPostingFilterer(self, job_posting_generator:Generator[Dict[str, Any], NoneType, NoneType], filter_funcs:List[Callable]) Filter common schema job postings through a number of filtering functions Args job_posting_generator : An iterable of job postings (each in dict form) filter_funcs : A list of filtering functions, each taking in a job posting document (as dict) and returning a boolean instructing whether or not the posting passes the filter skills_ml.job_postings.geography_queriers Extracting geographies from job posting datasets job_posting_search_strings job_posting_search_strings(job_posting) Convert a job posting to a geocode-ready search string Includes city and state if present, or just city Args job_posting (dict) A job posting in schema.org/JobPosting json form Returns : (string) A geocode-ready search string skills_ml.job_postings.geography_queriers.base JobGeographyQuerier JobGeographyQuerier(self, /, *args, **kwargs) Base class for retrievers/computers of geography data from job postings The main interface is query(job_posting), which returns a tuple the same length as self.output_columns. Subclasses must implement output_columns (property/attribute): a collection of two-tuples with a name and description for each column output by the querier name (property/attribute) a name of the querier _query(job_posting) to take a job posting object and return a tuple of the same length as self.output_columns. skills_ml.job_postings.geography_queriers.cbsa Look up the CBSA for a job posting from a census crosswalk (job location -> Census Place -> Census UA -> Census CBSA) JobCBSAFromGeocodeQuerier JobCBSAFromGeocodeQuerier(self, geocoder, cbsa_finder) Queries the Core-Based Statistical Area for a job This object delegates the CBSA-finding algorithm to a passed-in finder. In practice, you can look at the skills_ml.algorithms.geocoders.cbsa module for an example of how this can be generated. Instead, this object focuses on the job posting-centric logic necessary, such as converting the job posting to the form needed to use the cache and dealing with differents kinds of cache misses. Args cbsa_finder (dict) A mapping of geocoding search strings to (CBSA FIPS, CBSA Name) tuples JobCBSAFromCrosswalkQuerier JobCBSAFromCrosswalkQuerier(self) Queries the Core-Based Statistical Area for a job using a census crosswalk First looks up a Place or County Subdivision by the job posting's state and city. If it finds a result, it will then take the Urbanized Area for that Place or County Subdivison and find CBSAs associated with it. Queries return all hits, so there may be multiple CBSAs for a given query. skills_ml.job_postings.geography_queriers.state skills_ml.job_postings.raw skills_ml.job_postings.raw.usajobs Import USAJobs postings into the Open Skills common schema USAJobsTransformer USAJobsTransformer(self, bucket_name=None, prefix=None, **kwargs) skills_ml.job_postings.raw.virginia VirginiaTransformer VirginiaTransformer(self, bucket_name=None, prefix=None, **kwargs) skills_ml.job_postings.sample Sample job postings JobSampler JobSampler(self, job_posting_generator, k, weights=None, key=<function JobSampler.<lambda> at 0x7f00f9c8d400>, random_state=None) Job posting sampler using reservoir sampling methods It takes a job_posting generator as an input. To sample based on weights, one should sepecify a weight dictionary. Attributes job_posting_generator (iterator) : Job posting iterator to sample from. k (int) : number of documents to sample weights (dict) : a dictionary that has key-value pairs as label-weighting pairs. It expects every label in the iterator to be present as a key in the weights dictionary For example, weights = {'11' : 2, '13', 1}. In this case, the label/key is the occupation major group and the value is the weight you want to sample with. key (callable) : a function to be called on each element to associate to the key of weights dictionary random_state (int) : the seed used by the random number generator","title":"Job Posting Dataset Processors"},{"location":"skills_ml.ontologies/","text":"skills_ml.ontologies.base Competency Competency(self, identifier:str, name:str=None, category:str=None, **kwargs) Represents a competency not necessarily tied to an ontology Args: identifier: A unique identifier for this competency. Choose the identifier wisely as it will be used for equivalence with other competency objects name: A name for the competency (e.g. Microsoft Office) category: An optional text category for the competency that is not a higher-level competency itself Occupation Occupation(self, identifier, name=None, **kwargs) Represents an occupation that may or may not be part of an ontology Args: identifier: A unique identifier for this occupation. Choose the identifier wisely as it will be used for equivalence with other occupation objects name: A name for the occupation (e.g. Civil Engineer) CompetencyOntology CompetencyOntology(self, edges=None) An ontology of competencies and occupations and the edges between them Can be initialized with a set of edges, in which case the competencies and occupations will be initialized with any that are present in the edge list skills_ml.ontologies.onet","title":"Skills ml.ontologies"},{"location":"skills_ml_tour/","text":"Skills-ML Tour \u00b6 Skills-ML is an open source software library for applying NLP and ML to labor market data. It allows the user to perform tasks like skill extraction and occupation classification to collections of documents such as job postings, profiles, and course descriptions. Competency \u00b6 A competency is any expertise or talent that is useful for a job. Developed capacities (e.g. active listening), proficiency with tools or technology (e.g. lancets, Microsoft Word), innate abilities (e.g. originality), and academic knowledge (e.g. medicine) are all considered competencies. from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='dino_riding', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) Competency Relationships \u00b6 Competencies are often related to each other. Defining parent-child relationships is a standard building block of existing competency frameworks like ONET and ESCO. A parent-child relationship generally implies that the child is a \"type of\" the parent. from skills_ml.ontologies import Competency from skills_ml.ontologies.viz import display_nodes dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) train_surfing = Competency( identifier='12346', name='Train Surfing', description='Standing on the train while it goes' ) time_travel = Competency( identifier='23456', name='Time Travel', description='Traveling Through Time' ) advanced_science = Competency( identifier='2345', name='Advanced Science', ) extreme_transportation = Competency( identifier='123', name='Extreme Transportation', description='Comically dangerous forms of transportation' ) time_travel.add_parent(advanced_science) dinosaur_riding.add_parent(extreme_transportation) train_surfing.add_parent(extreme_transportation) display_nodes([dinosaur_riding, train_surfing, extreme_transportation, time_travel, advanced_science]) Occupation \u00b6 An occupation is a job or profession that a person can hold. Similar to competencies, these are also often defined hierarchically. from skills_ml.ontologies import Occupation extreme_postal_workers = Occupation(identifier='999', name='Extreme Postal Workers') dino_postal_worker = Occupation(identifier='9998', name='Deliverer of Items to the Past') train_yard_postal_worker = Occupation(identifier='9999', name='Deliverer of Items to Train Yards') dino_postal_worker.add_parent(extreme_postal_workers) train_yard_postal_worker.add_parent(extreme_postal_workers) display_nodes([extreme_postal_workers, dino_postal_worker, train_yard_postal_worker]) CompetencyOntology \u00b6 A CompetencyOntology is a model of the labor market, or some subset thereof, consisting of a collection of competencies, a collection of occupations, and all of the relationships between them. from skills_ml.ontologies import CompetencyOntology from skills_ml.ontologies.viz import display_ontology ontology = CompetencyOntology() ontology.add_competency(dinosaur_riding) ontology.add_competency(train_surfing) ontology.add_competency(extreme_transportation) ontology.add_competency(time_travel) ontology.add_competency(advanced_science) ontology.add_occupation(dino_postal_worker) ontology.add_occupation(train_yard_postal_worker) ontology.add_occupation(extreme_postal_workers) ontology.add_edge(occupation=dino_postal_worker, competency=dinosaur_riding) ontology.add_edge(occupation=dino_postal_worker, competency=time_travel) ontology.add_edge(occupation=train_yard_postal_worker, competency=train_surfing) display_ontology(ontology) Prebuilt Ontologies \u00b6 To move on we'll want to level up to a full ontology. The example we'll use is O*NET, built from survey data and maintained by the US Department of Labor. A CompetencyOntology subclass that downloads the source files from the O*NET web site is included in Skills-ML. from skills_ml.ontologies.onet import Onet onet = Onet() onet.print_summary_stats() Ontology summary statistics for onet Num competencies: 32030 Num occupations: 1133 Num competency-occupation edges: 107305 Median occupations per competency: 1 Median competencies per occupation: 89 Mean occupations per competency: 3.350245090386837 Mean competencies per occupation: 94.70873786407768 list(onet.competencies)[0:5] [Competency(identifier=43232108-Rezgo, name=Rezgo, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232605-Hansen CAMA, name=Hansen CAMA, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=27111526-Arrow squaring devices, name=Arrow squaring devices, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=48101817-Marzipan tools, name=Marzipan tools, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=21111507-Commercial fishing line sinkers, name=Commercial fishing line sinkers, categories=['Tools', 'O*NET T2'], {})] Filtering \u00b6 ~34000 competencies and ~1100 occupations is a lot. Let's explore the filtering functionality of the CompetencyOntology to zoom in on a more specific slice. filter_by filters using edges: the filtering function it expects takes in an edge (between a Competency and Occupation) and returns whether or not it should be in the result. The result takes the form of a new CompetencyOntology, so you can interact with it in the same way as you would the source ontology. nurse_practitioners = onet.filter_by(lambda edge: 'Nurse Practitioners' in edge.occupation.name) nurse_practitioners.competencies {Competency(identifier=1.A.1.a.1, name=Oral Comprehension, categories=['Abilities'], {'competencyText': 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'}), Competency(identifier=1.A.1.a.2, name=Written Comprehension, categories=['Abilities'], {'competencyText': 'The ability to read and understand information and ideas presented in writing.'}), Competency(identifier=1.A.1.a.3, name=Oral Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in speaking so others will understand.'}), Competency(identifier=1.A.1.a.4, name=Written Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in writing so others will understand.'}), Competency(identifier=1.A.1.b.1, name=Fluency of Ideas, categories=['Abilities'], {'competencyText': 'The ability to come up with a number of ideas about a topic (the number of ideas is important, not their quality, correctness, or creativity).'}), Competency(identifier=1.A.1.b.2, name=Originality, categories=['Abilities'], {'competencyText': 'The ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.'}), Competency(identifier=1.A.1.b.3, name=Problem Sensitivity, categories=['Abilities'], {'competencyText': 'The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.'}), Competency(identifier=1.A.1.b.4, name=Deductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to apply general rules to specific problems to produce answers that make sense.'}), Competency(identifier=1.A.1.b.5, name=Inductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events).'}), Competency(identifier=1.A.1.b.6, name=Information Ordering, categories=['Abilities'], {'competencyText': 'The ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations).'}), Competency(identifier=1.A.1.b.7, name=Category Flexibility, categories=['Abilities'], {'competencyText': 'The ability to generate or use different sets of rules for combining or grouping things in different ways.'}), Competency(identifier=1.A.1.c.1, name=Mathematical Reasoning, categories=['Abilities'], {'competencyText': 'The ability to choose the right mathematical methods or formulas to solve a problem.'}), Competency(identifier=1.A.1.d.1, name=Memorization, categories=['Abilities'], {'competencyText': 'The ability to remember information such as words, numbers, pictures, and procedures.'}), Competency(identifier=1.A.1.e.1, name=Speed of Closure, categories=['Abilities'], {'competencyText': 'The ability to quickly make sense of, combine, and organize information into meaningful patterns.'}), Competency(identifier=1.A.1.e.2, name=Flexibility of Closure, categories=['Abilities'], {'competencyText': 'The ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material.'}), Competency(identifier=1.A.1.e.3, name=Perceptual Speed, categories=['Abilities'], {'competencyText': 'The ability to quickly and accurately compare similarities and differences among sets of letters, numbers, objects, pictures, or patterns. The things to be compared may be presented at the same time or one after the other. This ability also includes comparing a presented object with a remembered object.'}), Competency(identifier=1.A.2.a.1, name=Arm-Hand Steadiness, categories=['Abilities'], {'competencyText': 'The ability to keep your hand and arm steady while moving your arm or while holding your arm and hand in one position.'}), Competency(identifier=1.A.2.a.3, name=Finger Dexterity, categories=['Abilities'], {'competencyText': 'The ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.'}), Competency(identifier=1.A.4.a.1, name=Near Vision, categories=['Abilities'], {'competencyText': 'The ability to see details at close range (within a few feet of the observer).'}), Competency(identifier=1.A.4.b.4, name=Speech Recognition, categories=['Abilities'], {'competencyText': 'The ability to identify and understand the speech of another person.'}), Competency(identifier=1.A.4.b.5, name=Speech Clarity, categories=['Abilities'], {'competencyText': 'The ability to speak clearly so others can understand you.'}), Competency(identifier=2.A.1.a, name=Reading Comprehension, categories=['Skills'], {'competencyText': 'Understanding written sentences and paragraphs in work related documents.'}), Competency(identifier=2.A.1.b, name=Active Listening, categories=['Skills'], {'competencyText': 'Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.'}), Competency(identifier=2.A.1.c, name=Writing, categories=['Skills'], {'competencyText': 'Communicating effectively in writing as appropriate for the needs of the audience.'}), Competency(identifier=2.A.1.d, name=Speaking, categories=['Skills'], {'competencyText': 'Talking to others to convey information effectively.'}), Competency(identifier=2.A.1.f, name=Science, categories=['Skills'], {'competencyText': 'Using scientific rules and methods to solve problems.'}), Competency(identifier=2.A.2.a, name=Critical Thinking, categories=['Skills'], {'competencyText': 'Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.'}), Competency(identifier=2.A.2.b, name=Active Learning, categories=['Skills'], {'competencyText': 'Understanding the implications of new information for both current and future problem-solving and decision-making.'}), Competency(identifier=2.A.2.c, name=Learning Strategies, categories=['Skills'], {'competencyText': 'Selecting and using training/instructional methods and procedures appropriate for the situation when learning or teaching new things.'}), Competency(identifier=2.A.2.d, name=Monitoring, categories=['Skills'], {'competencyText': 'Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.'}), Competency(identifier=2.B.1.a, name=Social Perceptiveness, categories=['Skills'], {'competencyText': \"Being aware of others' reactions and understanding why they react as they do.\"}), Competency(identifier=2.B.1.b, name=Coordination, categories=['Skills'], {'competencyText': \"Adjusting actions in relation to others' actions.\"}), Competency(identifier=2.B.1.c, name=Persuasion, categories=['Skills'], {'competencyText': 'Persuading others to change their minds or behavior.'}), Competency(identifier=2.B.1.d, name=Negotiation, categories=['Skills'], {'competencyText': 'Bringing others together and trying to reconcile differences.'}), Competency(identifier=2.B.1.e, name=Instructing, categories=['Skills'], {'competencyText': 'Teaching others how to do something.'}), Competency(identifier=2.B.1.f, name=Service Orientation, categories=['Skills'], {'competencyText': 'Actively looking for ways to help people.'}), Competency(identifier=2.B.2.i, name=Complex Problem Solving, categories=['Skills'], {'competencyText': 'Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.'}), Competency(identifier=2.B.3.a, name=Operations Analysis, categories=['Skills'], {'competencyText': 'Analyzing needs and product requirements to create a design.'}), Competency(identifier=2.B.4.e, name=Judgment and Decision Making, categories=['Skills'], {'competencyText': 'Considering the relative costs and benefits of potential actions to choose the most appropriate one.'}), Competency(identifier=2.B.4.g, name=Systems Analysis, categories=['Skills'], {'competencyText': 'Determining how a system should work and how changes in conditions, operations, and the environment will affect outcomes.'}), Competency(identifier=2.B.4.h, name=Systems Evaluation, categories=['Skills'], {'competencyText': 'Identifying measures or indicators of system performance and the actions needed to improve or correct performance, relative to the goals of the system.'}), Competency(identifier=2.B.5.a, name=Time Management, categories=['Skills'], {'competencyText': \"Managing one's own time and the time of others.\"}), Competency(identifier=2.B.5.d, name=Management of Personnel Resources, categories=['Skills'], {'competencyText': 'Motivating, developing, and directing people as they work, identifying the best people for the job.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=41103901-Microhematocrit centrifuges, name=Microhematocrit centrifuges, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104102-Lancets, name=Lancets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104104-Tourniquets, name=Tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104107-Evacuated blood collection tubes, name=Evacuated blood collection tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104118-Specimen collection containers, name=Specimen collection containers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104403-Tissue culture incubators, name=Tissue culture incubators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41111709-Binocular light compound microscopes, name=Binocular light compound microscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41115807-Hemoglobin analyzers, name=Hemoglobin analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116138-Urinalysis test strips, name=Urinalysis test strips, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116201-Glucometers, name=Glucometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131606-Protective face shields, name=Protective face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131612-Protective gowns, name=Protective gowns, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42132203-Medical examination protective gloves, name=Medical examination protective gloves, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42141807-Transcutaneous electric nerve stimulation TENS equipment, name=Transcutaneous electric nerve stimulation TENS equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Nasal suctioning equipment, name=Nasal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Oral suctioning equipment, name=Oral suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Tracheal suctioning equipment, name=Tracheal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142509-Epidural catheters, name=Epidural catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142532-Pericardiocentesis kits, name=Pericardiocentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142537-Thoracentesis kits, name=Thoracentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142609-Hypodermic syringes, name=Hypodermic syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142616-Blood drawing syringes, name=Blood drawing syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142715-Urinary catheters, name=Urinary catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42144102-Chest tubes, name=Chest tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171608-Head immobilization devices, name=Head immobilization devices, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171613-Spinal immobilization equipment, name=Spinal immobilization equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172101-Automated external defibrillators AED, name=Automated external defibrillators AED, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172102-Cardiopulmonary resuscitation CPR face shields, name=Cardiopulmonary resuscitation CPR face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Electronic blood pressure monitors, name=Electronic blood pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Pediatric blood pressure cuffs, name=Pediatric blood pressure cuffs, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Electrocardiography EKG machines, name=Electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Portable electrocardiography EKG machines, name=Portable electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181713-Holter monitors, name=Holter monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181801-Pulse oximeters, name=Pulse oximeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181902-Intracranial pressure monitors, name=Intracranial pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Cardiac monitors, name=Cardiac monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Hemodynamic monitors, name=Hemodynamic monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Ophthalmoscopes, name=Ophthalmoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Otoscopes, name=Otoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182013-Vaginal exam specula, name=Vaginal exam specula, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182103-Mechanical stethoscopes, name=Mechanical stethoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182201-Digital medical thermometers, name=Digital medical thermometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182302-Reflex hammers, name=Reflex hammers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182412-Diagnostic tuning forks, name=Diagnostic tuning forks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182416-Tympanometers, name=Tympanometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182805-Medical scales, name=Medical scales, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Snellen eye charts, name=Snellen eye charts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Visual acuity testing cards, name=Visual acuity testing cards, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42192401-Crash carts, name=Crash carts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42201701-Doppler ultrasound equipment, name=Doppler ultrasound equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Angiocaths, name=Angiocaths, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Pulmonary artery catheters, name=Pulmonary artery catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Pacemaker analyzers, name=Pacemaker analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Transcutaneous pacemakers, name=Transcutaneous pacemakers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42211502-Crutches, name=Crutches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221501-Arterial line catheters, name=Arterial line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221503-Multiple lumen central line catheters, name=Multiple lumen central line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221506-Umbilical catheters, name=Umbilical catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV administration sets, name=Intravenous IV administration sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV cutdown trays, name=Intravenous IV cutdown trays, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42222001-Intravenous IV infusion pumps, name=Intravenous IV infusion pumps, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42231701-Nasogastric tubes, name=Nasogastric tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241507-Orthopedic splinting equipment, name=Orthopedic splinting equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241705-Lower extremity braces, name=Lower extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241707-Walking braces, name=Walking braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241801-Upper extremity braces, name=Upper extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241802-Back braces, name=Back braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241803-Neck braces, name=Neck braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42242103-Halo traction equipment, name=Halo traction equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271501-Apnea monitors, name=Apnea monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271502-Arterial blood gas monitoring equipment, name=Arterial blood gas monitoring equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271602-Incentive spirometers, name=Incentive spirometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271604-Peak flowmeters, name=Peak flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271702-Oxygen concentrators, name=Oxygen concentrators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271707-Oxygen flowmeters, name=Oxygen flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271708-Oxygen delivery masks, name=Oxygen delivery masks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271710-Nasal catheters, name=Nasal catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271802-Handheld nebulizers, name=Handheld nebulizers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271903-Endotracheal ET tubes, name=Endotracheal ET tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271910-Tracheotomy sets, name=Tracheotomy sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272001-Fiberoptic laryngoscopes, name=Fiberoptic laryngoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272017-Intubation sets, name=Intubation sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272201-Mechanical intermittent positive pressure ventilators, name=Mechanical intermittent positive pressure ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272203-Bilevel positive airway pressure BiPAP ventilators, name=Bilevel positive airway pressure BiPAP ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291613-Surgical scalpels, name=Surgical scalpels, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291614-Straight surgical scissors, name=Straight surgical scissors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291704-Biopsy punches, name=Biopsy punches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291802-Mosquito hemostats, name=Mosquito hemostats, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294702-Intra-aortic balloon pumps IABP, name=Intra-aortic balloon pumps IABP, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294717-Ventricular assist devices VAD, name=Ventricular assist devices VAD, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Fiberoptic endoscopes, name=Fiberoptic endoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Flexible sigmoidoscopes, name=Flexible sigmoidoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295104-Electrosurgical cauterization machines, name=Electrosurgical cauterization machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Argon lasers, name=Argon lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Carbon dioxide CO2 lasers, name=Carbon dioxide CO2 lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Pulsed dye lasers, name=Pulsed dye lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295122-Pneumatic tourniquets, name=Pneumatic tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312008-Surgical staple removers, name=Surgical staple removers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312010-Skin staplers, name=Skin staplers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312202-Suturing kits, name=Suturing kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43191507-Multi-line telephone systems, name=Multi-line telephone systems, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211503-Laptop computers, name=Laptop computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211504-Personal digital assistants PDA, name=Personal digital assistants PDA, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211508-Personal computers, name=Personal computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211509-Tablet computers, name=Tablet computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43231507-Microsoft SharePoint, name=Microsoft SharePoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43231513-Microsoft Office, name=Microsoft Office, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232104-Microsoft Word, name=Microsoft Word, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232106-Microsoft PowerPoint, name=Microsoft PowerPoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232110-Microsoft Excel, name=Microsoft Excel, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232306-Microsoft Access, name=Microsoft Access, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Allscripts Professional EHR, name=Allscripts Professional EHR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Amkai AmkaiCharts, name=Amkai AmkaiCharts, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Bizmatics PrognoCIS EMR, name=Bizmatics PrognoCIS EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Cerner Millennium, name=Cerner Millennium, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-GE Healthcare Centricity EMR, name=GE Healthcare Centricity EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Healthcare common procedure coding system HCPCS, name=Healthcare common procedure coding system HCPCS, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MEDITECH software, name=MEDITECH software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical condition coding software, name=Medical condition coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical procedure coding software, name=Medical procedure coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medscribbler Enterprise, name=Medscribbler Enterprise, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MicroFour PracticeStudio.NET EMR, name=MicroFour PracticeStudio.NET EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-NextGen Healthcare Information Systems EMR, name=NextGen Healthcare Information Systems EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-PCC Pediatric Partner, name=PCC Pediatric Partner, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SOAPware EMR, name=SOAPware EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-StatCom Patient Flow Logistics Enterprise Suite, name=StatCom Patient Flow Logistics Enterprise Suite, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SynaMed EMR, name=SynaMed EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Texas Medical Software SpringCharts EMR, name=Texas Medical Software SpringCharts EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-e-MDs software, name=e-MDs software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-eClinicalWorks, name=eClinicalWorks, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Microsoft Internet Explorer, name=Microsoft Internet Explorer, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Web browser software, name=Web browser software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43233501-Microsoft Outlook, name=Microsoft Outlook, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=46181804-Safety goggles, name=Safety goggles, categories=['Tools', 'O*NET T2'], {})} That's a big list of competencies. Each competency has a list of categories, so let's get all of the different possible categories set(cat for competency in nurse_practitioners.competencies for cat in competency.categories) {'Abilities', 'Knowledge', 'O*NET T2', 'Skills', 'Technology', 'Tools'} So we can ask questions like: what type of Knowledge do nurse practitioners need? [competency for competency in nurse_practitioners.competencies if 'Knowledge' in competency.categories] [Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'})] There are many other questions we can ask of just an ontology, but the real value will come from combining the knowledge contained in the ontology with larger unstructured datasets. In this next section we will explore how Skills-ML helps the user deal with such datasets as job postings, profiles, or course descriptions. Spirit of Skills-ML \u00b6 Dataflow Programming : Skills-ML 's design philosophy builds on dataflow programming or so called data streaming to process very large datasets (larger than RAM; potentially infinite). One-pass algorithm : Data points are processed one at a time. Lazy evaluation : an evaluation strategy which delays the evaluation of an expression until its value is needed. In Skills-ML , most of the classes and functions here incorporates the concept of Iterable or Generator . We build the expression first and evaluate later. Creating Dataset \u00b6 Before we do anything with the context, we need dataset. Skills-ML makes use of schema.org\u2019s JobPosting standard. As it has been in use for a long time, some open sources are already using this standard, which is easy to import. Other job posting data sources are converted into the schema.org Schema and all work on job postings is done using this standard schema. In Skills-ML , job_postings module has all the functionalities to create the data we need for later usage. Common Schema \u00b6 We have an useful function to help create the data generator from s3. JobPostingCollectionFromS3 : Stream job posting from s3. JobPostingCollectionSample : Stream a finite number of job postings stored within the library. However, we are not restrcted to just JobPosting data. One can easily create whatever data generator such as ProfileGenerator or CourseGenerator . For example, we want to use the Vrigina Dataset which is an open data set of job postings. We just have to create a job posting generator with some transformation. from skills_ml.job_postings.raw.virginia import VirginiaTransformer from urllib.request import urlopen import json va_url = \"http://opendata.cs.vt.edu/dataset/ab0abac3-2293-4c9d-8d80-22d450254389/resource/074f7e44-9275-4bba-874e-4795e8f6830c/download/openjobs-jobpostings.may-2016.json\" class VAJobposting(object): def __init__(self, uri): self.uri = uri def __iter__(self): request = urlopen(self.uri) for line in request.readlines(): raw = json.loads(line) yield VirginiaTransformer(partner_id=\"va\")._transform(raw) jobpostings_va = VAJobposting(va_url) print(len(list(jobpostings_va))) 40098 Filtering \u00b6 To create a good dataset, we might want to have some criteria for choosing the proper job posting based on the task we want to perform, like job postings that have the label information, job postings that belong to certain occupation, or job postings that have rich enough information in the description field. JobPostingFilterer : Filter common schema job postings through a number of filtering functions. This function also follows lazy evaluation strategy. from skills_ml.job_postings.filtering import JobPostingFilterer def is_tech_jobs(job): if job['onet_soc_code'][:2] in ['15', '17', '19']: return True else: return False tech_jobs = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[is_tech_jobs] ) from skills_ml.ontologies.onet import majorgroupname from collections import Counter import pandas as pd import matplotlib.pyplot as plt import matplotlib import seaborn as sns sns.set(style=\"darkgrid\", font_scale=2) %matplotlib inline # major group distribution plotting function def plot_major_group_distribution(job_postings): c = Counter() for job in job_postings: c.update([job['onet_soc_code'][:2]]) s = pd.Series(c).sort_index() s.index = s.index.map(majorgroupname) ax = s.plot.bar(figsize=(20,10),rot=90) ax.set_xlabel('soc_major_group') ax.set_ylabel('number of job posting') ax.set_title(f\"total number: {s.sum()}\") return s plot_major_group_distribution(tech_jobs) Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 dtype: int64 What if we want to make sure that all the job postings have ONet SOC Code and it's not unknown(first 2 digit 99)? We can define filter functions like these which can be either generic function or lambda function. def filter_onet_soc_code(job): if job['onet_soc_code'] and job['onet_soc_code'][:2] != '99': return True else: return False has_soc = lambda x: x['onet_soc_code'] not_unknown_soc = lambda x: x['onet_soc_code'][:2] != '99' jobpostings_filtered = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[has_soc, not_unknown_soc] ) plot_major_group_distribution(jobpostings_filtered) Management 6506 Business and Financial Operations 3867 Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 Community and Social Service 282 Legal 94 Education, Training, and Library 679 Arts, Design, Entertainment, Sports, and Media 598 Healthcare Practitioners and Technical 3447 Healthcare Support 494 Protective Service 484 Food Preparation and Serving Related 792 Building and Grounds Cleaning and Maintenance 189 Personal Care and Service 97 Sales and Related 1415 Office and Administrative Support 2580 Construction and Extraction 196 Installation, Maintenance, and Repair 832 Production 442 Transportation and Material Moving 2722 Military Specific 2 dtype: int64 Random Sampling \u00b6 Even though we have a lot of data, most of time we don't need all of them to do the analysis. Or we can't even fit all the data into memory to do the analysis. What we need more importantly is a suitable sampled dataset. JobSampler : Sample job posting by (weighted) reservoir sampling. Random Sampling from Streaming Data - Reservoir Sampling \u00b6 \"Say you have a stream of items of large and unknown length that we can only iterate over once.\" It's memeory efficient and just one iteration There is a great overview of reservoir sampling in https://gregable.com/2007/10/reservoir-sampling.html. Let's say the original job postings dataset are too much for my Mac Yosemite to do any analysis and I want only 1000 job postings but still preserve the statistical characteristics of the original dataset. from skills_ml.job_postings.sample import JobSampler sampler = JobSampler( job_posting_generator=jobpostings_filtered, k=1000, ) plot_major_group_distribution(sampler) Management 191 Business and Financial Operations 126 Computer and Mathematical 134 Architecture and Engineering 65 Life, Physical, and Social Science 5 Community and Social Service 11 Legal 1 Education, Training, and Library 25 Arts, Design, Entertainment, Sports, and Media 15 Healthcare Practitioners and Technical 106 Healthcare Support 20 Protective Service 16 Food Preparation and Serving Related 27 Building and Grounds Cleaning and Maintenance 5 Personal Care and Service 1 Sales and Related 51 Office and Administrative Support 75 Construction and Extraction 6 Installation, Maintenance, and Repair 19 Production 12 Transportation and Material Moving 89 dtype: int64 Something wrong happened! We are missing Military Occupations ! Because military job postings are extremely rare in the original dataset, simple ramdom sampling might result in lack of classes. Weighted Reservoir Sampling \u00b6 How would you sample from a weighted distribution where each element has a given weight associated with it in the stream? For certain task, we need some curated sample. For example, if we want to build a occupation classifier, we want similar amounts of job posting for each occupation. Now we want to have a more uniform distributed sample across all major groups. Here we need to provide a weight dictionary in the JobSampler c = Counter() for job in jobpostings_filtered: c.update([job['onet_soc_code'][:2]]) weights = dict() for key, value in c.items(): weights[key] = max(c.values()) / value weights {'11': 1.0, '15': 1.2845014807502468, '17': 3.3588022715539494, '29': 1.8874383521903104, '41': 4.597879858657244, '43': 2.521705426356589, '13': 1.6824411688647531, '49': 7.819711538461538, '33': 13.442148760330578, '27': 10.879598662207357, '47': 33.19387755102041, '51': 14.71945701357466, '35': 8.214646464646465, '25': 9.581737849779087, '31': 13.17004048582996, '19': 33.885416666666664, '21': 23.070921985815602, '37': 34.423280423280424, '53': 2.3901542983100663, '39': 67.0721649484536, '23': 69.2127659574468, '55': 3253.0} sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=1000, key=lambda x: x['onet_soc_code'][:2], weights=weights) plot_major_group_distribution(sampler) Management 54 Business and Financial Operations 47 Computer and Mathematical 58 Architecture and Engineering 47 Life, Physical, and Social Science 39 Community and Social Service 53 Legal 43 Education, Training, and Library 47 Arts, Design, Entertainment, Sports, and Media 54 Healthcare Practitioners and Technical 43 Healthcare Support 46 Protective Service 48 Food Preparation and Serving Related 41 Building and Grounds Cleaning and Maintenance 49 Personal Care and Service 36 Sales and Related 48 Office and Administrative Support 48 Construction and Extraction 62 Installation, Maintenance, and Repair 44 Production 44 Transportation and Material Moving 47 Military Specific 2 dtype: int64 Skill Extraction \u00b6 A common task is extracting competencies from unstructured text. Sometimes this is ontology-based (finding concepts from a known ontology in text), but this is not necessarily true. Skills-ML unites these with a common interface in the SkillExtractor class. The common interface is that every SkillExtractor needs to be able to take in a collection of documents, and yield what we call CandidateSkill objects. What Is a CandidateSkill? \u00b6 A CandidateSkill is a possible occurrence of a skill/competency in context in some document. It consists of the following fields: skill_name - The text version of the skill as it appears in the document matched_skill_identifier - A reference to the skill in some ontology. This may be empty, if no ontology was used to search for skills. context - The text surrounding the skill in the document. The goal is for a human labeler to be able to use this to determine whether or not the occurrence represents a true skill. How much context is included is up to the algorithm. start_index - The start index of the skill occurrence within the document string. confidence - The confidence level the algorithm has in this candidate skill being a true occurrence of a skill. This may be empty, if the algorithm has now way of producing a confidence value. document_id - A unique identifier for the source document. document_type - The type of document (examples: Job Posting, Profile, Course Description) source_object - The entire source document. skill_extractor_name - The name of the skill extractor algorithm. Every SkillExtractor subclass defines a name property that is used to give processes downstream context about how their output data was produced. The idea behind the CandidateSkill object is to serve as a common interface between SkillExtractor objects, automatic evaluation methods, and manual evaluation methods. A labeling interface might intake CandidateSkill objects for humans to say yes/no to. Another type of labeling interface might involve the export of CandidateSkill objects based on what a human highlighted in the interface when shown the entire document Unsupervised evaluation metrics may take in one set of CandidateSkills to produce simple descriptive metrics Supervised evaluation metrics may take in one set of CandidateSkills from a SkillExtractor and another set of CandidateSkills from a human labeling interface and use the latter to evaluate the former We'll talk about some of these use cases in more detail later. But for now, let's start with a simple example that uses NLP rules and isn't tied to an ontology. Let's define a method for extracting skills as 'all noun phrases that end in the word skill or skills'. This is a simple method that realistically won't cover all possible occurrences of skills, but this is a start. from skills_ml.algorithms.skill_extractors import SkillEndingPatternExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample job_posting_generator = JobPostingCollectionSample() # instantiate the skill extractor. This class defaults to only considering lines that # start with a bullet, which doesn't work for this dataset. So we set this flag to False. skill_extractor = SkillEndingPatternExtractor(only_bulleted_lines=False) job_posting = next(iter(job_posting_generator)) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: communication skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: organizational skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: communication skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required skill name: organizational skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required The results for one job posting are modest. Two distinct skill names, each occurring two different times in the document. This is a start. Now let's try another skill extractor: matching with ONET data. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: self context: The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric. skill name: self context: We are proud to be an equal opportunity employer College degree preferred, 2-5 years experience in print and/or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients/prospects is required, Minimal non local travel a few times a year is required Yikes. What is this? As it turns out, 'Self' is a real programming language . But it's not applicable here. Simply searching for skill names has its limitations. To help with this, there is also the SocScopedExactMatchSkillExtractor. This does exact matching, but only for the occupation that the document is tagged with. This, of course, is only applicable if the document has one. And it needs a full CompetencyOntology to work. from skills_ml.algorithms.skill_extractors import SocScopedExactMatchSkillExtractor skill_extractor = SocScopedExactMatchSkillExtractor(onet) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') No results. This is expected: For an occupation that is not related to computer programming, the language 'Self' is likely irrelevant. Here's a list of all the other skill extractors available. FuzzyMatchSkillExtractor - Similar to the ExactMatchSkillExtractor, but using a configurable edit distance to find skill names that are very close to the targets. AbilityEndingPatternExtractor - Similar to the SkillEndingPatternExtractor, but finding noun phrases that end in 'ability' or 'abilities'. SectionExtractSkillExtractor - Attempts to divide the text into sections with headers, which is a common pattern found in job postings. Return each individual sentence found in sections with certain headers (Skills, Competencies, Qualifications). Evaluating Skill Extractors \u00b6 We want to be able to evaluate skill extractors. We may or may not have labeled skills but do want to be able to generate descriptive metrics either way. from skills_ml.evaluation.skill_extraction_metrics import TotalOccurrences, TotalVocabularySize, OntologyCompetencyRecall metrics = [ TotalOccurrences(), TotalVocabularySize(), OntologyCompetencyRecall(onet) ] exact_match_skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for metric in metrics: candidate_skills = [] for job_posting in job_posting_generator: candidate_skills += list(exact_match_skill_extractor.candidate_skills(job_posting)) print('metric:', metric.name, 'value:', metric.eval(candidate_skills, 50)) metric: total_candidate_skills value: 153 metric: total_vocabulary_size value: 40 metric: onet_ksat_competency_recall value: 0.001248868213181804 Embedding \u00b6 Labor market data tends to be large in scale, but represented as raw text. Consequently, an important early step for most tasks is to transform texts into a mathematical form that can be used in the downstream tasks. In the context of skills and jobs, an embedding model trained on large amount of job posting data is able to map a skill or a job title into a high dimensional space as well as preserving the contextual and semantic relationship. Ideally, a good embedding model will cluster similar skills and jobs. Embedding Models \u00b6 Many word embedding techniques have been developed since the most impactful embedding algorithm word2vec was published in 2013. Currently, Skills-ML includes word2vec, doc2vec and fastext and may include more in the future. Word2VecModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, a random vector will be assigned with the same dimension. Doc2VecModel is able to look up a word vector and infer a sentence/paragraph vector by gradient descending on the fly, so it is non-deterministic. It does not support online learning. FastTextModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, it sums all vectors of the unseen word\u2019s char-ngrams. If none of the char-ngrams of the unseen word is present, a random vector will be assigned with the same dimension. from skills_ml.algorithms.embedding.models import Word2VecModel, FastTextModel cbow = Word2VecModel(size=200, sg=0, window=7, iter=3, batch_words=1000) skip_gram = Word2VecModel(size=200, sg=1, window=7, iter=3, batch_words=1000) fasttext = FastTextModel(size=200, window=7, iter=3, batch_words=1000) Corpora \u00b6 Next, we need some text corpus to train embedding modelss. Skills-ML provides pre-defined classes to convert common schema job listings into a corpus in documnet level suitable for use by machine learning algorithms or specific tasks. Word2VecGensimCorpusCreator Doc2VecGensimCorpusCreator from skills_ml.job_postings.corpora import Word2VecGensimCorpusCreator, Doc2VecGensimCorpusCreator sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) w2v_corpus_generator = Word2VecGensimCorpusCreator(sampler) Preprocessing \u00b6 Or we can build our own corpus generator by using some preprocessing tools Function Compostition - ProcessingPipeline will compose processing functions together to become a callable object that takes in the input from the very first processing function and returns the output of the last processing function. - IterablePipeline will compose processing functions together to be passed to different stages(training/ prediction) from skills_ml.algorithms.preprocessing import IterablePipeline from skills_ml.algorithms import nlp from functools import partial document_schema_fields = ['description','experienceRequirements', 'qualifications', 'skills'] pipeline = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_html, nlp.clean_str, nlp.word_tokenize, ) corpus_generator = pipeline(sampler) Train Embedding \u00b6 The EmbeddingTrainer provides online batch learning for Word2VecModel and FastTextModel. from skills_ml.algorithms.embedding.train import EmbeddingTrainer trainer = EmbeddingTrainer(cbow, skip_gram, fasttext, batch_size=100) trainer.train(corpus_generator) Storage \u00b6 Skills-ML has couple useful storage classes that could benefit both local or cloud. - S3Store : S3 storage engine - FSStore : File system storage engine - ModelStorage : Serialization model storage. from skills_ml.storage import FSStore, S3Store, ModelStorage fs = FSStore(path=\"tmp/model_cache/embedding/examples\") trainer.save_model(storage=fs) print(cbow.model_name) print(cbow.storage) word2vec_7bdfa911ccc14b971f92b35d529c1dc6.model FSStore(path=tmp/model_cache/embedding/examples) Examples \u00b6 for c, s in zip(cbow.wv.most_similar(['engineer']), skip_gram.wv.most_similar(['engineer'])): print(c, s) ('developer', 0.7927991151809692) ('developer', 0.6523622870445251) ('analyst', 0.7418307662010193) ('tester', 0.624006986618042) ('sr', 0.6851106882095337) ('rhev', 0.6191227436065674) ('designer', 0.6370913982391357) ('devops', 0.6076809167861938) ('devops', 0.6367722749710083) ('modis', 0.5969630479812622) ('intern', 0.6265694499015808) ('designer', 0.5858316421508789) ('inc', 0.6245660781860352) ('writer', 0.568120002746582) ('architect', 0.619118332862854) ('chantilly', 0.5666274428367615) ('scientist', 0.6182761192321777) ('island', 0.5646683573722839) ('tester', 0.5959925651550293) ('herndon', 0.5613434910774231) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. if np.issubdtype(vec.dtype, np.int): for c, s in zip(cbow.wv.most_similar(['python']), skip_gram.wv.most_similar(['python'])): print(c, s) ('perl', 0.9427046775817871) ('perl', 0.9121636152267456) ('ruby', 0.908499002456665) ('ruby', 0.8980081081390381) ('scripting', 0.9076182842254639) ('bash', 0.8887742757797241) ('languages', 0.9037582874298096) ('mysql', 0.8292362689971924) ('programming', 0.8735607862472534) ('centos', 0.821326494216919) ('linux', 0.8646563291549683) ('jsp', 0.8192467093467712) ('java', 0.8616237640380859) ('ee', 0.8136248588562012) ('javascript', 0.8528693914413452) ('languages', 0.8108319044113159) ('css', 0.8422471284866333) ('xml', 0.8019323348999023) ('xml', 0.8400859832763672) ('scripting', 0.8000487089157104) for c, s in zip(cbow.wv.most_similar(['cnc']), skip_gram.wv.most_similar(['cnc'])): print(c, s) ('confined', 0.7270200252532959) ('machining', 0.7596360445022583) ('machining', 0.7186369895935059) ('press', 0.7382968664169312) ('grinders', 0.6886007785797119) ('igniters', 0.7340461015701294) ('graphs', 0.6842017769813538) ('turbines', 0.7310390472412109) ('feed', 0.6837735772132874) ('harness', 0.7257314920425415) ('lathes', 0.6761114597320557) ('lathes', 0.7163777351379395) ('controllers', 0.6728384494781494) ('ntcss', 0.7149447202682495) ('wood', 0.6689127683639526) ('comms', 0.7094639539718628) ('toxic', 0.6676516532897949) ('xendesktop', 0.7087377905845642) ('wire', 0.6662098169326782) ('metal', 0.707713782787323) Skills-ML also provides a function to visualize the embedding in tensorboard from skills_ml.algorithms.embedding.models import visualize_in_tensorboard visualize_in_tensorboard(cbow) Run `tensorboard --logdir=/home/ubuntu/skills-ml/word2vec_7bdfa911ccc14b971f92b35d529c1dc6 --host 127.0.0.1` to run visualize result on tensorboard Evaluation \u00b6 Although there is an emerging trend towards generating embeddings for structured and unstructured data, there is not yet any systematic suite for measuring the quality of embeddings. We generally follow one of the few works in embedding evaluation [Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts] to create metrics for evaluating embedding against the gold standard ontology dataset. The gold standard ontology is curated by domain experts like O*NET, so a good embedding should replicate the structure of the entities in the gold standard taxonomy. In other words, it is useful to see how an embedding reflects the clustering structure. One trivial clustering is Major Groups of occupations. A good embedding should cluster the occupations which belong to the same major groups. CategorizationMetric : The cosine similarity between the embedding of the concept and the mean vector of embeddings of all the entities within that concept cluster. This metric aligns a clustering of entities into different categories, reflecting how well the embedding of a concept cluster performs as the background concept of the entities typed by it. IntraClusterCohesion : The sum of squared error of the embedding of the centroid of the concept cluster and the embedding of each entities within that cluster. It measures how near the data points in a cluster are to the cluster centroid. MajorGroupRecall : For a major group, calculate the cosine similarity against all the occupations and find the top n closest occupations. The recall is defined as the number of true positives from top n closest occupations divided by the total number of occupation within the major group. MajorGroupPrecision : Similarly to MajorGroupRecall which is called Coherence Score in the paper, start by finding the top n closest occupations. The precision is defined as the number of true positives from top n closest occupations divided by n from skills_ml.ontologies.onet import Onet major_group_occupation_des_clustering = onet.major_group_occupation_description_clustering from skills_ml.evaluation.embedding_metrics import metrics_for_embedding, CategorizationMetric, IntraClusterCohesion, RecallTopN, PrecisionTopN from skills_ml.algorithms.preprocessing import ProcessingPipeline def vectorization(embedding): p = ProcessingPipeline( nlp.normalize, nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=embedding) ) return p categorization_metric = CategorizationMetric(major_group_occupation_des_clustering) intra_cohesion = IntraClusterCohesion(major_group_occupation_des_clustering) recall_top = RecallTopN(major_group_occupation_des_clustering, topn=10) precision_top = PrecisionTopN(major_group_occupation_des_clustering, topn=10) categorization_metric.eval(vectorization(fasttext)) {'Building and Grounds Cleaning and Maintenance': 0.34224581131861653, 'Food Preparation and Serving Related': 0.2654245949239141, 'Construction and Extraction': 0.47868056098318057, 'Healthcare Practitioners and Technical': 0.3940974596927912, 'Transportation and Material Moving': 0.3232632841344041, 'Computer and Mathematical': 0.2728775771822607, 'Life, Physical, and Social Science': 0.5065041654514952, 'Farming, Fishing, and Forestry': 0.46221691564956713, 'Installation, Maintenance, and Repair': 0.18825461785204867, 'Business and Financial Operations': 0.4141572197688639, 'Education, Training, and Library': 0.4499782804964112, 'Military Specific': 0.8406518707733225, 'Production': 0.767452990807525, 'Office and Administrative Support': 0.6310052316123922, 'Healthcare Support': 0.9414934205345233, 'Sales and Related': 0.318627279388384, 'Protective Service': 0.7164475858481743, 'Management': 0.6463669464049742, 'Personal Care and Service': 0.5464152492907202, 'Architecture and Engineering': 0.34074216780013, 'Community and Social Service': 0.39121662365108567, 'Arts, Design, Entertainment, Sports, and Media': 0.4462032355049601, 'Legal': 0.5343651523623081} import statistics import operator from collections import defaultdict # We define some helper functions to evaluate multiple embeddings def algorithm_name(emb): if emb.model_type == 'word2vec' or emb.model_type == 'fasttext': if getattr(emb, 'sg', None) == 1: return 'Skip-Gram' else: return 'Continuous Bag of Words' elif emb.model_type == 'doc2vec': if getattr(emb, 'dm', None) == 1: return 'Distributed Memory' else: return 'Distributed Bag of Words' def evaluate_multiple_embeddings(embeddings, vectorization, metric): result = defaultdict(dict) for emb in embeddings: c = metric.eval(vectorization(emb)) name = emb.model_name.split('.')[0] result[name]['mean'] = statistics.mean(list(c.values())) result[name]['variance'] = statistics.variance(list(c.values())) result[name]['std'] = statistics.stdev(list(c.values())) result[name]['max'] = max(c.items(), key=operator.itemgetter(1))[1] result[name]['max_cluster'] = max(c.items(), key=operator.itemgetter(1))[0] result[name]['min'] = min(c.items(), key=operator.itemgetter(1))[1] result[name]['min_cluster'] = min(c.items(), key=operator.itemgetter(1))[0] result[name]['type'] = emb.model_type result[name]['algorithm'] = algorithm_name(emb) result[name]['window'] = emb.window return pd.DataFrame(result) evaluate_multiple_embeddings([cbow, skip_gram, fasttext], vectorization, categorization_metric) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word2vec_7bdfa911ccc14b971f92b35d529c1dc6 word2vec_fee333d52ec0101d0e95848a8f376d37 fasttext_6885e864e01553bde10e120d40491de1 algorithm Continuous Bag of Words Skip-Gram Continuous Bag of Words max 0.951364 0.617921 0.941493 max_cluster Military Specific Production Healthcare Support mean 0.49476 0.349691 0.487769 min 0.149769 0.169084 0.188255 min_cluster Installation, Maintenance, and Repair Building and Grounds Cleaning and Maintenance Installation, Maintenance, and Repair std 0.226956 0.119819 0.193501 type word2vec word2vec fasttext variance 0.0515092 0.0143566 0.0374426 window 7 7 7 Occupation Classification \u00b6 A common issue with job posting data is incomplete, incorrect, and inconsistent occupation classification. The majority of job postings in the US are using the O*NET SOC classification system, but many are either missing or poorly classified. This can be improved by using machine learning. SOC Codes \u00b6 Most of the job posting data collected are aligned with the O*NET SOC system. The occupations in the SOC are classified at four levels of aggregation: major group, minor group, broad occupation, and detailed occupation. Each lower level of detail identifies a more specific group of occupations. Each item in the SOC is designated by a six-digit code. The first two digits represent the major group, the third digit represents the minor group, the fourth and fifth digits represent the broad occupation, and the sixth digit represents the detailed occupation. - Major group codes end with 0000 (e.g., 29-0000 Healthcare Practitioners and Technical Occupations \u2014the exceptions are minor groups 15-1200 Computer Occupations, 31- 1100 Home Health and Personal Care Aides; and Nursing Assistants, Orderlies, and Psychiatric Aides, and 51-5100 Printing Workers, which end with 00). - Minor groups generally end with 000 (e.g., 29-1000 Health Diagnosing or Treating Practitioners). - Broad occupations end with 0 (e.g., 29-1020 Dentists). - Detailed occupations end with a number other than 0 (e.g., 29-1022 Oral and Maxillofacial Surgeons). Target Variable \u00b6 FullSOC SOCMajorGroup from skills_ml.algorithms.occupation_classifiers import FullSOC, SOCMajorGroup full_soc = FullSOC(onet_cache=onet) Design Matrix \u00b6 The classification task consists of inferring a SOC code from a job posting and is accomplished through several stages: preprocessing, filtering, training and testing. DesignMatrix helps users accomplish this task. import random from itertools import islice from skills_ml.utils import itershuffle from skills_ml.algorithms.occupation_classifiers import DesignMatrix sample = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) dataset = itershuffle(sample) train = islice(dataset, 0, 4000) test = islice(dataset, 4000) pipe_x = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=fasttext) ) pipe_y = IterablePipeline( full_soc.transformer ) matrix = DesignMatrix( train, full_soc, pipe_x, pipe_y, ) OccupationClassifierTrainer \u00b6 OccupationClassifierTrainer trains classifiers with cross validation and picks the best classifier with a grid search based on the metric. It takes in a dictionary for the grid search. from skills_ml.algorithms.occupation_classifiers.train import OccupationClassifierTrainer grid_config = { 'sklearn.ensemble.ExtraTreesClassifier': { 'n_estimators': [50, 100], 'criterion': ['entropy', 'gini'], 'max_depth': [20], 'max_features': ['log2'], 'min_samples_split': [10] }, 'sklearn.neural_network.MLPClassifier': { 'hidden_layer_sizes': [100, 500], 'activation': ['logistic', 'relu'], 'solver': ['adam'] }, } cls_trainer = OccupationClassifierTrainer( matrix=matrix, k_folds=3, grid_config=grid_config, storage=FSStore('tmp/model_cache/soc_classifiers/examples'), n_jobs=4) cls_trainer.train(save=False) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1d /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) cls_trainer.best_estimators [<ProxyObjectWithStorage at 0x7f568b8360f8 for GridSearchCV at 0x7f5688979358>, <ProxyObjectWithStorage at 0x7f568b38d888 for GridSearchCV at 0x7f5688c20d68>] Evaluation \u00b6 Accuracy, recall, precision and f1 are the metrics taken into consideration. Since it is a multi-class classification problem, an overall performance is evaluated by looking at the micro-average and macro-average for the metrics. A macro-average will compute the metric independently for each class and then take the average, whereas a micro-average will aggregate the contributions of all classes and then computes the average. In other words, a macro-average is treating all classes equally. from skills_ml.algorithms.occupation_classifiers.test import OccupationClassifierTester from skills_ml.evaluation.occ_cls_evaluator import OnetOccupationClassificationEvaluator from skills_ml.algorithms.occupation_classifiers.classifiers import CombinedClassifier from skills_ml.algorithms.embedding.train import Reiterable steps = [ partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.normalize, nlp.clean_str, nlp.word_tokenize, ] evaluators = [] test_data = list(test) for cls in cls_trainer.best_estimators: tester = OccupationClassifierTester( test_data_generator=test_data, preprocessing=steps, classifier=CombinedClassifier(fasttext, cls) ) evaluators.append(OnetOccupationClassificationEvaluator(tester)) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'itershuffle' raised StopIteration for e, c in zip(evaluators, cls_trainer.best_estimators): print(c.best_estimator_) print('accuracy: ', e.accuracy) print('precision: ', e.precision) print('f1: ', e.f1) print('major group: ', e.accuracy_major_group) print('macro precision: ', e.macro_precision) print('micro precision: ', e.micro_precision) print('recall: ', e.recall) print('macro recall: ', e.macro_recall) print('micro recall: ', e.micro_recall) print('macro f1: ', e.macro_f1) print('micro f1: ', e.micro_f1) print('\\n') ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini', max_depth=20, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4, oob_score=False, random_state=None, verbose=0, warm_start=False) accuracy: 0.478 precision: [0. 0. 0.10891089 ... 0. 0. 0. ] f1: [0. 0. 0.18032787 ... 0. 0. 0. ] major group: 0.554 macro precision: 0.3553188734850493 micro precision: 0.478 recall: [0. 0. 0.52380952 ... 0. 0. 0. ] macro recall: 0.2807984722668254 micro recall: 0.478 macro f1: 0.28787327281642394 micro f1: 0.478 MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=500, learning_rate='constant', learning_rate_init=0.001, max_iter=200, momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) accuracy: 0.514 precision: [0. 0. 0.14285714 ... 0. 0. 0. ] f1: [0. 0. 0.2 ... 0. 0. 0. ] major group: 0.618 macro precision: 0.2966284764221296 micro precision: 0.514 recall: [0. 0. 0.33333333 ... 0. 0. 0. ] macro recall: 0.2887478119504879 micro recall: 0.514 macro f1: 0.27660324759833993 micro f1: 0.514 /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for)","title":"Tour"},{"location":"skills_ml_tour/#skills-ml-tour","text":"Skills-ML is an open source software library for applying NLP and ML to labor market data. It allows the user to perform tasks like skill extraction and occupation classification to collections of documents such as job postings, profiles, and course descriptions.","title":"Skills-ML Tour"},{"location":"skills_ml_tour/#competency","text":"A competency is any expertise or talent that is useful for a job. Developed capacities (e.g. active listening), proficiency with tools or technology (e.g. lancets, Microsoft Word), innate abilities (e.g. originality), and academic knowledge (e.g. medicine) are all considered competencies. from skills_ml.ontologies import Competency dinosaur_riding = Competency( identifier='dino_riding', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' )","title":"Competency"},{"location":"skills_ml_tour/#competency-relationships","text":"Competencies are often related to each other. Defining parent-child relationships is a standard building block of existing competency frameworks like ONET and ESCO. A parent-child relationship generally implies that the child is a \"type of\" the parent. from skills_ml.ontologies import Competency from skills_ml.ontologies.viz import display_nodes dinosaur_riding = Competency( identifier='12345', name='Dinosaur Riding', description='Using the back of a dinosaur for transportation' ) train_surfing = Competency( identifier='12346', name='Train Surfing', description='Standing on the train while it goes' ) time_travel = Competency( identifier='23456', name='Time Travel', description='Traveling Through Time' ) advanced_science = Competency( identifier='2345', name='Advanced Science', ) extreme_transportation = Competency( identifier='123', name='Extreme Transportation', description='Comically dangerous forms of transportation' ) time_travel.add_parent(advanced_science) dinosaur_riding.add_parent(extreme_transportation) train_surfing.add_parent(extreme_transportation) display_nodes([dinosaur_riding, train_surfing, extreme_transportation, time_travel, advanced_science])","title":"Competency Relationships"},{"location":"skills_ml_tour/#occupation","text":"An occupation is a job or profession that a person can hold. Similar to competencies, these are also often defined hierarchically. from skills_ml.ontologies import Occupation extreme_postal_workers = Occupation(identifier='999', name='Extreme Postal Workers') dino_postal_worker = Occupation(identifier='9998', name='Deliverer of Items to the Past') train_yard_postal_worker = Occupation(identifier='9999', name='Deliverer of Items to Train Yards') dino_postal_worker.add_parent(extreme_postal_workers) train_yard_postal_worker.add_parent(extreme_postal_workers) display_nodes([extreme_postal_workers, dino_postal_worker, train_yard_postal_worker])","title":"Occupation"},{"location":"skills_ml_tour/#competencyontology","text":"A CompetencyOntology is a model of the labor market, or some subset thereof, consisting of a collection of competencies, a collection of occupations, and all of the relationships between them. from skills_ml.ontologies import CompetencyOntology from skills_ml.ontologies.viz import display_ontology ontology = CompetencyOntology() ontology.add_competency(dinosaur_riding) ontology.add_competency(train_surfing) ontology.add_competency(extreme_transportation) ontology.add_competency(time_travel) ontology.add_competency(advanced_science) ontology.add_occupation(dino_postal_worker) ontology.add_occupation(train_yard_postal_worker) ontology.add_occupation(extreme_postal_workers) ontology.add_edge(occupation=dino_postal_worker, competency=dinosaur_riding) ontology.add_edge(occupation=dino_postal_worker, competency=time_travel) ontology.add_edge(occupation=train_yard_postal_worker, competency=train_surfing) display_ontology(ontology)","title":"CompetencyOntology"},{"location":"skills_ml_tour/#prebuilt-ontologies","text":"To move on we'll want to level up to a full ontology. The example we'll use is O*NET, built from survey data and maintained by the US Department of Labor. A CompetencyOntology subclass that downloads the source files from the O*NET web site is included in Skills-ML. from skills_ml.ontologies.onet import Onet onet = Onet() onet.print_summary_stats() Ontology summary statistics for onet Num competencies: 32030 Num occupations: 1133 Num competency-occupation edges: 107305 Median occupations per competency: 1 Median competencies per occupation: 89 Mean occupations per competency: 3.350245090386837 Mean competencies per occupation: 94.70873786407768 list(onet.competencies)[0:5] [Competency(identifier=43232108-Rezgo, name=Rezgo, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232605-Hansen CAMA, name=Hansen CAMA, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=27111526-Arrow squaring devices, name=Arrow squaring devices, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=48101817-Marzipan tools, name=Marzipan tools, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=21111507-Commercial fishing line sinkers, name=Commercial fishing line sinkers, categories=['Tools', 'O*NET T2'], {})]","title":"Prebuilt Ontologies"},{"location":"skills_ml_tour/#filtering","text":"~34000 competencies and ~1100 occupations is a lot. Let's explore the filtering functionality of the CompetencyOntology to zoom in on a more specific slice. filter_by filters using edges: the filtering function it expects takes in an edge (between a Competency and Occupation) and returns whether or not it should be in the result. The result takes the form of a new CompetencyOntology, so you can interact with it in the same way as you would the source ontology. nurse_practitioners = onet.filter_by(lambda edge: 'Nurse Practitioners' in edge.occupation.name) nurse_practitioners.competencies {Competency(identifier=1.A.1.a.1, name=Oral Comprehension, categories=['Abilities'], {'competencyText': 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'}), Competency(identifier=1.A.1.a.2, name=Written Comprehension, categories=['Abilities'], {'competencyText': 'The ability to read and understand information and ideas presented in writing.'}), Competency(identifier=1.A.1.a.3, name=Oral Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in speaking so others will understand.'}), Competency(identifier=1.A.1.a.4, name=Written Expression, categories=['Abilities'], {'competencyText': 'The ability to communicate information and ideas in writing so others will understand.'}), Competency(identifier=1.A.1.b.1, name=Fluency of Ideas, categories=['Abilities'], {'competencyText': 'The ability to come up with a number of ideas about a topic (the number of ideas is important, not their quality, correctness, or creativity).'}), Competency(identifier=1.A.1.b.2, name=Originality, categories=['Abilities'], {'competencyText': 'The ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.'}), Competency(identifier=1.A.1.b.3, name=Problem Sensitivity, categories=['Abilities'], {'competencyText': 'The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.'}), Competency(identifier=1.A.1.b.4, name=Deductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to apply general rules to specific problems to produce answers that make sense.'}), Competency(identifier=1.A.1.b.5, name=Inductive Reasoning, categories=['Abilities'], {'competencyText': 'The ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events).'}), Competency(identifier=1.A.1.b.6, name=Information Ordering, categories=['Abilities'], {'competencyText': 'The ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations).'}), Competency(identifier=1.A.1.b.7, name=Category Flexibility, categories=['Abilities'], {'competencyText': 'The ability to generate or use different sets of rules for combining or grouping things in different ways.'}), Competency(identifier=1.A.1.c.1, name=Mathematical Reasoning, categories=['Abilities'], {'competencyText': 'The ability to choose the right mathematical methods or formulas to solve a problem.'}), Competency(identifier=1.A.1.d.1, name=Memorization, categories=['Abilities'], {'competencyText': 'The ability to remember information such as words, numbers, pictures, and procedures.'}), Competency(identifier=1.A.1.e.1, name=Speed of Closure, categories=['Abilities'], {'competencyText': 'The ability to quickly make sense of, combine, and organize information into meaningful patterns.'}), Competency(identifier=1.A.1.e.2, name=Flexibility of Closure, categories=['Abilities'], {'competencyText': 'The ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material.'}), Competency(identifier=1.A.1.e.3, name=Perceptual Speed, categories=['Abilities'], {'competencyText': 'The ability to quickly and accurately compare similarities and differences among sets of letters, numbers, objects, pictures, or patterns. The things to be compared may be presented at the same time or one after the other. This ability also includes comparing a presented object with a remembered object.'}), Competency(identifier=1.A.2.a.1, name=Arm-Hand Steadiness, categories=['Abilities'], {'competencyText': 'The ability to keep your hand and arm steady while moving your arm or while holding your arm and hand in one position.'}), Competency(identifier=1.A.2.a.3, name=Finger Dexterity, categories=['Abilities'], {'competencyText': 'The ability to make precisely coordinated movements of the fingers of one or both hands to grasp, manipulate, or assemble very small objects.'}), Competency(identifier=1.A.4.a.1, name=Near Vision, categories=['Abilities'], {'competencyText': 'The ability to see details at close range (within a few feet of the observer).'}), Competency(identifier=1.A.4.b.4, name=Speech Recognition, categories=['Abilities'], {'competencyText': 'The ability to identify and understand the speech of another person.'}), Competency(identifier=1.A.4.b.5, name=Speech Clarity, categories=['Abilities'], {'competencyText': 'The ability to speak clearly so others can understand you.'}), Competency(identifier=2.A.1.a, name=Reading Comprehension, categories=['Skills'], {'competencyText': 'Understanding written sentences and paragraphs in work related documents.'}), Competency(identifier=2.A.1.b, name=Active Listening, categories=['Skills'], {'competencyText': 'Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.'}), Competency(identifier=2.A.1.c, name=Writing, categories=['Skills'], {'competencyText': 'Communicating effectively in writing as appropriate for the needs of the audience.'}), Competency(identifier=2.A.1.d, name=Speaking, categories=['Skills'], {'competencyText': 'Talking to others to convey information effectively.'}), Competency(identifier=2.A.1.f, name=Science, categories=['Skills'], {'competencyText': 'Using scientific rules and methods to solve problems.'}), Competency(identifier=2.A.2.a, name=Critical Thinking, categories=['Skills'], {'competencyText': 'Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.'}), Competency(identifier=2.A.2.b, name=Active Learning, categories=['Skills'], {'competencyText': 'Understanding the implications of new information for both current and future problem-solving and decision-making.'}), Competency(identifier=2.A.2.c, name=Learning Strategies, categories=['Skills'], {'competencyText': 'Selecting and using training/instructional methods and procedures appropriate for the situation when learning or teaching new things.'}), Competency(identifier=2.A.2.d, name=Monitoring, categories=['Skills'], {'competencyText': 'Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.'}), Competency(identifier=2.B.1.a, name=Social Perceptiveness, categories=['Skills'], {'competencyText': \"Being aware of others' reactions and understanding why they react as they do.\"}), Competency(identifier=2.B.1.b, name=Coordination, categories=['Skills'], {'competencyText': \"Adjusting actions in relation to others' actions.\"}), Competency(identifier=2.B.1.c, name=Persuasion, categories=['Skills'], {'competencyText': 'Persuading others to change their minds or behavior.'}), Competency(identifier=2.B.1.d, name=Negotiation, categories=['Skills'], {'competencyText': 'Bringing others together and trying to reconcile differences.'}), Competency(identifier=2.B.1.e, name=Instructing, categories=['Skills'], {'competencyText': 'Teaching others how to do something.'}), Competency(identifier=2.B.1.f, name=Service Orientation, categories=['Skills'], {'competencyText': 'Actively looking for ways to help people.'}), Competency(identifier=2.B.2.i, name=Complex Problem Solving, categories=['Skills'], {'competencyText': 'Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.'}), Competency(identifier=2.B.3.a, name=Operations Analysis, categories=['Skills'], {'competencyText': 'Analyzing needs and product requirements to create a design.'}), Competency(identifier=2.B.4.e, name=Judgment and Decision Making, categories=['Skills'], {'competencyText': 'Considering the relative costs and benefits of potential actions to choose the most appropriate one.'}), Competency(identifier=2.B.4.g, name=Systems Analysis, categories=['Skills'], {'competencyText': 'Determining how a system should work and how changes in conditions, operations, and the environment will affect outcomes.'}), Competency(identifier=2.B.4.h, name=Systems Evaluation, categories=['Skills'], {'competencyText': 'Identifying measures or indicators of system performance and the actions needed to improve or correct performance, relative to the goals of the system.'}), Competency(identifier=2.B.5.a, name=Time Management, categories=['Skills'], {'competencyText': \"Managing one's own time and the time of others.\"}), Competency(identifier=2.B.5.d, name=Management of Personnel Resources, categories=['Skills'], {'competencyText': 'Motivating, developing, and directing people as they work, identifying the best people for the job.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=41103901-Microhematocrit centrifuges, name=Microhematocrit centrifuges, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104102-Lancets, name=Lancets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104104-Tourniquets, name=Tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104107-Evacuated blood collection tubes, name=Evacuated blood collection tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104118-Specimen collection containers, name=Specimen collection containers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41104403-Tissue culture incubators, name=Tissue culture incubators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41111709-Binocular light compound microscopes, name=Binocular light compound microscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41115807-Hemoglobin analyzers, name=Hemoglobin analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116138-Urinalysis test strips, name=Urinalysis test strips, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=41116201-Glucometers, name=Glucometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131606-Protective face shields, name=Protective face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42131612-Protective gowns, name=Protective gowns, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42132203-Medical examination protective gloves, name=Medical examination protective gloves, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42141807-Transcutaneous electric nerve stimulation TENS equipment, name=Transcutaneous electric nerve stimulation TENS equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Nasal suctioning equipment, name=Nasal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Oral suctioning equipment, name=Oral suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142404-Tracheal suctioning equipment, name=Tracheal suctioning equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142509-Epidural catheters, name=Epidural catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142532-Pericardiocentesis kits, name=Pericardiocentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142537-Thoracentesis kits, name=Thoracentesis kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142609-Hypodermic syringes, name=Hypodermic syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142616-Blood drawing syringes, name=Blood drawing syringes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42142715-Urinary catheters, name=Urinary catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42144102-Chest tubes, name=Chest tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171608-Head immobilization devices, name=Head immobilization devices, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42171613-Spinal immobilization equipment, name=Spinal immobilization equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172101-Automated external defibrillators AED, name=Automated external defibrillators AED, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42172102-Cardiopulmonary resuscitation CPR face shields, name=Cardiopulmonary resuscitation CPR face shields, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Electronic blood pressure monitors, name=Electronic blood pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181602-Pediatric blood pressure cuffs, name=Pediatric blood pressure cuffs, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Electrocardiography EKG machines, name=Electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181701-Portable electrocardiography EKG machines, name=Portable electrocardiography EKG machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181713-Holter monitors, name=Holter monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181801-Pulse oximeters, name=Pulse oximeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181902-Intracranial pressure monitors, name=Intracranial pressure monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Cardiac monitors, name=Cardiac monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42181903-Hemodynamic monitors, name=Hemodynamic monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Ophthalmoscopes, name=Ophthalmoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182005-Otoscopes, name=Otoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182013-Vaginal exam specula, name=Vaginal exam specula, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182103-Mechanical stethoscopes, name=Mechanical stethoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182201-Digital medical thermometers, name=Digital medical thermometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182302-Reflex hammers, name=Reflex hammers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182412-Diagnostic tuning forks, name=Diagnostic tuning forks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182416-Tympanometers, name=Tympanometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42182805-Medical scales, name=Medical scales, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Snellen eye charts, name=Snellen eye charts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42183001-Visual acuity testing cards, name=Visual acuity testing cards, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42192401-Crash carts, name=Crash carts, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42201701-Doppler ultrasound equipment, name=Doppler ultrasound equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Angiocaths, name=Angiocaths, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203402-Pulmonary artery catheters, name=Pulmonary artery catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Pacemaker analyzers, name=Pacemaker analyzers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42203501-Transcutaneous pacemakers, name=Transcutaneous pacemakers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42211502-Crutches, name=Crutches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221501-Arterial line catheters, name=Arterial line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221503-Multiple lumen central line catheters, name=Multiple lumen central line catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221506-Umbilical catheters, name=Umbilical catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV administration sets, name=Intravenous IV administration sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42221509-Intravenous IV cutdown trays, name=Intravenous IV cutdown trays, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42222001-Intravenous IV infusion pumps, name=Intravenous IV infusion pumps, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42231701-Nasogastric tubes, name=Nasogastric tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241507-Orthopedic splinting equipment, name=Orthopedic splinting equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241705-Lower extremity braces, name=Lower extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241707-Walking braces, name=Walking braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241801-Upper extremity braces, name=Upper extremity braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241802-Back braces, name=Back braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42241803-Neck braces, name=Neck braces, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42242103-Halo traction equipment, name=Halo traction equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271501-Apnea monitors, name=Apnea monitors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271502-Arterial blood gas monitoring equipment, name=Arterial blood gas monitoring equipment, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271602-Incentive spirometers, name=Incentive spirometers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271604-Peak flowmeters, name=Peak flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271702-Oxygen concentrators, name=Oxygen concentrators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271707-Oxygen flowmeters, name=Oxygen flowmeters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271708-Oxygen delivery masks, name=Oxygen delivery masks, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271710-Nasal catheters, name=Nasal catheters, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271802-Handheld nebulizers, name=Handheld nebulizers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271903-Endotracheal ET tubes, name=Endotracheal ET tubes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42271910-Tracheotomy sets, name=Tracheotomy sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272001-Fiberoptic laryngoscopes, name=Fiberoptic laryngoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272017-Intubation sets, name=Intubation sets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272201-Mechanical intermittent positive pressure ventilators, name=Mechanical intermittent positive pressure ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42272203-Bilevel positive airway pressure BiPAP ventilators, name=Bilevel positive airway pressure BiPAP ventilators, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291613-Surgical scalpels, name=Surgical scalpels, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291614-Straight surgical scissors, name=Straight surgical scissors, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291704-Biopsy punches, name=Biopsy punches, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42291802-Mosquito hemostats, name=Mosquito hemostats, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294702-Intra-aortic balloon pumps IABP, name=Intra-aortic balloon pumps IABP, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294717-Ventricular assist devices VAD, name=Ventricular assist devices VAD, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Fiberoptic endoscopes, name=Fiberoptic endoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42294802-Flexible sigmoidoscopes, name=Flexible sigmoidoscopes, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295104-Electrosurgical cauterization machines, name=Electrosurgical cauterization machines, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Argon lasers, name=Argon lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Carbon dioxide CO2 lasers, name=Carbon dioxide CO2 lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295119-Pulsed dye lasers, name=Pulsed dye lasers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42295122-Pneumatic tourniquets, name=Pneumatic tourniquets, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312008-Surgical staple removers, name=Surgical staple removers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312010-Skin staplers, name=Skin staplers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=42312202-Suturing kits, name=Suturing kits, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43191507-Multi-line telephone systems, name=Multi-line telephone systems, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211503-Laptop computers, name=Laptop computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211504-Personal digital assistants PDA, name=Personal digital assistants PDA, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211508-Personal computers, name=Personal computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43211509-Tablet computers, name=Tablet computers, categories=['Tools', 'O*NET T2'], {}), Competency(identifier=43231507-Microsoft SharePoint, name=Microsoft SharePoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43231513-Microsoft Office, name=Microsoft Office, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232104-Microsoft Word, name=Microsoft Word, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232106-Microsoft PowerPoint, name=Microsoft PowerPoint, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232110-Microsoft Excel, name=Microsoft Excel, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232306-Microsoft Access, name=Microsoft Access, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Allscripts Professional EHR, name=Allscripts Professional EHR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Amkai AmkaiCharts, name=Amkai AmkaiCharts, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Bizmatics PrognoCIS EMR, name=Bizmatics PrognoCIS EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Cerner Millennium, name=Cerner Millennium, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-GE Healthcare Centricity EMR, name=GE Healthcare Centricity EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Healthcare common procedure coding system HCPCS, name=Healthcare common procedure coding system HCPCS, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MEDITECH software, name=MEDITECH software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical condition coding software, name=Medical condition coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medical procedure coding software, name=Medical procedure coding software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Medscribbler Enterprise, name=Medscribbler Enterprise, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-MicroFour PracticeStudio.NET EMR, name=MicroFour PracticeStudio.NET EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-NextGen Healthcare Information Systems EMR, name=NextGen Healthcare Information Systems EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-PCC Pediatric Partner, name=PCC Pediatric Partner, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SOAPware EMR, name=SOAPware EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-StatCom Patient Flow Logistics Enterprise Suite, name=StatCom Patient Flow Logistics Enterprise Suite, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-SynaMed EMR, name=SynaMed EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-Texas Medical Software SpringCharts EMR, name=Texas Medical Software SpringCharts EMR, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-e-MDs software, name=e-MDs software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232610-eClinicalWorks, name=eClinicalWorks, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Microsoft Internet Explorer, name=Microsoft Internet Explorer, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43232705-Web browser software, name=Web browser software, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=43233501-Microsoft Outlook, name=Microsoft Outlook, categories=['Technology', 'O*NET T2'], {}), Competency(identifier=46181804-Safety goggles, name=Safety goggles, categories=['Tools', 'O*NET T2'], {})} That's a big list of competencies. Each competency has a list of categories, so let's get all of the different possible categories set(cat for competency in nurse_practitioners.competencies for cat in competency.categories) {'Abilities', 'Knowledge', 'O*NET T2', 'Skills', 'Technology', 'Tools'} So we can ask questions like: what type of Knowledge do nurse practitioners need? [competency for competency in nurse_practitioners.competencies if 'Knowledge' in competency.categories] [Competency(identifier=2.C.7.a, name=English Language, categories=['Knowledge'], {'competencyText': 'Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.'}), Competency(identifier=2.C.1.b, name=Clerical, categories=['Knowledge'], {'competencyText': 'Knowledge of administrative and clerical procedures and systems such as word processing, managing files and records, stenography and transcription, designing forms, and other office procedures and terminology.'}), Competency(identifier=2.C.4.a, name=Mathematics, categories=['Knowledge'], {'competencyText': 'Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.'}), Competency(identifier=2.C.8.b, name=Law and Government, categories=['Knowledge'], {'competencyText': 'Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.'}), Competency(identifier=2.C.6, name=Education and Training, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects.'}), Competency(identifier=2.C.4.f, name=Sociology and Anthropology, categories=['Knowledge'], {'competencyText': 'Knowledge of group behavior and dynamics, societal trends and influences, human migrations, ethnicity, cultures and their history and origins.'}), Competency(identifier=2.C.7.e, name=Philosophy and Theology, categories=['Knowledge'], {'competencyText': 'Knowledge of different philosophical systems and religions. This includes their basic principles, values, ethics, ways of thinking, customs, practices, and their impact on human culture.'}), Competency(identifier=2.C.4.d, name=Biology, categories=['Knowledge'], {'competencyText': 'Knowledge of plant and animal organisms, their tissues, cells, functions, interdependencies, and interactions with each other and the environment.'}), Competency(identifier=2.C.4.c, name=Chemistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the chemical composition, structure, and properties of substances and of the chemical processes and transformations that they undergo. This includes uses of chemicals and their interactions, danger signs, production techniques, and disposal methods.'}), Competency(identifier=2.C.8.a, name=Public Safety and Security, categories=['Knowledge'], {'competencyText': 'Knowledge of relevant equipment, policies, procedures, and strategies to promote effective local, state, or national security operations for the protection of people, data, property, and institutions.'}), Competency(identifier=2.C.5.a, name=Medicine and Dentistry, categories=['Knowledge'], {'competencyText': 'Knowledge of the information and techniques needed to diagnose and treat human injuries, diseases, and deformities. This includes symptoms, treatment alternatives, drug properties and interactions, and preventive health-care measures.'}), Competency(identifier=2.C.5.b, name=Therapy and Counseling, categories=['Knowledge'], {'competencyText': 'Knowledge of principles, methods, and procedures for diagnosis, treatment, and rehabilitation of physical and mental dysfunctions, and for career counseling and guidance.'}), Competency(identifier=2.C.4.e, name=Psychology, categories=['Knowledge'], {'competencyText': 'Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.'}), Competency(identifier=2.C.9.b, name=Communications and Media, categories=['Knowledge'], {'competencyText': 'Knowledge of media production, communication, and dissemination techniques and methods. This includes alternative ways to inform and entertain via written, oral, and visual media.'}), Competency(identifier=2.C.1.e, name=Customer and Personal Service, categories=['Knowledge'], {'competencyText': 'Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.'}), Competency(identifier=2.C.3.a, name=Computers and Electronics, categories=['Knowledge'], {'competencyText': 'Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.'})] There are many other questions we can ask of just an ontology, but the real value will come from combining the knowledge contained in the ontology with larger unstructured datasets. In this next section we will explore how Skills-ML helps the user deal with such datasets as job postings, profiles, or course descriptions.","title":"Filtering"},{"location":"skills_ml_tour/#spirit-of-skills-ml","text":"Dataflow Programming : Skills-ML 's design philosophy builds on dataflow programming or so called data streaming to process very large datasets (larger than RAM; potentially infinite). One-pass algorithm : Data points are processed one at a time. Lazy evaluation : an evaluation strategy which delays the evaluation of an expression until its value is needed. In Skills-ML , most of the classes and functions here incorporates the concept of Iterable or Generator . We build the expression first and evaluate later.","title":"Spirit of Skills-ML"},{"location":"skills_ml_tour/#creating-dataset","text":"Before we do anything with the context, we need dataset. Skills-ML makes use of schema.org\u2019s JobPosting standard. As it has been in use for a long time, some open sources are already using this standard, which is easy to import. Other job posting data sources are converted into the schema.org Schema and all work on job postings is done using this standard schema. In Skills-ML , job_postings module has all the functionalities to create the data we need for later usage.","title":"Creating Dataset"},{"location":"skills_ml_tour/#common-schema","text":"We have an useful function to help create the data generator from s3. JobPostingCollectionFromS3 : Stream job posting from s3. JobPostingCollectionSample : Stream a finite number of job postings stored within the library. However, we are not restrcted to just JobPosting data. One can easily create whatever data generator such as ProfileGenerator or CourseGenerator . For example, we want to use the Vrigina Dataset which is an open data set of job postings. We just have to create a job posting generator with some transformation. from skills_ml.job_postings.raw.virginia import VirginiaTransformer from urllib.request import urlopen import json va_url = \"http://opendata.cs.vt.edu/dataset/ab0abac3-2293-4c9d-8d80-22d450254389/resource/074f7e44-9275-4bba-874e-4795e8f6830c/download/openjobs-jobpostings.may-2016.json\" class VAJobposting(object): def __init__(self, uri): self.uri = uri def __iter__(self): request = urlopen(self.uri) for line in request.readlines(): raw = json.loads(line) yield VirginiaTransformer(partner_id=\"va\")._transform(raw) jobpostings_va = VAJobposting(va_url) print(len(list(jobpostings_va))) 40098","title":"Common Schema"},{"location":"skills_ml_tour/#filtering_1","text":"To create a good dataset, we might want to have some criteria for choosing the proper job posting based on the task we want to perform, like job postings that have the label information, job postings that belong to certain occupation, or job postings that have rich enough information in the description field. JobPostingFilterer : Filter common schema job postings through a number of filtering functions. This function also follows lazy evaluation strategy. from skills_ml.job_postings.filtering import JobPostingFilterer def is_tech_jobs(job): if job['onet_soc_code'][:2] in ['15', '17', '19']: return True else: return False tech_jobs = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[is_tech_jobs] ) from skills_ml.ontologies.onet import majorgroupname from collections import Counter import pandas as pd import matplotlib.pyplot as plt import matplotlib import seaborn as sns sns.set(style=\"darkgrid\", font_scale=2) %matplotlib inline # major group distribution plotting function def plot_major_group_distribution(job_postings): c = Counter() for job in job_postings: c.update([job['onet_soc_code'][:2]]) s = pd.Series(c).sort_index() s.index = s.index.map(majorgroupname) ax = s.plot.bar(figsize=(20,10),rot=90) ax.set_xlabel('soc_major_group') ax.set_ylabel('number of job posting') ax.set_title(f\"total number: {s.sum()}\") return s plot_major_group_distribution(tech_jobs) Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 dtype: int64 What if we want to make sure that all the job postings have ONet SOC Code and it's not unknown(first 2 digit 99)? We can define filter functions like these which can be either generic function or lambda function. def filter_onet_soc_code(job): if job['onet_soc_code'] and job['onet_soc_code'][:2] != '99': return True else: return False has_soc = lambda x: x['onet_soc_code'] not_unknown_soc = lambda x: x['onet_soc_code'][:2] != '99' jobpostings_filtered = JobPostingFilterer( job_posting_generator=VAJobposting(va_url), filter_funcs=[has_soc, not_unknown_soc] ) plot_major_group_distribution(jobpostings_filtered) Management 6506 Business and Financial Operations 3867 Computer and Mathematical 5065 Architecture and Engineering 1937 Life, Physical, and Social Science 192 Community and Social Service 282 Legal 94 Education, Training, and Library 679 Arts, Design, Entertainment, Sports, and Media 598 Healthcare Practitioners and Technical 3447 Healthcare Support 494 Protective Service 484 Food Preparation and Serving Related 792 Building and Grounds Cleaning and Maintenance 189 Personal Care and Service 97 Sales and Related 1415 Office and Administrative Support 2580 Construction and Extraction 196 Installation, Maintenance, and Repair 832 Production 442 Transportation and Material Moving 2722 Military Specific 2 dtype: int64","title":"Filtering"},{"location":"skills_ml_tour/#random-sampling","text":"Even though we have a lot of data, most of time we don't need all of them to do the analysis. Or we can't even fit all the data into memory to do the analysis. What we need more importantly is a suitable sampled dataset. JobSampler : Sample job posting by (weighted) reservoir sampling.","title":"Random Sampling"},{"location":"skills_ml_tour/#random-sampling-from-streaming-data-reservoir-sampling","text":"\"Say you have a stream of items of large and unknown length that we can only iterate over once.\" It's memeory efficient and just one iteration There is a great overview of reservoir sampling in https://gregable.com/2007/10/reservoir-sampling.html. Let's say the original job postings dataset are too much for my Mac Yosemite to do any analysis and I want only 1000 job postings but still preserve the statistical characteristics of the original dataset. from skills_ml.job_postings.sample import JobSampler sampler = JobSampler( job_posting_generator=jobpostings_filtered, k=1000, ) plot_major_group_distribution(sampler) Management 191 Business and Financial Operations 126 Computer and Mathematical 134 Architecture and Engineering 65 Life, Physical, and Social Science 5 Community and Social Service 11 Legal 1 Education, Training, and Library 25 Arts, Design, Entertainment, Sports, and Media 15 Healthcare Practitioners and Technical 106 Healthcare Support 20 Protective Service 16 Food Preparation and Serving Related 27 Building and Grounds Cleaning and Maintenance 5 Personal Care and Service 1 Sales and Related 51 Office and Administrative Support 75 Construction and Extraction 6 Installation, Maintenance, and Repair 19 Production 12 Transportation and Material Moving 89 dtype: int64 Something wrong happened! We are missing Military Occupations ! Because military job postings are extremely rare in the original dataset, simple ramdom sampling might result in lack of classes.","title":"Random Sampling from Streaming Data - Reservoir Sampling"},{"location":"skills_ml_tour/#weighted-reservoir-sampling","text":"How would you sample from a weighted distribution where each element has a given weight associated with it in the stream? For certain task, we need some curated sample. For example, if we want to build a occupation classifier, we want similar amounts of job posting for each occupation. Now we want to have a more uniform distributed sample across all major groups. Here we need to provide a weight dictionary in the JobSampler c = Counter() for job in jobpostings_filtered: c.update([job['onet_soc_code'][:2]]) weights = dict() for key, value in c.items(): weights[key] = max(c.values()) / value weights {'11': 1.0, '15': 1.2845014807502468, '17': 3.3588022715539494, '29': 1.8874383521903104, '41': 4.597879858657244, '43': 2.521705426356589, '13': 1.6824411688647531, '49': 7.819711538461538, '33': 13.442148760330578, '27': 10.879598662207357, '47': 33.19387755102041, '51': 14.71945701357466, '35': 8.214646464646465, '25': 9.581737849779087, '31': 13.17004048582996, '19': 33.885416666666664, '21': 23.070921985815602, '37': 34.423280423280424, '53': 2.3901542983100663, '39': 67.0721649484536, '23': 69.2127659574468, '55': 3253.0} sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=1000, key=lambda x: x['onet_soc_code'][:2], weights=weights) plot_major_group_distribution(sampler) Management 54 Business and Financial Operations 47 Computer and Mathematical 58 Architecture and Engineering 47 Life, Physical, and Social Science 39 Community and Social Service 53 Legal 43 Education, Training, and Library 47 Arts, Design, Entertainment, Sports, and Media 54 Healthcare Practitioners and Technical 43 Healthcare Support 46 Protective Service 48 Food Preparation and Serving Related 41 Building and Grounds Cleaning and Maintenance 49 Personal Care and Service 36 Sales and Related 48 Office and Administrative Support 48 Construction and Extraction 62 Installation, Maintenance, and Repair 44 Production 44 Transportation and Material Moving 47 Military Specific 2 dtype: int64","title":"Weighted Reservoir Sampling"},{"location":"skills_ml_tour/#skill-extraction","text":"A common task is extracting competencies from unstructured text. Sometimes this is ontology-based (finding concepts from a known ontology in text), but this is not necessarily true. Skills-ML unites these with a common interface in the SkillExtractor class. The common interface is that every SkillExtractor needs to be able to take in a collection of documents, and yield what we call CandidateSkill objects.","title":"Skill Extraction"},{"location":"skills_ml_tour/#what-is-a-candidateskill","text":"A CandidateSkill is a possible occurrence of a skill/competency in context in some document. It consists of the following fields: skill_name - The text version of the skill as it appears in the document matched_skill_identifier - A reference to the skill in some ontology. This may be empty, if no ontology was used to search for skills. context - The text surrounding the skill in the document. The goal is for a human labeler to be able to use this to determine whether or not the occurrence represents a true skill. How much context is included is up to the algorithm. start_index - The start index of the skill occurrence within the document string. confidence - The confidence level the algorithm has in this candidate skill being a true occurrence of a skill. This may be empty, if the algorithm has now way of producing a confidence value. document_id - A unique identifier for the source document. document_type - The type of document (examples: Job Posting, Profile, Course Description) source_object - The entire source document. skill_extractor_name - The name of the skill extractor algorithm. Every SkillExtractor subclass defines a name property that is used to give processes downstream context about how their output data was produced. The idea behind the CandidateSkill object is to serve as a common interface between SkillExtractor objects, automatic evaluation methods, and manual evaluation methods. A labeling interface might intake CandidateSkill objects for humans to say yes/no to. Another type of labeling interface might involve the export of CandidateSkill objects based on what a human highlighted in the interface when shown the entire document Unsupervised evaluation metrics may take in one set of CandidateSkills to produce simple descriptive metrics Supervised evaluation metrics may take in one set of CandidateSkills from a SkillExtractor and another set of CandidateSkills from a human labeling interface and use the latter to evaluate the former We'll talk about some of these use cases in more detail later. But for now, let's start with a simple example that uses NLP rules and isn't tied to an ontology. Let's define a method for extracting skills as 'all noun phrases that end in the word skill or skills'. This is a simple method that realistically won't cover all possible occurrences of skills, but this is a start. from skills_ml.algorithms.skill_extractors import SkillEndingPatternExtractor from skills_ml.job_postings.common_schema import JobPostingCollectionSample job_posting_generator = JobPostingCollectionSample() # instantiate the skill extractor. This class defaults to only considering lines that # start with a bullet, which doesn't work for this dataset. So we set this flag to False. skill_extractor = SkillEndingPatternExtractor(only_bulleted_lines=False) job_posting = next(iter(job_posting_generator)) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: communication skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: organizational skills context: Excellent client presentation and communication skills as well as strong customer service and organizational skills. skill name: communication skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required skill name: organizational skills context: We are proud to be an equal opportunity employer College degree preferred, 2 - 5 years experience in print and / or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self - motivated, team - oriented, and customer - centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients / prospects is required, Minimal non local travel a few times a year is required The results for one job posting are modest. Two distinct skill names, each occurring two different times in the document. This is a start. Now let's try another skill extractor: matching with ONET data. from skills_ml.algorithms.skill_extractors import ExactMatchSkillExtractor skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') skill name: self context: The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric. skill name: self context: We are proud to be an equal opportunity employer College degree preferred, 2-5 years experience in print and/or online advertising sales and be able to show consistent sales results in previous positions, Knowledge of the IT industry is preferred, Track record of creativity in sales approaches and solutions, Track record of successfully meeting and exceeding sales goals in media sales relevant to 1105 Medias line of business, Excellent client presentation and communication skills as well as strong customer service and organizational skills, The ideal candidate is energetic, self-motivated, team-oriented, and customer-centric, Understanding of how to research potential customers and use online analytics from a sales perspective, Weekly local travel to meet with clients/prospects is required, Minimal non local travel a few times a year is required Yikes. What is this? As it turns out, 'Self' is a real programming language . But it's not applicable here. Simply searching for skill names has its limitations. To help with this, there is also the SocScopedExactMatchSkillExtractor. This does exact matching, but only for the occupation that the document is tagged with. This, of course, is only applicable if the document has one. And it needs a full CompetencyOntology to work. from skills_ml.algorithms.skill_extractors import SocScopedExactMatchSkillExtractor skill_extractor = SocScopedExactMatchSkillExtractor(onet) for candidate_skill in skill_extractor.candidate_skills(job_posting): print('skill name:', candidate_skill.skill_name) print('context:', candidate_skill.context) print('') No results. This is expected: For an occupation that is not related to computer programming, the language 'Self' is likely irrelevant. Here's a list of all the other skill extractors available. FuzzyMatchSkillExtractor - Similar to the ExactMatchSkillExtractor, but using a configurable edit distance to find skill names that are very close to the targets. AbilityEndingPatternExtractor - Similar to the SkillEndingPatternExtractor, but finding noun phrases that end in 'ability' or 'abilities'. SectionExtractSkillExtractor - Attempts to divide the text into sections with headers, which is a common pattern found in job postings. Return each individual sentence found in sections with certain headers (Skills, Competencies, Qualifications).","title":"What Is a CandidateSkill?"},{"location":"skills_ml_tour/#evaluating-skill-extractors","text":"We want to be able to evaluate skill extractors. We may or may not have labeled skills but do want to be able to generate descriptive metrics either way. from skills_ml.evaluation.skill_extraction_metrics import TotalOccurrences, TotalVocabularySize, OntologyCompetencyRecall metrics = [ TotalOccurrences(), TotalVocabularySize(), OntologyCompetencyRecall(onet) ] exact_match_skill_extractor = ExactMatchSkillExtractor(onet.competency_framework) for metric in metrics: candidate_skills = [] for job_posting in job_posting_generator: candidate_skills += list(exact_match_skill_extractor.candidate_skills(job_posting)) print('metric:', metric.name, 'value:', metric.eval(candidate_skills, 50)) metric: total_candidate_skills value: 153 metric: total_vocabulary_size value: 40 metric: onet_ksat_competency_recall value: 0.001248868213181804","title":"Evaluating Skill Extractors"},{"location":"skills_ml_tour/#embedding","text":"Labor market data tends to be large in scale, but represented as raw text. Consequently, an important early step for most tasks is to transform texts into a mathematical form that can be used in the downstream tasks. In the context of skills and jobs, an embedding model trained on large amount of job posting data is able to map a skill or a job title into a high dimensional space as well as preserving the contextual and semantic relationship. Ideally, a good embedding model will cluster similar skills and jobs.","title":"Embedding"},{"location":"skills_ml_tour/#embedding-models","text":"Many word embedding techniques have been developed since the most impactful embedding algorithm word2vec was published in 2013. Currently, Skills-ML includes word2vec, doc2vec and fastext and may include more in the future. Word2VecModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, a random vector will be assigned with the same dimension. Doc2VecModel is able to look up a word vector and infer a sentence/paragraph vector by gradient descending on the fly, so it is non-deterministic. It does not support online learning. FastTextModel is able to look up a word vector and infer a sentence/paragraph vector by averaging each word in a sentence/paragraph. It supports online learning. For out-of-vocabulary word handling of sentence/paragraph inference, it sums all vectors of the unseen word\u2019s char-ngrams. If none of the char-ngrams of the unseen word is present, a random vector will be assigned with the same dimension. from skills_ml.algorithms.embedding.models import Word2VecModel, FastTextModel cbow = Word2VecModel(size=200, sg=0, window=7, iter=3, batch_words=1000) skip_gram = Word2VecModel(size=200, sg=1, window=7, iter=3, batch_words=1000) fasttext = FastTextModel(size=200, window=7, iter=3, batch_words=1000)","title":"Embedding Models"},{"location":"skills_ml_tour/#corpora","text":"Next, we need some text corpus to train embedding modelss. Skills-ML provides pre-defined classes to convert common schema job listings into a corpus in documnet level suitable for use by machine learning algorithms or specific tasks. Word2VecGensimCorpusCreator Doc2VecGensimCorpusCreator from skills_ml.job_postings.corpora import Word2VecGensimCorpusCreator, Doc2VecGensimCorpusCreator sampler = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) w2v_corpus_generator = Word2VecGensimCorpusCreator(sampler)","title":"Corpora"},{"location":"skills_ml_tour/#preprocessing","text":"Or we can build our own corpus generator by using some preprocessing tools Function Compostition - ProcessingPipeline will compose processing functions together to become a callable object that takes in the input from the very first processing function and returns the output of the last processing function. - IterablePipeline will compose processing functions together to be passed to different stages(training/ prediction) from skills_ml.algorithms.preprocessing import IterablePipeline from skills_ml.algorithms import nlp from functools import partial document_schema_fields = ['description','experienceRequirements', 'qualifications', 'skills'] pipeline = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_html, nlp.clean_str, nlp.word_tokenize, ) corpus_generator = pipeline(sampler)","title":"Preprocessing"},{"location":"skills_ml_tour/#train-embedding","text":"The EmbeddingTrainer provides online batch learning for Word2VecModel and FastTextModel. from skills_ml.algorithms.embedding.train import EmbeddingTrainer trainer = EmbeddingTrainer(cbow, skip_gram, fasttext, batch_size=100) trainer.train(corpus_generator)","title":"Train Embedding"},{"location":"skills_ml_tour/#storage","text":"Skills-ML has couple useful storage classes that could benefit both local or cloud. - S3Store : S3 storage engine - FSStore : File system storage engine - ModelStorage : Serialization model storage. from skills_ml.storage import FSStore, S3Store, ModelStorage fs = FSStore(path=\"tmp/model_cache/embedding/examples\") trainer.save_model(storage=fs) print(cbow.model_name) print(cbow.storage) word2vec_7bdfa911ccc14b971f92b35d529c1dc6.model FSStore(path=tmp/model_cache/embedding/examples)","title":"Storage"},{"location":"skills_ml_tour/#examples","text":"for c, s in zip(cbow.wv.most_similar(['engineer']), skip_gram.wv.most_similar(['engineer'])): print(c, s) ('developer', 0.7927991151809692) ('developer', 0.6523622870445251) ('analyst', 0.7418307662010193) ('tester', 0.624006986618042) ('sr', 0.6851106882095337) ('rhev', 0.6191227436065674) ('designer', 0.6370913982391357) ('devops', 0.6076809167861938) ('devops', 0.6367722749710083) ('modis', 0.5969630479812622) ('intern', 0.6265694499015808) ('designer', 0.5858316421508789) ('inc', 0.6245660781860352) ('writer', 0.568120002746582) ('architect', 0.619118332862854) ('chantilly', 0.5666274428367615) ('scientist', 0.6182761192321777) ('island', 0.5646683573722839) ('tester', 0.5959925651550293) ('herndon', 0.5613434910774231) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`. if np.issubdtype(vec.dtype, np.int): for c, s in zip(cbow.wv.most_similar(['python']), skip_gram.wv.most_similar(['python'])): print(c, s) ('perl', 0.9427046775817871) ('perl', 0.9121636152267456) ('ruby', 0.908499002456665) ('ruby', 0.8980081081390381) ('scripting', 0.9076182842254639) ('bash', 0.8887742757797241) ('languages', 0.9037582874298096) ('mysql', 0.8292362689971924) ('programming', 0.8735607862472534) ('centos', 0.821326494216919) ('linux', 0.8646563291549683) ('jsp', 0.8192467093467712) ('java', 0.8616237640380859) ('ee', 0.8136248588562012) ('javascript', 0.8528693914413452) ('languages', 0.8108319044113159) ('css', 0.8422471284866333) ('xml', 0.8019323348999023) ('xml', 0.8400859832763672) ('scripting', 0.8000487089157104) for c, s in zip(cbow.wv.most_similar(['cnc']), skip_gram.wv.most_similar(['cnc'])): print(c, s) ('confined', 0.7270200252532959) ('machining', 0.7596360445022583) ('machining', 0.7186369895935059) ('press', 0.7382968664169312) ('grinders', 0.6886007785797119) ('igniters', 0.7340461015701294) ('graphs', 0.6842017769813538) ('turbines', 0.7310390472412109) ('feed', 0.6837735772132874) ('harness', 0.7257314920425415) ('lathes', 0.6761114597320557) ('lathes', 0.7163777351379395) ('controllers', 0.6728384494781494) ('ntcss', 0.7149447202682495) ('wood', 0.6689127683639526) ('comms', 0.7094639539718628) ('toxic', 0.6676516532897949) ('xendesktop', 0.7087377905845642) ('wire', 0.6662098169326782) ('metal', 0.707713782787323) Skills-ML also provides a function to visualize the embedding in tensorboard from skills_ml.algorithms.embedding.models import visualize_in_tensorboard visualize_in_tensorboard(cbow) Run `tensorboard --logdir=/home/ubuntu/skills-ml/word2vec_7bdfa911ccc14b971f92b35d529c1dc6 --host 127.0.0.1` to run visualize result on tensorboard","title":"Examples"},{"location":"skills_ml_tour/#evaluation","text":"Although there is an emerging trend towards generating embeddings for structured and unstructured data, there is not yet any systematic suite for measuring the quality of embeddings. We generally follow one of the few works in embedding evaluation [Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts] to create metrics for evaluating embedding against the gold standard ontology dataset. The gold standard ontology is curated by domain experts like O*NET, so a good embedding should replicate the structure of the entities in the gold standard taxonomy. In other words, it is useful to see how an embedding reflects the clustering structure. One trivial clustering is Major Groups of occupations. A good embedding should cluster the occupations which belong to the same major groups. CategorizationMetric : The cosine similarity between the embedding of the concept and the mean vector of embeddings of all the entities within that concept cluster. This metric aligns a clustering of entities into different categories, reflecting how well the embedding of a concept cluster performs as the background concept of the entities typed by it. IntraClusterCohesion : The sum of squared error of the embedding of the centroid of the concept cluster and the embedding of each entities within that cluster. It measures how near the data points in a cluster are to the cluster centroid. MajorGroupRecall : For a major group, calculate the cosine similarity against all the occupations and find the top n closest occupations. The recall is defined as the number of true positives from top n closest occupations divided by the total number of occupation within the major group. MajorGroupPrecision : Similarly to MajorGroupRecall which is called Coherence Score in the paper, start by finding the top n closest occupations. The precision is defined as the number of true positives from top n closest occupations divided by n from skills_ml.ontologies.onet import Onet major_group_occupation_des_clustering = onet.major_group_occupation_description_clustering from skills_ml.evaluation.embedding_metrics import metrics_for_embedding, CategorizationMetric, IntraClusterCohesion, RecallTopN, PrecisionTopN from skills_ml.algorithms.preprocessing import ProcessingPipeline def vectorization(embedding): p = ProcessingPipeline( nlp.normalize, nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=embedding) ) return p categorization_metric = CategorizationMetric(major_group_occupation_des_clustering) intra_cohesion = IntraClusterCohesion(major_group_occupation_des_clustering) recall_top = RecallTopN(major_group_occupation_des_clustering, topn=10) precision_top = PrecisionTopN(major_group_occupation_des_clustering, topn=10) categorization_metric.eval(vectorization(fasttext)) {'Building and Grounds Cleaning and Maintenance': 0.34224581131861653, 'Food Preparation and Serving Related': 0.2654245949239141, 'Construction and Extraction': 0.47868056098318057, 'Healthcare Practitioners and Technical': 0.3940974596927912, 'Transportation and Material Moving': 0.3232632841344041, 'Computer and Mathematical': 0.2728775771822607, 'Life, Physical, and Social Science': 0.5065041654514952, 'Farming, Fishing, and Forestry': 0.46221691564956713, 'Installation, Maintenance, and Repair': 0.18825461785204867, 'Business and Financial Operations': 0.4141572197688639, 'Education, Training, and Library': 0.4499782804964112, 'Military Specific': 0.8406518707733225, 'Production': 0.767452990807525, 'Office and Administrative Support': 0.6310052316123922, 'Healthcare Support': 0.9414934205345233, 'Sales and Related': 0.318627279388384, 'Protective Service': 0.7164475858481743, 'Management': 0.6463669464049742, 'Personal Care and Service': 0.5464152492907202, 'Architecture and Engineering': 0.34074216780013, 'Community and Social Service': 0.39121662365108567, 'Arts, Design, Entertainment, Sports, and Media': 0.4462032355049601, 'Legal': 0.5343651523623081} import statistics import operator from collections import defaultdict # We define some helper functions to evaluate multiple embeddings def algorithm_name(emb): if emb.model_type == 'word2vec' or emb.model_type == 'fasttext': if getattr(emb, 'sg', None) == 1: return 'Skip-Gram' else: return 'Continuous Bag of Words' elif emb.model_type == 'doc2vec': if getattr(emb, 'dm', None) == 1: return 'Distributed Memory' else: return 'Distributed Bag of Words' def evaluate_multiple_embeddings(embeddings, vectorization, metric): result = defaultdict(dict) for emb in embeddings: c = metric.eval(vectorization(emb)) name = emb.model_name.split('.')[0] result[name]['mean'] = statistics.mean(list(c.values())) result[name]['variance'] = statistics.variance(list(c.values())) result[name]['std'] = statistics.stdev(list(c.values())) result[name]['max'] = max(c.items(), key=operator.itemgetter(1))[1] result[name]['max_cluster'] = max(c.items(), key=operator.itemgetter(1))[0] result[name]['min'] = min(c.items(), key=operator.itemgetter(1))[1] result[name]['min_cluster'] = min(c.items(), key=operator.itemgetter(1))[0] result[name]['type'] = emb.model_type result[name]['algorithm'] = algorithm_name(emb) result[name]['window'] = emb.window return pd.DataFrame(result) evaluate_multiple_embeddings([cbow, skip_gram, fasttext], vectorization, categorization_metric) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } word2vec_7bdfa911ccc14b971f92b35d529c1dc6 word2vec_fee333d52ec0101d0e95848a8f376d37 fasttext_6885e864e01553bde10e120d40491de1 algorithm Continuous Bag of Words Skip-Gram Continuous Bag of Words max 0.951364 0.617921 0.941493 max_cluster Military Specific Production Healthcare Support mean 0.49476 0.349691 0.487769 min 0.149769 0.169084 0.188255 min_cluster Installation, Maintenance, and Repair Building and Grounds Cleaning and Maintenance Installation, Maintenance, and Repair std 0.226956 0.119819 0.193501 type word2vec word2vec fasttext variance 0.0515092 0.0143566 0.0374426 window 7 7 7","title":"Evaluation"},{"location":"skills_ml_tour/#occupation-classification","text":"A common issue with job posting data is incomplete, incorrect, and inconsistent occupation classification. The majority of job postings in the US are using the O*NET SOC classification system, but many are either missing or poorly classified. This can be improved by using machine learning.","title":"Occupation Classification"},{"location":"skills_ml_tour/#soc-codes","text":"Most of the job posting data collected are aligned with the O*NET SOC system. The occupations in the SOC are classified at four levels of aggregation: major group, minor group, broad occupation, and detailed occupation. Each lower level of detail identifies a more specific group of occupations. Each item in the SOC is designated by a six-digit code. The first two digits represent the major group, the third digit represents the minor group, the fourth and fifth digits represent the broad occupation, and the sixth digit represents the detailed occupation. - Major group codes end with 0000 (e.g., 29-0000 Healthcare Practitioners and Technical Occupations \u2014the exceptions are minor groups 15-1200 Computer Occupations, 31- 1100 Home Health and Personal Care Aides; and Nursing Assistants, Orderlies, and Psychiatric Aides, and 51-5100 Printing Workers, which end with 00). - Minor groups generally end with 000 (e.g., 29-1000 Health Diagnosing or Treating Practitioners). - Broad occupations end with 0 (e.g., 29-1020 Dentists). - Detailed occupations end with a number other than 0 (e.g., 29-1022 Oral and Maxillofacial Surgeons).","title":"SOC Codes"},{"location":"skills_ml_tour/#target-variable","text":"FullSOC SOCMajorGroup from skills_ml.algorithms.occupation_classifiers import FullSOC, SOCMajorGroup full_soc = FullSOC(onet_cache=onet)","title":"Target Variable"},{"location":"skills_ml_tour/#design-matrix","text":"The classification task consists of inferring a SOC code from a job posting and is accomplished through several stages: preprocessing, filtering, training and testing. DesignMatrix helps users accomplish this task. import random from itertools import islice from skills_ml.utils import itershuffle from skills_ml.algorithms.occupation_classifiers import DesignMatrix sample = JobSampler(job_posting_generator=jobpostings_filtered, k=5000, key=lambda x: x['onet_soc_code'][:2], weights=weights) dataset = itershuffle(sample) train = islice(dataset, 0, 4000) test = islice(dataset, 4000) pipe_x = IterablePipeline( partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.clean_str, nlp.word_tokenize, partial(nlp.vectorize, embedding_model=fasttext) ) pipe_y = IterablePipeline( full_soc.transformer ) matrix = DesignMatrix( train, full_soc, pipe_x, pipe_y, )","title":"Design Matrix"},{"location":"skills_ml_tour/#occupationclassifiertrainer","text":"OccupationClassifierTrainer trains classifiers with cross validation and picks the best classifier with a grid search based on the metric. It takes in a dictionary for the grid search. from skills_ml.algorithms.occupation_classifiers.train import OccupationClassifierTrainer grid_config = { 'sklearn.ensemble.ExtraTreesClassifier': { 'n_estimators': [50, 100], 'criterion': ['entropy', 'gini'], 'max_depth': [20], 'max_features': ['log2'], 'min_samples_split': [10] }, 'sklearn.neural_network.MLPClassifier': { 'hidden_layer_sizes': [100, 500], 'activation': ['logistic', 'relu'], 'solver': ['adam'] }, } cls_trainer = OccupationClassifierTrainer( matrix=matrix, k_folds=3, grid_config=grid_config, storage=FSStore('tmp/model_cache/soc_classifiers/examples'), n_jobs=4) cls_trainer.train(save=False) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release. from numpy.core.umath_tests import inner1d /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3. % (min_groups, self.n_splits)), Warning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet. % self.max_iter, ConvergenceWarning) cls_trainer.best_estimators [<ProxyObjectWithStorage at 0x7f568b8360f8 for GridSearchCV at 0x7f5688979358>, <ProxyObjectWithStorage at 0x7f568b38d888 for GridSearchCV at 0x7f5688c20d68>]","title":"OccupationClassifierTrainer"},{"location":"skills_ml_tour/#evaluation_1","text":"Accuracy, recall, precision and f1 are the metrics taken into consideration. Since it is a multi-class classification problem, an overall performance is evaluated by looking at the micro-average and macro-average for the metrics. A macro-average will compute the metric independently for each class and then take the average, whereas a micro-average will aggregate the contributions of all classes and then computes the average. In other words, a macro-average is treating all classes equally. from skills_ml.algorithms.occupation_classifiers.test import OccupationClassifierTester from skills_ml.evaluation.occ_cls_evaluator import OnetOccupationClassificationEvaluator from skills_ml.algorithms.occupation_classifiers.classifiers import CombinedClassifier from skills_ml.algorithms.embedding.train import Reiterable steps = [ partial(nlp.fields_join, document_schema_fields=document_schema_fields), nlp.normalize, nlp.clean_str, nlp.word_tokenize, ] evaluators = [] test_data = list(test) for cls in cls_trainer.best_estimators: tester = OccupationClassifierTester( test_data_generator=test_data, preprocessing=steps, classifier=CombinedClassifier(fasttext, cls) ) evaluators.append(OnetOccupationClassificationEvaluator(tester)) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'itershuffle' raised StopIteration for e, c in zip(evaluators, cls_trainer.best_estimators): print(c.best_estimator_) print('accuracy: ', e.accuracy) print('precision: ', e.precision) print('f1: ', e.f1) print('major group: ', e.accuracy_major_group) print('macro precision: ', e.macro_precision) print('micro precision: ', e.micro_precision) print('recall: ', e.recall) print('macro recall: ', e.macro_recall) print('micro recall: ', e.micro_recall) print('macro f1: ', e.macro_f1) print('micro f1: ', e.micro_f1) print('\\n') ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini', max_depth=20, max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4, oob_score=False, random_state=None, verbose=0, warm_start=False) accuracy: 0.478 precision: [0. 0. 0.10891089 ... 0. 0. 0. ] f1: [0. 0. 0.18032787 ... 0. 0. 0. ] major group: 0.554 macro precision: 0.3553188734850493 micro precision: 0.478 recall: [0. 0. 0.52380952 ... 0. 0. 0. ] macro recall: 0.2807984722668254 micro recall: 0.478 macro f1: 0.28787327281642394 micro f1: 0.478 MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=500, learning_rate='constant', learning_rate_init=0.001, max_iter=200, momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) accuracy: 0.514 precision: [0. 0. 0.14285714 ... 0. 0. 0. ] f1: [0. 0. 0.2 ... 0. 0. 0. ] major group: 0.618 macro precision: 0.2966284764221296 micro precision: 0.514 recall: [0. 0. 0.33333333 ... 0. 0. 0. ] macro recall: 0.2887478119504879 micro recall: 0.514 macro f1: 0.27660324759833993 micro f1: 0.514 /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for) /home/ubuntu/.pyenv/versions/3.6.5/envs/env3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. 'recall', 'true', average, warn_for)","title":"Evaluation"}]}