{
    "docs": [
        {
            "location": "/",
            "text": "skill-ml\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Skills Project - Machine Learning\n\n\nThis is the library for the methods usable by the Open Skills API, including processing algorithms and utilities for computing our jobs and skills taxonomy.\n\n\nDocumentation\n\u00b6\n\n\nHosted on Github Pages\n\n\nQuick Start\n\u00b6\n\n\n1. Virtualenv\n\u00b6\n\n\nskills-ml\n depends on python3.6, so create a virtual environment using a python3.6 executable.\n\n\nvirtualenv venv -p /usr/bin/python3.6\n\n\n\n\nActivate your virtualenv\n\n\nsource venv/bin/activate\n\n\n\n\n2. Installation\n\u00b6\n\n\npip install skills-ml\n\n\n\n\n3. Import skills_ml\n\u00b6\n\n\nimport skills_ml\n\n\n\n\nskills-ml doesn't have a tutorial yet, but here are some useful places to start.\n- There are a couple of examples of specific uses of components to perform specific tasks in \nexamples\n.\n- Check out the descriptions of different algorithm types in \nalgorithms/\n and look at any individual directories that match what you'd like to do (e.g. skill extraction, job title normalization)\n- \nskills-airflow\n is the open-source production system that uses skills-ml algorithms in an Airflow pipeline to generate open datasets\n\n\nBuilding the Documentation\n\u00b6\n\n\nskills-ml uses a forked version of pydocmd, and a custom script to keep the pydocmd config file up to date. Here's how to keep the docs updated before you push:\n\n\n$ cd docs\n$ PYTHONPATH=\"../\" python update_docs.py # this will update docs/pydocmd.yml with the package/module structure\n$ pydocmd serve # will serve local documentation that you can check in your browser\n$ pydocmd gh-deploy # will update the gh-pages branch\n\n\nStructure\n\u00b6\n\n\n\n\nalgorithms/\n - Core algorithmic module. Each submodule is meant to contain a different type of component, such as a job title normalizer or a skill tagger, with a common interface so different pipelines can try out different versions of the components.\n\n\ndatasets/\n - Wrappers for interfacing with different datasets, such as ONET, Urbanized Area.\n\n\nevaluation/\n - Code for testing different components against each other.\n\n\n\n\nContributors\n\u00b6\n\n\n\n\nKwame Porter Robinson - \nGithub\n\n\nEddie Lin - \nGithub\n\n\nTristan Crockett - \nGithub\n\n\nZoo Chai - \nGithub\n\n\n\n\nLicense\n\u00b6\n\n\nThis project is licensed under the MIT License - see the \nLICENSE.md\n file for details.",
            "title": "Home"
        },
        {
            "location": "/#skill-ml",
            "text": "Open Skills Project - Machine Learning  This is the library for the methods usable by the Open Skills API, including processing algorithms and utilities for computing our jobs and skills taxonomy.",
            "title": "skill-ml"
        },
        {
            "location": "/#documentation",
            "text": "Hosted on Github Pages",
            "title": "Documentation"
        },
        {
            "location": "/#quick-start",
            "text": "",
            "title": "Quick Start"
        },
        {
            "location": "/#1-virtualenv",
            "text": "skills-ml  depends on python3.6, so create a virtual environment using a python3.6 executable.  virtualenv venv -p /usr/bin/python3.6  Activate your virtualenv  source venv/bin/activate",
            "title": "1. Virtualenv"
        },
        {
            "location": "/#2-installation",
            "text": "pip install skills-ml",
            "title": "2. Installation"
        },
        {
            "location": "/#3-import-skills_ml",
            "text": "import skills_ml  skills-ml doesn't have a tutorial yet, but here are some useful places to start.\n- There are a couple of examples of specific uses of components to perform specific tasks in  examples .\n- Check out the descriptions of different algorithm types in  algorithms/  and look at any individual directories that match what you'd like to do (e.g. skill extraction, job title normalization)\n-  skills-airflow  is the open-source production system that uses skills-ml algorithms in an Airflow pipeline to generate open datasets",
            "title": "3. Import skills_ml"
        },
        {
            "location": "/#building-the-documentation",
            "text": "skills-ml uses a forked version of pydocmd, and a custom script to keep the pydocmd config file up to date. Here's how to keep the docs updated before you push:  $ cd docs\n$ PYTHONPATH=\"../\" python update_docs.py # this will update docs/pydocmd.yml with the package/module structure\n$ pydocmd serve # will serve local documentation that you can check in your browser\n$ pydocmd gh-deploy # will update the gh-pages branch",
            "title": "Building the Documentation"
        },
        {
            "location": "/#structure",
            "text": "algorithms/  - Core algorithmic module. Each submodule is meant to contain a different type of component, such as a job title normalizer or a skill tagger, with a common interface so different pipelines can try out different versions of the components.  datasets/  - Wrappers for interfacing with different datasets, such as ONET, Urbanized Area.  evaluation/  - Code for testing different components against each other.",
            "title": "Structure"
        },
        {
            "location": "/#contributors",
            "text": "Kwame Porter Robinson -  Github  Eddie Lin -  Github  Tristan Crockett -  Github  Zoo Chai -  Github",
            "title": "Contributors"
        },
        {
            "location": "/#license",
            "text": "This project is licensed under the MIT License - see the  LICENSE.md  file for details.",
            "title": "License"
        },
        {
            "location": "/examples/",
            "text": "Usage Examples\n\u00b6\n\n\nCorpus Creator with Sampling and Filtering\n\u00b6\n\n\nTo showcase the corpus creator and its options, we generate a few different job postings corpora:\n\n\n\n\na simple one from a single quarter's worth of data\n\n\nfiltered on different fields like SOC code and base salary\n\n\nreservoir-sampled versions of each of the above\n\n\n\n\nExtracting Skills using Noun Phrase Endings\n\u00b6\n\n\nTo showcase the noun phrase skill extractor, we download open job postings\nfrom Virginia Tech's open data portal and run them through the skill extractor.\nIn the end, we have the most commonly occurring noun phrases ending in\n'skill' or 'skills'.\n\n\nGenerate Skill Candidates for Further Evaluation\n\u00b6\n\n\nTo showcase how skill extraction algorithms can be tested, we run extraction several times with different parameters:\n\n\n\n\nSkill extraction algorithms (exact, fuzzy matching)\n\n\nBase skill lists (ONET abilities, ONET skills, ONET knowledge)\n\n\nSamples (a 300 job posting sample, a 10k job posting sample)\n\n\n\n\nFor each combination of the above parameters, we upload the extracted skill candidates to S3 for further evaluation, for instance by a human labeller. In addition, this example shows how to parallelize the skill extraction.\n\n\nTrain an Word2Vec Embedding Model using Quarterly Jobposting Data\n\u00b6\n\n\nTo showcase the interface of training a word2vec embedding model in an online batch learning fashion:\n\n\n\n\nA list of quarters for creating the corpus from job posting data\n\n\nA trainer object that specifies some parameters of source, s3 path, batch size, model type ...etc.\n\n\nThe train method takes whatever arugments \ngensim.models.word2vec.Word2Vec\n or \ngensim.model.doc2vec.Doc2Vec\n has\n\n\n\n\nCompute and Aggregate Properties of Job Postings as a Tabular Dataset\n\u00b6\n\n\nTo show job posting property computation and aggregation,\nwe calculate job posting counts by cleaned title, and upload\nthe resulting CSV to S3.\n\n\nThis is essentially a mini version of the Data@Work Research Hub.\n\n\nTo enable this example to be run with as few dependencies as possible, we use:\n\n\n\n\na fake local s3 instance\n\n\na sample of the Virginia Tech open job postings dataset\n\n\nonly title cleaning and job counting.",
            "title": "Examples"
        },
        {
            "location": "/examples/#usage-examples",
            "text": "",
            "title": "Usage Examples"
        },
        {
            "location": "/examples/#corpus-creator-with-sampling-and-filtering",
            "text": "To showcase the corpus creator and its options, we generate a few different job postings corpora:   a simple one from a single quarter's worth of data  filtered on different fields like SOC code and base salary  reservoir-sampled versions of each of the above",
            "title": "Corpus Creator with Sampling and Filtering"
        },
        {
            "location": "/examples/#extracting-skills-using-noun-phrase-endings",
            "text": "To showcase the noun phrase skill extractor, we download open job postings\nfrom Virginia Tech's open data portal and run them through the skill extractor.\nIn the end, we have the most commonly occurring noun phrases ending in\n'skill' or 'skills'.",
            "title": "Extracting Skills using Noun Phrase Endings"
        },
        {
            "location": "/examples/#generate-skill-candidates-for-further-evaluation",
            "text": "To showcase how skill extraction algorithms can be tested, we run extraction several times with different parameters:   Skill extraction algorithms (exact, fuzzy matching)  Base skill lists (ONET abilities, ONET skills, ONET knowledge)  Samples (a 300 job posting sample, a 10k job posting sample)   For each combination of the above parameters, we upload the extracted skill candidates to S3 for further evaluation, for instance by a human labeller. In addition, this example shows how to parallelize the skill extraction.",
            "title": "Generate Skill Candidates for Further Evaluation"
        },
        {
            "location": "/examples/#train-an-word2vec-embedding-model-using-quarterly-jobposting-data",
            "text": "To showcase the interface of training a word2vec embedding model in an online batch learning fashion:   A list of quarters for creating the corpus from job posting data  A trainer object that specifies some parameters of source, s3 path, batch size, model type ...etc.  The train method takes whatever arugments  gensim.models.word2vec.Word2Vec  or  gensim.model.doc2vec.Doc2Vec  has",
            "title": "Train an Word2Vec Embedding Model using Quarterly Jobposting Data"
        },
        {
            "location": "/examples/#compute-and-aggregate-properties-of-job-postings-as-a-tabular-dataset",
            "text": "To show job posting property computation and aggregation,\nwe calculate job posting counts by cleaned title, and upload\nthe resulting CSV to S3.  This is essentially a mini version of the Data@Work Research Hub.  To enable this example to be run with as few dependencies as possible, we use:   a fake local s3 instance  a sample of the Virginia Tech open job postings dataset  only title cleaning and job counting.",
            "title": "Compute and Aggregate Properties of Job Postings as a Tabular Dataset"
        },
        {
            "location": "/ontologies/",
            "text": "Working With Ontologies\n\u00b6\n\n\nskills-ml is introducing the CompetencyOntology class, for a rich, flexible representation of competencies, occupations, and their relationships with each other. The CompetencyOntology class is backed by JSON-LD, and based on Credential Engine's \nCTDL-ASN format for Competencies\n. The goal is to be able to read in any CTDL-ASN framework and produce a CompetencyOntology object for use throughout the skills-ml library.\n\n\nFurthermore, skills-ml contains pre-mapped versions of open frameworks like ONET for use out of the box.\n\n\nCompetency\n\u00b6\n\n\nA competency, in the CTDL-ASN context, refers some knowledge, skill, or ability that a person can possess or learn. Each competency contains:\n\n\n\n\nA unique identifier within the ontology. If you're familiar with ONET, think the table of contents identifiers (e.g. '1.A.1.a.1')\n\n\nSome basic textual information: a name (e.g. Oral Comprehension) and/or description (e.g. 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'),\n, and maybe a general textual category (e.g. Ability)\n\n\nAssociative information with other competencies. A basic example is a parent/child relationship, for instance ONET's definition of 'Oral Comprehension' as the child of another competency called 'Verbal Abilities'. CTDL-ASN encodes this using the 'hasChild' and 'isChildOf' properties, and this is used in skills-ml. There many other types of associations competencies can have with each other that the Competency class in skills-ml does not yet address, you can read more at the \nCredential Engine's definition ofCompetency\n.\n\n\n\n\nThe Competency class tracks all of this. It can be created using either keyword arguments in the class' Constructor or through a class method that loads from JSON-LD. \n\n\nBasic Example\n\u00b6\n\n\nUsing Python Constructor\n\n\nfrom skills_ml.ontologies import Competency\n\ndinosaur_riding = Competency(\n    identifier='12345',\n    name='Dinosaur Riding',\n    description='Using the back of a dinosaur for transportation'\n)\n\n\n\n\nUsing JSON-LD\n\n\nfrom skills_ml.ontologies import Competency\n\ndinosaur_riding = Competency.from_jsonld({\n    '@type': 'Competency',\n    '@id': '12345',\n    'name': 'Dinosaur Riding',\n    'description': 'Using the back of a dinosaur for transportation'\n})\n\n\n\n\nTo aid in bi-directional searching, the Competency object is meant to include a parent/child relationshiop on both the parent and child objects. The add_parent and add_child methods modify both the parent and child objects to easily maintain this bi-directional relationship.\n\n\nExample parent/child relationship\n\u00b6\n\n\nUsing Python Constructor\n\n\nfrom skills_ml.ontologies import Competency\n\ndinosaur_riding = Competency(\n    identifier='12345',\n    name='Dinosaur Riding',\n    description='Using the back of a dinosaur for transportation'\n)\n\nextreme_transportation = Competency(\n    identifier='123',\n    name='Extreme Transportation',\n    description='Comically dangerous forms of transportation'\n)\ndinosaur_riding.add_parent(extreme_transportation)\nprint(dinosaur_riding.parents)\nprint(extreme_transportation.children)\n\n\n\n\nUsing JSON-LD\n\n\ndinosaur_riding = Competency.from_jsonld({\n    '@type': 'Competency',\n    '@id': '12345',\n    'name': 'Dinosaur Riding',\n    'description': 'Using the back of a dinosaur for transportation',\n    'isChildOf': [{'@type': 'Competency', '@id': '123'}]\n})\n\nextreme_transportation = Competency.from_jsonld({\n    '@type': 'Competency',\n    '@id': '123',\n    'name': 'Extreme Transportation',\n    'description': 'Comically dangerous forms of transportation',\n    'hasChild': [{'@type': 'Competency', '@id': '12345'}]\n\n\n\n\nOccupation\n\u00b6\n\n\nAn Occupation is a job or profession that a person can hold. CTDL-ASN does not define this, so skills-ml models the Occupation similarly to the Competency, albeit with far less detail. \n\n\n\n\nA unique identifier within the ontology. If you're familiar with ONET, think of an ONET SOC code (11-1011.00)\n\n\nSome basic textual information: a name (e.g. Civil Engineer), maybe a description.\n\n\nAssociative information with other occupations. So far the only relationship modeled in skills-ml between occupations is a parent/child one, similarly to Competency. Going back to the ONET example, an occupation representing the major group (identifier 11) may be thought of as the parent of SOC code 11-1011.00.\n\n\n\n\nBasic Example\n\u00b6\n\n\nUsing Python Constructor\n\n\nfrom skills_ml.ontologies import Occupation\n\ndinosaur_rider = Occupation(\n    identifier='9999',\n    name='Dinosaur Rider',\n)\n\n\n\n\nUsing JSON-LD\n\n\nfrom skills_ml.ontologies import Occupation\n\ndinosaur_rider = Occupation.from_jsonld({\n    '@type': 'Occupation',\n    '@id': '9999',\n    'name': 'Dinosaur Rider'\n})\n\n\n\n\nCompetencyOccupationEdge\n\u00b6\n\n\nA CompetencyOccupationEdge is simply a relationship between a Competency and an Occupation. Currently, tthere are no further properties defined on this edge, though this will likely change in the future. \n\n\nBasic Example\n\u00b6\n\n\nUsing Python Constructor\n\n\nfrom skills_ml.ontologies import CompetencyOccupationEdge\n\nCompetencyOccupationEdge(\n    occupation=dinosaur_rider,\n    competency=dinosaur_riding\n)\n\n\n\n\nUsing JSON-LD\n\n\nfrom skills_ml.ontologies import CompetencyOccupationEdge\n\nCompetencyOccupationEdge.from_jsonld({\n    '@type': 'CompetencyOccupationEdge',\n    '@id': 'competency=12345;occupation=9999',\n    'competency': {'@type': 'Competency', '@id': '12345'},\n    'occupation': {'@type': 'Occupation', '@id': '9999'}\n})\n\n\n\n\nCompetencyOntology\n\u00b6\n\n\nAn ontology represents a collection of competencies, a collection of occupations, and a collection of all relationships between competencies and occupations. The CompetencyOntology class represents each of these three collections using a \nset\n object. The identifiers for all of those objects are used to disambiguate between items in each of these sets. The JSON-LD representation of the ontology mirrors this internal structure.\n\n\nBelow is an example of the objects defined above arranged into a CompetencyOntology. For brevity, the descriptions are omitted. \n\n\nNote in the Python example that importing the CompetencyOccupationEdge class is not necessary when using the Ontology; the \nadd_edge\n method of Ontology can simply take a competency and occupation directly. \n\n\nBasic Example\n\u00b6\n\n\nUsing Python Constructor\n\n\nfrom skills_ml.ontologies import Competency, Occupation, CompetencyOntology\n\nontology = CompetencyOntology()\n\ndinosaur_riding = Competency(identifier='12345', name='Dinosaur Riding')\nextreme_transportation = Competency(identifier='123', name='Extreme Transportation')\ndinosaur_riding.add_parent(extreme_transportation)\n\n\ndinosaur_rider = Occupation(identifier='9999', name='Dinosaur Rider')\n\nontology.add_competency(dinosaur_riding)\nontology.add_competency(extreme_transportation)\nontology.add_occupation(dinosaur_rider)\nontology.add_edge(occupation=dinosaur_rider, competency=dinosaur_riding)\n\n\n\n\nUsing JSON-LD\n\n\nfrom skills_ml.ontologies import CompetencyOntology\n\nontology = CompetencyOntology.from_jsonld({\n    'competencies': [{\n        '@type': 'Competency',\n        '@id': '12345',\n        'name': 'Dinosaur Riding',\n        'description': 'Using the back of a dinosaur for transportation',\n        'isChildOf': [{'@type': 'Competency', '@id': '123'}]\n    }, {\n        '@type': 'Competency',\n        '@id': '123',\n        'name': 'Extreme Transportation',\n        'description': 'Comically dangerous forms of transportation',\n        'hasChild': [{'@type': 'Competency', '@id': '12345'}]\n    }],\n    'occupations': [{\n        '@type': 'Occupation',\n        '@id': '9999',\n        'name': 'Dinosaur Rider'\n    }],\n    'edges': [{\n        '@type': 'CompetencyOccupationEdge',\n        '@id': 'competency=12345;occupation=9999',\n        'competency': {'@type': 'Competency', '@id': '12345'},\n        'occupation': {'@type': 'Occupation', '@id': '9999'}\n    }]\n})\n\n\n\n\nIncluded Ontologies\n\u00b6\n\n\nONET\n\u00b6\n\n\nThe \nskills_ml.ontologies.onet\n module contains a builder function to create a CompetencyOntology object from a variety of files on the ONET site, using at the time of writing the latest version of onet (db_v22_3):\n\n\n\n\nContent Model Reference.txt\n\n\nKnowledge.txt\n\n\nSkills.txt\n\n\nAbilities.txt\n\n\nTools and Technology.txt\n\n\nOccupation Data.txt\n\n\n\n\n\nfrom skills.ml.ontologies.onet import build_onet\n\nONET = build_onet()\n# this will take a while as it downloads the relatively large files and processes them\nONET.filter_by(lambda edge: 'forklift' in edge.competency.name)\n\n\n\n\nIf you pass in an ONET cache object, the raw ONET files can be cached on your filesystem so that building it the second time will be faster.\n\n\nfrom skills_ml.storage import FSStore\nfrom skills_ml.datasets.onet_cache import OnetSiteCache\nfrom skills_ml.ontologies.onet import build_onet\n\nONET = build_onet(OnetSiteCache(FSStore('onet_cache')))\n\n\n\n\nUses of Ontologies\n\u00b6\n\n\nThere isn't much functionality built up around ontologies just yet, but the class is under heavy development. \n\n\nFiltering\n\u00b6\n\n\nYou can filter the graph to produce subsets based on the list of edges. This will return another CompetencyOntology object, so any code that takes an ontology as input will work on the subsetted graph.\n\n\n\n# Return an ontology that consists only of competencies with 'python' in the name, along with their related occupations\nontology.filter_by(lambda edge: 'python' in edge.competency.name.lower())\n\n# Return an ontology that consists only of occupations with 'software' in the name, along with their associated competencies\nontology.filter_by(lambda edge: 'software' in edge.competency.name.lower())\n\n# Return an ontology that is the intersection of 'python' competencies and 'software' occupations\nontology.filter_by(lambda edge: 'software' in edge.occupation.name.lower() and 'python' in edge.competency.name.lower())\n\n# Return only competencies who have a parent competency containing 'software'\nontology.filter_by(lambda edge: any('software' in parent.name.lower() for parent in edge.parents)\n\n\n\n\nRetrieving Leaf Strings\n\u00b6\n\n\nRetrieving all the detailed competencies as a list of strings can be helpful for using as a reference list for skill extraction.\n\n\n\nleaf_skill_strings = [\n    competency.name for competency in \n    ontology.filter_by(lambda edge: len(edge.competency.children) == 0).competencies\n]\n\n\n\n\nExporting as JSON-LD\n\u00b6\n\n\nYou can export an ontology as a JSON-LD object for storage that you can later import\n\n\n\nimport json\n\nwith open('out.json', 'w') as f:\n    json.dump(ontology.jsonld, f)",
            "title": "Ontology Class"
        },
        {
            "location": "/ontologies/#working-with-ontologies",
            "text": "skills-ml is introducing the CompetencyOntology class, for a rich, flexible representation of competencies, occupations, and their relationships with each other. The CompetencyOntology class is backed by JSON-LD, and based on Credential Engine's  CTDL-ASN format for Competencies . The goal is to be able to read in any CTDL-ASN framework and produce a CompetencyOntology object for use throughout the skills-ml library.  Furthermore, skills-ml contains pre-mapped versions of open frameworks like ONET for use out of the box.",
            "title": "Working With Ontologies"
        },
        {
            "location": "/ontologies/#competency",
            "text": "A competency, in the CTDL-ASN context, refers some knowledge, skill, or ability that a person can possess or learn. Each competency contains:   A unique identifier within the ontology. If you're familiar with ONET, think the table of contents identifiers (e.g. '1.A.1.a.1')  Some basic textual information: a name (e.g. Oral Comprehension) and/or description (e.g. 'The ability to listen to and understand information and ideas presented through spoken words and sentences.'),\n, and maybe a general textual category (e.g. Ability)  Associative information with other competencies. A basic example is a parent/child relationship, for instance ONET's definition of 'Oral Comprehension' as the child of another competency called 'Verbal Abilities'. CTDL-ASN encodes this using the 'hasChild' and 'isChildOf' properties, and this is used in skills-ml. There many other types of associations competencies can have with each other that the Competency class in skills-ml does not yet address, you can read more at the  Credential Engine's definition ofCompetency .   The Competency class tracks all of this. It can be created using either keyword arguments in the class' Constructor or through a class method that loads from JSON-LD.",
            "title": "Competency"
        },
        {
            "location": "/ontologies/#basic-example",
            "text": "Using Python Constructor  from skills_ml.ontologies import Competency\n\ndinosaur_riding = Competency(\n    identifier='12345',\n    name='Dinosaur Riding',\n    description='Using the back of a dinosaur for transportation'\n)  Using JSON-LD  from skills_ml.ontologies import Competency\n\ndinosaur_riding = Competency.from_jsonld({\n    '@type': 'Competency',\n    '@id': '12345',\n    'name': 'Dinosaur Riding',\n    'description': 'Using the back of a dinosaur for transportation'\n})  To aid in bi-directional searching, the Competency object is meant to include a parent/child relationshiop on both the parent and child objects. The add_parent and add_child methods modify both the parent and child objects to easily maintain this bi-directional relationship.",
            "title": "Basic Example"
        },
        {
            "location": "/ontologies/#example-parentchild-relationship",
            "text": "Using Python Constructor  from skills_ml.ontologies import Competency\n\ndinosaur_riding = Competency(\n    identifier='12345',\n    name='Dinosaur Riding',\n    description='Using the back of a dinosaur for transportation'\n)\n\nextreme_transportation = Competency(\n    identifier='123',\n    name='Extreme Transportation',\n    description='Comically dangerous forms of transportation'\n)\ndinosaur_riding.add_parent(extreme_transportation)\nprint(dinosaur_riding.parents)\nprint(extreme_transportation.children)  Using JSON-LD  dinosaur_riding = Competency.from_jsonld({\n    '@type': 'Competency',\n    '@id': '12345',\n    'name': 'Dinosaur Riding',\n    'description': 'Using the back of a dinosaur for transportation',\n    'isChildOf': [{'@type': 'Competency', '@id': '123'}]\n})\n\nextreme_transportation = Competency.from_jsonld({\n    '@type': 'Competency',\n    '@id': '123',\n    'name': 'Extreme Transportation',\n    'description': 'Comically dangerous forms of transportation',\n    'hasChild': [{'@type': 'Competency', '@id': '12345'}]",
            "title": "Example parent/child relationship"
        },
        {
            "location": "/ontologies/#occupation",
            "text": "An Occupation is a job or profession that a person can hold. CTDL-ASN does not define this, so skills-ml models the Occupation similarly to the Competency, albeit with far less detail.    A unique identifier within the ontology. If you're familiar with ONET, think of an ONET SOC code (11-1011.00)  Some basic textual information: a name (e.g. Civil Engineer), maybe a description.  Associative information with other occupations. So far the only relationship modeled in skills-ml between occupations is a parent/child one, similarly to Competency. Going back to the ONET example, an occupation representing the major group (identifier 11) may be thought of as the parent of SOC code 11-1011.00.",
            "title": "Occupation"
        },
        {
            "location": "/ontologies/#basic-example_1",
            "text": "Using Python Constructor  from skills_ml.ontologies import Occupation\n\ndinosaur_rider = Occupation(\n    identifier='9999',\n    name='Dinosaur Rider',\n)  Using JSON-LD  from skills_ml.ontologies import Occupation\n\ndinosaur_rider = Occupation.from_jsonld({\n    '@type': 'Occupation',\n    '@id': '9999',\n    'name': 'Dinosaur Rider'\n})",
            "title": "Basic Example"
        },
        {
            "location": "/ontologies/#competencyoccupationedge",
            "text": "A CompetencyOccupationEdge is simply a relationship between a Competency and an Occupation. Currently, tthere are no further properties defined on this edge, though this will likely change in the future.",
            "title": "CompetencyOccupationEdge"
        },
        {
            "location": "/ontologies/#basic-example_2",
            "text": "Using Python Constructor  from skills_ml.ontologies import CompetencyOccupationEdge\n\nCompetencyOccupationEdge(\n    occupation=dinosaur_rider,\n    competency=dinosaur_riding\n)  Using JSON-LD  from skills_ml.ontologies import CompetencyOccupationEdge\n\nCompetencyOccupationEdge.from_jsonld({\n    '@type': 'CompetencyOccupationEdge',\n    '@id': 'competency=12345;occupation=9999',\n    'competency': {'@type': 'Competency', '@id': '12345'},\n    'occupation': {'@type': 'Occupation', '@id': '9999'}\n})",
            "title": "Basic Example"
        },
        {
            "location": "/ontologies/#competencyontology",
            "text": "An ontology represents a collection of competencies, a collection of occupations, and a collection of all relationships between competencies and occupations. The CompetencyOntology class represents each of these three collections using a  set  object. The identifiers for all of those objects are used to disambiguate between items in each of these sets. The JSON-LD representation of the ontology mirrors this internal structure.  Below is an example of the objects defined above arranged into a CompetencyOntology. For brevity, the descriptions are omitted.   Note in the Python example that importing the CompetencyOccupationEdge class is not necessary when using the Ontology; the  add_edge  method of Ontology can simply take a competency and occupation directly.",
            "title": "CompetencyOntology"
        },
        {
            "location": "/ontologies/#basic-example_3",
            "text": "Using Python Constructor  from skills_ml.ontologies import Competency, Occupation, CompetencyOntology\n\nontology = CompetencyOntology()\n\ndinosaur_riding = Competency(identifier='12345', name='Dinosaur Riding')\nextreme_transportation = Competency(identifier='123', name='Extreme Transportation')\ndinosaur_riding.add_parent(extreme_transportation)\n\n\ndinosaur_rider = Occupation(identifier='9999', name='Dinosaur Rider')\n\nontology.add_competency(dinosaur_riding)\nontology.add_competency(extreme_transportation)\nontology.add_occupation(dinosaur_rider)\nontology.add_edge(occupation=dinosaur_rider, competency=dinosaur_riding)  Using JSON-LD  from skills_ml.ontologies import CompetencyOntology\n\nontology = CompetencyOntology.from_jsonld({\n    'competencies': [{\n        '@type': 'Competency',\n        '@id': '12345',\n        'name': 'Dinosaur Riding',\n        'description': 'Using the back of a dinosaur for transportation',\n        'isChildOf': [{'@type': 'Competency', '@id': '123'}]\n    }, {\n        '@type': 'Competency',\n        '@id': '123',\n        'name': 'Extreme Transportation',\n        'description': 'Comically dangerous forms of transportation',\n        'hasChild': [{'@type': 'Competency', '@id': '12345'}]\n    }],\n    'occupations': [{\n        '@type': 'Occupation',\n        '@id': '9999',\n        'name': 'Dinosaur Rider'\n    }],\n    'edges': [{\n        '@type': 'CompetencyOccupationEdge',\n        '@id': 'competency=12345;occupation=9999',\n        'competency': {'@type': 'Competency', '@id': '12345'},\n        'occupation': {'@type': 'Occupation', '@id': '9999'}\n    }]\n})",
            "title": "Basic Example"
        },
        {
            "location": "/ontologies/#included-ontologies",
            "text": "",
            "title": "Included Ontologies"
        },
        {
            "location": "/ontologies/#onet",
            "text": "The  skills_ml.ontologies.onet  module contains a builder function to create a CompetencyOntology object from a variety of files on the ONET site, using at the time of writing the latest version of onet (db_v22_3):   Content Model Reference.txt  Knowledge.txt  Skills.txt  Abilities.txt  Tools and Technology.txt  Occupation Data.txt   \nfrom skills.ml.ontologies.onet import build_onet\n\nONET = build_onet()\n# this will take a while as it downloads the relatively large files and processes them\nONET.filter_by(lambda edge: 'forklift' in edge.competency.name)  If you pass in an ONET cache object, the raw ONET files can be cached on your filesystem so that building it the second time will be faster.  from skills_ml.storage import FSStore\nfrom skills_ml.datasets.onet_cache import OnetSiteCache\nfrom skills_ml.ontologies.onet import build_onet\n\nONET = build_onet(OnetSiteCache(FSStore('onet_cache')))",
            "title": "ONET"
        },
        {
            "location": "/ontologies/#uses-of-ontologies",
            "text": "There isn't much functionality built up around ontologies just yet, but the class is under heavy development.",
            "title": "Uses of Ontologies"
        },
        {
            "location": "/ontologies/#filtering",
            "text": "You can filter the graph to produce subsets based on the list of edges. This will return another CompetencyOntology object, so any code that takes an ontology as input will work on the subsetted graph.  \n# Return an ontology that consists only of competencies with 'python' in the name, along with their related occupations\nontology.filter_by(lambda edge: 'python' in edge.competency.name.lower())\n\n# Return an ontology that consists only of occupations with 'software' in the name, along with their associated competencies\nontology.filter_by(lambda edge: 'software' in edge.competency.name.lower())\n\n# Return an ontology that is the intersection of 'python' competencies and 'software' occupations\nontology.filter_by(lambda edge: 'software' in edge.occupation.name.lower() and 'python' in edge.competency.name.lower())\n\n# Return only competencies who have a parent competency containing 'software'\nontology.filter_by(lambda edge: any('software' in parent.name.lower() for parent in edge.parents)",
            "title": "Filtering"
        },
        {
            "location": "/ontologies/#retrieving-leaf-strings",
            "text": "Retrieving all the detailed competencies as a list of strings can be helpful for using as a reference list for skill extraction.  \nleaf_skill_strings = [\n    competency.name for competency in \n    ontology.filter_by(lambda edge: len(edge.competency.children) == 0).competencies\n]",
            "title": "Retrieving Leaf Strings"
        },
        {
            "location": "/ontologies/#exporting-as-json-ld",
            "text": "You can export an ontology as a JSON-LD object for storage that you can later import  \nimport json\n\nwith open('out.json', 'w') as f:\n    json.dump(ontology.jsonld, f)",
            "title": "Exporting as JSON-LD"
        },
        {
            "location": "/common_schema/",
            "text": "Job Posting Common Schema\n\u00b6\n\n\nskills-ml makes heavy use of schema.org's \njob posting\n schema, generally stored in JSON format. Here is an example:\n\n\n{\n    \"incentiveCompensation\": \"\",\n    \"experienceRequirements\": \"Here are some experience and requirements\",\n    \"baseSalary\": {\"maxValue\": 0.0, \"@type\": \"MonetaryAmount\", \"minValue\": 0.0},\n    \"description\": \"We are looking for a person to fill this job\",\n    \"title\": \"Bilingual (Italian) Customer Service Rep (Work from Home)\",\n    \"employmentType\": \"Full-Time\",\n    \"industry\": \"Call Center / SSO / BPO, Consulting, Sales - Marketing\",\n    \"occupationalCategory\": \"\",\n    \"qualifications\": \"Here are some qualifications\",\n    \"educationRequirements\": \"Not Specified\",\n    \"skills\": \"Customer Service, Consultant, Entry Level\",\n    \"validThrough\": \"2014-02-05T00:00:00\",\n    \"jobLocation\": {\"@type\": \"Place\", \"address\": {\"addressLocality\": \"Salisbury\", \"addressRegion\": \"PA\", \"@type\": \"PostalAddress\"}},\n    \"@context\": \"http://schema.org\",\n    \"alternateName\": \"Customer Service Representative\",\n    \"datePosted\": \"2013-03-07\",\n    \"@type\": \"JobPosting\"\n}",
            "title": "Job Posting Common Schema"
        },
        {
            "location": "/common_schema/#job-posting-common-schema",
            "text": "skills-ml makes heavy use of schema.org's  job posting  schema, generally stored in JSON format. Here is an example:  {\n    \"incentiveCompensation\": \"\",\n    \"experienceRequirements\": \"Here are some experience and requirements\",\n    \"baseSalary\": {\"maxValue\": 0.0, \"@type\": \"MonetaryAmount\", \"minValue\": 0.0},\n    \"description\": \"We are looking for a person to fill this job\",\n    \"title\": \"Bilingual (Italian) Customer Service Rep (Work from Home)\",\n    \"employmentType\": \"Full-Time\",\n    \"industry\": \"Call Center / SSO / BPO, Consulting, Sales - Marketing\",\n    \"occupationalCategory\": \"\",\n    \"qualifications\": \"Here are some qualifications\",\n    \"educationRequirements\": \"Not Specified\",\n    \"skills\": \"Customer Service, Consultant, Entry Level\",\n    \"validThrough\": \"2014-02-05T00:00:00\",\n    \"jobLocation\": {\"@type\": \"Place\", \"address\": {\"addressLocality\": \"Salisbury\", \"addressRegion\": \"PA\", \"@type\": \"PostalAddress\"}},\n    \"@context\": \"http://schema.org\",\n    \"alternateName\": \"Customer Service Representative\",\n    \"datePosted\": \"2013-03-07\",\n    \"@type\": \"JobPosting\"\n}",
            "title": "Job Posting Common Schema"
        },
        {
            "location": "/skills_ml.evaluation/",
            "text": "skills_ml.evaluation.annotators\n\n\n\nBratExperiment\n\n\n\nBratExperiment(self, experiment_name, brat_s3_path)\n\n\n\n\nManage a BRAT experiment. Handles:\n\n\n\n\nThe creation of BRAT config for a specific sample of job postings\n\n\nAdding users to the installation and allocating them semi-hidden job postings\n\n\nThe parsing of the annotation results at the end of the experiment\n\n\n\n\nSyncs data to an experiment directory on S3.\nBRAT installations are expected to sync this data down regularly.\n\n\nKeeps track of a metadata file,\navailable as a dictionary at self.metadata, with the following structure:\n\n\nthese first five keys are just storage of user input to either\n\n\nthe constructor or start()\n\n\nview relevant docstrings for definitions\n\n\nsample_base_path\nsample_name\nentities_with_shortcuts\nminimum_annotations_per_posting\nmax_postings_per_allocation\n\n\nunits and allocations are far more important when reading results of an experiment\n\n\nunits: {\n    # canonical list of 'unit' (bundle of job postings) names,\n    # along with a list of tuples of job posting keys (only unique within unit)\n        and globally unique job posting ids\n    'unit_1': [\n        (posting_key_1, job_posting_id_1),\n        (posting_key_2, job_posting_id_2),\n    ],\n    'unit_2': [\n        (posting_key_1, job_posting_id_3),\n        (posting_key_2, job_posting_id_4),\n    ]\n}\nallocations: {\n    # canonical list of unit assignments to users\n    'user_1': ['unit_1', 'unit_2'],\n    'user_2': ['unit_2']\n}\n\n\nskills_ml.evaluation.job_title_normalizers\n\n\n\nTest job normalizers\n\n\nRequires 'interesting_job_titles.csv' to be populated, of format:\ninput job title description of job  ONET code\n\n\nEach task will output two CSV files, one with the normalizer's ranks\nand one without ranks. The latter is for sending to people to fill out\nand the former is for testing those results against the normalizer's\n\n\nOriginally written by Kwame Porter Robinson\n\n\nInputSchema\n\n\n\nInputSchema(self, /, *args, **kwargs)\n\n\n\n\nAn enumeration listing the data elements and indices taken from source data\n\nInterimSchema\n\n\nInterimSchema(self, /, *args, **kwargs)\n\n\n\n\nAn enumeration listing the data elements and indices after normalization\n\nNormalizerResponse\n\n\nNormalizerResponse(self, name=None, access=None, num_examples=3)\n\n\n\n\nAbstract interface for enforcing common iteration, access patterns\nto a variety of possible normalizers.\n\n\nArgs:\n    name (string): A name for the normalizer\n    access (filename or file object): A tab-delimited CSV with column order {job_title, description, soc_code}\n    num_examples (int, optional): Number of top responses to include\n\n\nNormalizers should return a list of results, ordered by relevance,\nwith 'title' and optional 'relevance_score' keys\n\n\nMiniNormalizer\n\n\n\nMiniNormalizer(self, name, access, normalize_class)\n\n\n\n\nAccess normalizer classes which can be instantiated and\nimplement 'normalize_job_title(job_title)'\n\n\nskills_ml.evaluation.representativeness_calculators\n\n\n\nCalculate representativeness of a dataset, such as job postings\n\nskills_ml.evaluation.representativeness_calculators.geo_occupation\n\n\nComputes geographic representativeness of job postings based on ONET SOC Code\n\n\nGeoOccupationRepresentativenessCalculator\n\n\n\nGeoOccupationRepresentativenessCalculator(self, geo_querier=None, normalizer=None)\n\n\n\n\nCalculates geographic representativeness of SOC Codes.\nIf a job normalizer is given, will attempt to compute SOC codes\nof jobs that have missing SOC codes\n\n\nArgs:\n    geo_querier (skills_ml.job_postings.geography_queriers) An object that can return a CBSA from a job posting\n    normalizer (skills_ml.algorithms.occupation_classifiers) An object that can return the SOC code from a job posting\n\n\nskills_ml.evaluation.skill_extraction_metrics",
            "title": "Evaluation Tools"
        },
        {
            "location": "/skills_ml.algorithms/",
            "text": "skills_ml.algorithms.embedding\n\n\n\nskills_ml.algorithms.embedding.base\n\n\n\nModelStorage class to handle gensim model storage\n\nskills_ml.algorithms.embedding.models\n\n\nEmbedding model class inherited the interface from gensim\n\nWord2VecModel\n\n\nWord2VecModel(self, *args, **kwargs)\n\n\n\n\nThe Word2VecModel Object is a base object which specifies which word-embeding model.\n\n\nExample:\n\n\nfrom skills_ml.algorithms.embedding.base import Word2VecModel\n\nword2vec_model = Word2VecModel()\n\n\n\n\nDoc2VecModel\n\n\n\nDoc2VecModel(self, *args, **kwargs)\n\n\n\n\nThe Doc2VecModel Object is a base object which specifies which word-embeding model.\n\n\nExample:\n\n\nfrom skills_ml.algorithms.embedding.base import Doc2VecModel\n\ndoc2vec_model = Doc2VecModel()\n\n\n\n\nskills_ml.algorithms.embedding.train\n\n\n\nEmbeddingTrainer\n\n\n\nEmbeddingTrainer(self, corpus_generator, model, batch_size=2000)\n\n\n\n\nAn embedding learning class.\nExample:\n\n\nfrom skills_ml.algorithms.occupation_classifiers.train import EmbeddingTrainer\nfrom skills_ml.job_postings.common_schema import JobPostingCollectionSample\nfrom skills_ml.job_postings.corpora.basic import Doc2VecGensimCorpusCreator, Word2VecGensimCorpusCreator\nfrom skills_ml.storage import FSStore\n\nmodel = Word2VecModel(size=size, min_count=min_count, iter=iter, window=window, workers=workers, **kwargs)\n\ns3_conn = S3Hook().get_conn()\njob_postings_generator = JobPostingGenerator(s3_conn, quarters, s3_path, source=\"all\")\ncorpus_generator = Word2VecGensimCorpusCreator(job_postings_generator)\nw2v = Word2VecModel(storage=FSStore(path='/tmp'), size=10, min_count=3, iter=4, window=6, workers=3)\ntrainer = EmbeddingTrainer(corpus_generator, w2v)\ntrainer.train()\ntrainer.save()\n\n\n\n\nskills_ml.algorithms.geocoders\n\n\n\nGeocoders, with caching and throttling\n\nCachedGeocoder\n\n\nCachedGeocoder(self, cache_storage, cache_fname, geocode_func=<function osm at 0x7f82bb3dc048>, sleep_time=1)\n\n\n\n\nGeocoder that uses specified storage as a cache.\n\n\nArgs:\n    cache_storage (object) FSStore() or S3Store object to store the cache\n    cache_fname (string) cache file name\n    geocode_func (function) a function that geocodes a given search string\n        defaults to the OSM geocoder provided by the geocode library\n    sleep_time (int) The time, in seconds, between geocode calls\n\n\nskills_ml.algorithms.geocoders.cbsa\n\n\n\nGiven geocode results, find matching Core-Based Statistical Areas.\n\nMatch\n\n\nMatch(self, /, *args, **kwargs)\n\n\n\n\nMatch(index, area)\n\nCachedCBSAFinder\n\n\nCachedCBSAFinder(self, cache_storage, cache_fname, shapefile_name=None, cache_dir=None)\n\n\n\n\nFind CBSAs associated with geocode results and save them to the specified storage\n\n\nGeocode results are expected in the json format provided by the python\n\ngeocoder\n module, with a 'bbox'\n\n\nThe highest-level interface is the 'find_all_cbsas_and_save' method, which\nprovides storage caching. A minimal call looks like\n\n\ncache_storage = S3Store('some-bucket')\ncache_fname = 'cbsas.json'\ncbsa_finder = CachedCBSAFinder(cache_storage=cache_storage, cache_fname=cache_fname)\ncbsa_finder.find_all_cbsas_and_save({\n    \"Flushing, NY\": { 'bbox': ['southwest': [..., ...], 'northeast': [...,...] }\n    \"Houston, TX\": { 'bbox': ['southwest': [..., ...], 'northeast': [...,...] }\n})\n\n# This usage of 'bbox' is what you can retrieve from a `geocoder` call, such as:\ngeocoder.osm('Flushing, NY').json()\n\n\n\n\nThe keys in the resulting cache will be the original search strings.\n\n\nWarning: The caching is not parallel-safe! It is recommended you should run\nonly one copy of \nfind_all_cbsas_and_save\n at a time to avoid overwriting\nthe cache file.\n\n\nArgs:\n    cache_storage (object) FSStore() or S3Store object to store the cache\n    cache_fname (string) cache file name\n    shapefile_name (string) local path to a CBSA shapefile to use\n        optional, will download TIGER 2015 shapefile if absent\n    cache_dir (string) local path to a cache directory to use if the\n        shapefile needs to be downloaded\n        optional, will use 'tmp' in working directory if absent\n\n\nskills_ml.algorithms.job_normalizers\n\n\n\nAlgorithms to normalize a job title to a smaller space\n\nskills_ml.algorithms.job_normalizers.elasticsearch\n\n\nIndexes job postings for job title normalization\n\n\nNormalizeTopNIndexer\n\n\n\nNormalizeTopNIndexer(self, quarter, job_postings_generator, job_titles_index, alias_name, **kwargs)\n\n\n\n\nCreates an index that stores data for job title normalization.\n\n\nDepends on a previously created index with job titles and occupations.\n\n\nQueries the job title/occupation index for\n1. job titles or occupations that match the job description\n2. Occupation matches\n\n\nThe top three results are indexed.\n\n\nArgs:\n    quarter (string) the quarter from which to retrieve job postings\n    job_postings_generator (iterable) an iterable of job postings\n    job_title_index (string) The name of an already existing job title/occupation index\n\n\nskills_ml.algorithms.job_normalizers.esa_jobtitle_normalizer\n\n\n\nNormalize a job title through Explicit Semantic Analysis\n\n\nOriginally written by Kwame Porter Robinson\n\n\nESANormalizer\n\n\n\nESANormalizer(self, onet_source=<class 'skills_ml.datasets.onet_source.OnetToDiskDownloader'>)\n\n\n\n\nNormalize a job title to ONET occupation titles using explicit semantic analysis.\n\n\nUses ONET occupation titles and descriptions.\n\n\nskills_ml.algorithms.jobtitle_cleaner\n\n\n\nClean job titles\n\nskills_ml.algorithms.jobtitle_cleaner.clean\n\n\nClean job titles by utilizing a list of stopwords\n\n\nclean_by_rules\n\n\n\nclean_by_rules(jobtitle)\n\n\n\n\nRemove numbers and normalize spaces\n\n\nArgs:\n    jobtitle (string) A string\n\n\nReturns: (string) the string with numbers removes and spaces normalized\n\n\nclean_by_neg_dic\n\n\n\nclean_by_neg_dic(jobtitle, negative_list, positive_list)\n\n\n\n\nRemove words from the negative dictionary\n\n\nArgs:\n    jobtitle (string) A job title string\n    negative_list (collection) A list of stop words\n    positive_list (collection) A list of positive words to override stop words\n\n\nReturns: (string) The cleaned job title\n\n\naggregate\n\n\n\naggregate(df_jobtitles, groupby_keys)\n\n\n\n\nArgs:\n    df_jobtitles: job titles in pandas DataFrame\n    groupby_keys: a list of keys to be grouped by. should be something like ['title', 'geo']\nReturns:\n    agg_cleaned_jobtitles: a aggregated verison of job title in pandas DataFrame\n\n\nJobTitleStringClean\n\n\n\nJobTitleStringClean(self)\n\n\n\n\nClean job titles by stripping numbers, and removing place/state names (unless they are also ONET jobs)\n\n\nskills_ml.algorithms.occupation_classifiers\n\n\n\nskills_ml.algorithms.occupation_classifiers.classifiers\n\n\n\nSocClassifier\n\n\n\nSocClassifier(self, classifier)\n\n\n\n\nInterface of SOC Code Classifier.\n\n\nKNNDoc2VecClassifier\n\n\n\nKNNDoc2VecClassifier(self, embedding_model, k=1, indexer=None, **kwargs)\n\n\n\n\nNearest neightbors model to classify the jobposting data into soc code.\nIf the indexer is passed, then NearestNeighbors will use approximate nearest\nneighbor approach which is much faster than the built-in knn in gensim.\n\n\nAttributes:\n    embedding_model (:job: \nskills_ml.algorithms.embedding.models.Doc2VecModel\n): Doc2Vec embedding model\n    k (int): number of nearest neighbor. If k = 1, look for the soc code from single nearest neighbor.\n             If k > 1, classify the soc code by the majority vote of nearest k neighbors.\n    indexer (:obj: \ngensim.similarities.index\n): any kind of gensim compatible indexer\n\n\nskills_ml.algorithms.sampling\n\n\n\nGenerate and store samples of datasets\n\nskills_ml.algorithms.sampling.methods\n\n\nGeneric sampling methods\n\nreservoir\n\n\nreservoir(it, k)\n\n\n\n\nReservoir sampling with Random Sort from a job posting iterator\n\n\nRandomly choosing a sample of k items from a streaming iterator. Using random sort to implement the algorithm.\nBasically, it's assigning random number as keys to each item and maintain k items with minimum value for keys,\nwhich equals to assigning a random number to each item as key and sort items using these keys and take top k items.\n\n\nArgs:\n    it (iterator): Job posting iterator to sample from\n    k (int): Sample size\n\n\nReturns:\n    generator: The result sample of k items.\n\n\nreservoir_weighted\n\n\n\nreservoir_weighted(it, k, weights)\n\n\n\n\nWeighted reservoir Sampling from job posting iterator\n\n\nRandomly choosing a sample of k items from a streaming iterator based on the weights.\n\n\nArgs:\n    it (iterator): Job posting iterator to sample from. The format should be (job_posting, label)\n    k (int): Sample size\n    weights (dict): a dictionary that has key-value pairs as label-weighting pairs. It expects every\n                    label in the iterator to be present as a key in the weights dictionary For example,\n                    weights = {'11': 2, '13', 1}. In this case, the label/key is the occupation major\n                    group and the value is the weight you want to sample with.\n\n\nReturns:\n    generator: The result sample of k items from weighted reservori sampling.\n\n\nskills_ml.algorithms.skill_extractors\n\n\n\nExtract skills from text corpora, such as job postings\n\nskills_ml.algorithms.skill_extractors.base\n\n\nBase classes for skill extraction\n\nTrie\n\n\nTrie(self)\n\n\n\n\nRegex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\nThe corresponding Regex should match much faster than a simple Regex union.\n\nSkillExtractor\n\n\nSkillExtractor(self, transform_func:Callable=None)\n\n\n\n\nAbstract class for all skill extractors.\n\n\nAll subclasses must implement candidate_skills.\n\n\nAll subclasses must define properties\n'method' (a short machine readable property)\n'description' (a text description of how the extractor does its work)\n\n\nArgs:\n    transform_func (callable, optional) Function that transforms a structured object into text\n        Defaults to SimpleCorpusCreator's _join, which takes common text fields\n        in common schema job postings and concatenates them together.\n        For non-job postings another transform function may be needed.\n\n\nListBasedSkillExtractor\n\n\n\nListBasedSkillExtractor(self, skill_lookup_path, skill_lookup_name='onet_ksat', skill_lookup_description=None, *args, **kwargs)\n\n\n\n\nExtract skills by comparing with a known lookup/list\n\n\nSubclasses must implement _skills_lookup and _document_skills_in_lookup\n\n\nArgs:\n    skill_lookup_path (string) A path to the skill lookup file\n    skill_lookup_name (string, optional) An identifier for the skill lookup type.\n        Defaults to onet_ksat\n    skill_lookup_description (string, optional) A human-readable description of the skill lookup.\n\n\nskills_ml.algorithms.skill_extractors.exact_match\n\n\n\nUse exact matching with a source list to find skills\n\nExactMatchSkillExtractor\n\n\nExactMatchSkillExtractor(self, skill_lookup_path, skill_lookup_name='onet_ksat', skill_lookup_description=None, *args, **kwargs)\n\n\n\n\nExtract skills from unstructured text\n\n\nOriginally written by Kwame Porter Robinson\n\n\nskills_ml.algorithms.skill_extractors.fuzzy_match\n\n\n\nUse fuzzy matching with a source list to extract skills from a job posting\n\nFuzzyMatchSkillExtractor\n\n\nFuzzyMatchSkillExtractor(self, skill_lookup_path, skill_lookup_name='onet_ksat', skill_lookup_description=None, *args, **kwargs)\n\n\n\n\nExtract skills from unstructured text using fuzzy matching\n\nskills_ml.algorithms.skill_extractors.noun_phrase_ending\n\n\nUse noun phrases with specific endings to extract skills from job postings\n\nsentences_words_pos\n\n\nsentences_words_pos(document)\n\n\n\n\nChops raw text into part-of-speech (POS)-tagged words in sentences\n\n\nArgs:\n    document (string) A document in text format\n\n\nReturns: (list) of sentences, each being a list of word/POS pair\n\n\nExample:\n    sentences_words_pos(\n        '\n Develop and maintain relationship with key members of ' +\n        'ESPN\u2019s Spanish speaking editorial team'\n    )\n    [ # list of sentences\n        [ # list of word/POS pairs\n            ('\n', 'NN'),\n            ('Develop', 'NNP'),\n            ('and', 'CC'),\n            ('maintain', 'VB'),\n            ('relationship', 'NN'),\n            ('with', 'IN'),\n            ('key', 'JJ'),\n            ('members', 'NNS'),\n            ('of', 'IN'),\n            ('ESPN', 'NNP'),\n            ('\u2019', 'NNP'),\n            ('s', 'VBD'),\n            ('Spanish', 'JJ'),\n            ('speaking', 'NN'),\n            ('editorial', 'NN'),\n            ('team', 'NN')\n        ]\n    ]\n\n\nnoun_phrases_in_line_with_context\n\n\n\nnoun_phrases_in_line_with_context(line)\n\n\n\n\nGenerate noun phrases in the given line of text\n\n\nArgs:\n    text (string): A line of raw text\n\n\nYields:\n    tuples, each with two strings:\n        - a noun phrase\n        - the context of the noun phrase (currently defined as the surrounding sentence)\n\n\nis_bulleted\n\n\n\nis_bulleted(string)\n\n\n\n\nWhether or not a given string begins a 'bullet' character\n\n\nA bullet character is understood to indicate list membership.\nDiffereing common bullet characters are checked.\n\n\nArgs:\nstring (string): Any string\n\n\nReturns: (bool) whether or not the string begins with one of the characters\n    in a predefined list of common bullets\n\n\nclean_beginning\n\n\n\nclean_beginning(string)\n\n\n\n\nClean the beginning of a string of common undesired formatting substrings\n\n\nArgs:\nstring (string): Any string\n\n\nReturns: The string with beginning formatting substrings removed\n\n\nNPEndPatternExtractor\n\n\n\nNPEndPatternExtractor(self, endings, stop_phrases, only_bulleted_lines=True, *args, **kwargs)\n\n\n\n\nIdentify noun phrases with certain ending words (e.g 'skills', 'abilities') as skills\n\n\nArgs:\n    endings (list): Single words that should identify the ending of a noun phrase\n        as being a skill\n    stop_phrases (list): Noun phrases that should not be considered skills\n    only_bulleted_lines (bool, default True): Whether or not to only consider lines\n        that look like they are items in a list\n\n\nSkillEndingPatternExtractor\n\n\n\nSkillEndingPatternExtractor(self, *args, **kwargs)\n\n\n\n\nIdentify noun phrases ending with 'skill' or 'skills' as skills\n\nAbilityEndingPatternExtractor\n\n\nAbilityEndingPatternExtractor(self, *args, **kwargs)\n\n\n\n\nIdentify noun phrases ending in 'ability' or 'abilities' as skills\n\nskills_ml.algorithms.skill_extractors.soc_exact\n\n\nSocScopedExactMatchSkillExtractor\n\n\n\nSocScopedExactMatchSkillExtractor(self, skill_lookup_path, skill_lookup_name='onet_ksat', skill_lookup_description=None, *args, **kwargs)\n\n\n\n\nExtract skills from unstructured text,\nbut only return matches that agree with a known taxonomy\n\n\nskills_ml.algorithms.skill_feature_creator\n\n\n\nSequenceFeatureCreator\n\n\n\nSequenceFeatureCreator(self, job_posting_generator, sentence_tokenizer=None, word_tokenizer=None, features=None, embedding_model=None)\n\n\n\n\nSequence Feature Creator helps users to instantiate different\ntypes of feature at once and combine them together into a sentence(sequence) feature array for sequence modeling.\nIt's a generator that outputs a sentence array at a time. A sentence array is composed of word vectors.\n\n\nExample:\n    from skills_ml.algorithms.skill_feature_creator import FeatureCreator\n\n\nfeature_vector_generator = FeatureCreator(job_posting_generator)\nfeature_vector_generator = FeatureCreator(job_posting_generator, features=[\"StructuralFeature\", \"EmbeddingFeature\"])\n\n\n\nArgs:\n    job_posting_generator (generator): job posting generator.\n    sentence_tokenizer (func): sentence tokenization function\n    word_tokenizer (func): word tokenization function\n    features (list): list of feature types ones want to include. If it's None or by default, it includes all the feature types.\n\n\nYield:\n    sentence_array (numpy.array): an array of word vectors represents the words and punctuations in the sentence. The dimension\n                                  is (# of words)*(dimension of the concat word vector)\n\n\nStructuralFeature\n\n\n\nStructuralFeature(self, sentence_tokenizer=None, word_tokenizer=None, **kwargs)\n\n\n\n\nSturctural features\n\n\nContextualFeature\n\n\n\nContextualFeature(self, sentence_tokenizer=None, word_tokenizer=None, **kwargs)\n\n\n\n\nContextual features\n\n\nEmbeddingFeature\n\n\n\nEmbeddingFeature(self, sentence_tokenizer=None, word_tokenizer=None, **kwargs)\n\n\n\n\nEmbedding Feature\n\n\nskills_ml.algorithms.skill_feature_creator.contextual_features\n\n\n\nskills_ml.algorithms.skill_feature_creator.posTags\n\n\n\nskills_ml.algorithms.skill_feature_creator.structure_features\n\n\n\nskills_ml.algorithms.string_cleaners\n\n\n\nString cleaning algorithms\n\nskills_ml.algorithms.string_cleaners.nlp\n\n\nString transformations for cleaning",
            "title": "Algorithms"
        },
        {
            "location": "/skills_ml.job_postings/",
            "text": "skills_ml.job_postings.aggregate\n\n\n\nskills_ml.job_postings.aggregate.dataset_transform\n\n\n\nTrack stats of job listing datasets, before and after transformation\ninto the common schema.\n\n\nDatasetStatsCounter\n\n\n\nDatasetStatsCounter(self, dataset_id, quarter)\n\n\n\n\nAccumulate data Dataset ETL statistics for a quarter\nto show presence and absence of different fields,\nand the total count of rows\n\n\nArgs:\n    dataset_id (string) A dataset id\n    quarter (string) The quarter being analyzed\n\n\nDatasetStatsAggregator\n\n\n\nDatasetStatsAggregator(self, dataset_id, s3_conn)\n\n\n\n\nAggregate data Dataset ETL statistics up to the dataset level\n\n\nArgs:\n    dataset_id (string) A dataset id\n    s3_conn (boto.Connection) an s3 connection\n\n\nGlobalStatsAggregator\n\n\n\nGlobalStatsAggregator(self, s3_conn)\n\n\n\n\nAggregate Dataset ETL statistics up to the global level\n\n\nArgs:\n    s3_conn (boto.Connection) an s3 connection\n\n\nskills_ml.job_postings.aggregate.field_values\n\n\n\nTrack field value distribution of common schema job postings\n\nFieldValueCounter\n\n\nFieldValueCounter(self, quarter, field_values)\n\n\n\n\nAccumulate field distribution statistics for common schema job postings\n\n\nArgs:\n    quarter (string) The quarter being analyzed\n    field_values (list) each entry should be either:\n        1. a field key\n        2. a tuple, first value field key, second value function to fetch value or values from document\n\n\nskills_ml.job_postings.aggregate.pandas\n\n\n\nAggregation functions that can be used with pandas dataframes\n\nlisty_n_most_common\n\n\nlisty_n_most_common(*params, **kwparams)\n\n\n\n\nExpects each item to be iterable, each sub-item to be addable\n\nAggregateFunction\n\n\nAggregateFunction(self, returns)\n\n\n\n\nWrap a function with an attribute that indicates the return type name\n\nskills_ml.job_postings.common_schema\n\n\nA variety of common-schema job posting collections.\n\n\nEach class in this module should implement a generator that yields job postings (in the common schema, as a JSON string), and has a 'metadata' attribute so any users of the job postings can inspect meaningful metadata about the postings.\n\n\nJobPostingCollectionFromS3\n\n\n\nJobPostingCollectionFromS3(self, s3_conn, s3_paths, extra_metadata=None)\n\n\n\n\nStream job posting from s3.\n\n\nExpects that each will be stored in JSON format, one job posting per line.\nThe s3_path given will be iterated through as a prefix, so job postings may be\npartitioned under that prefix however you choose.\nIt will look in every file under that prefix.\n\n\nExample:\n\n\nimport json\nfrom airflow.hooks import S3Hook\nfrom skills_ml.job_postings.common_schema import JobPostingGenerator\ns3_conn = S3Hook().get_conn()\njob_postings_generator = JobPostingCollectionFromS3(s3_conn, s3_path='my-bucket/job_postings_common_schema')\nfor job_posting in job_postings_generator:\n    print(job_posting['title'])\n\n\n\n\nAttributes:\n    s3_conn: a boto s3 connection\n    s3_path: path to the job listings. there may be multiple\n\n\nJobPostingCollectionSample\n\n\n\nJobPostingCollectionSample(self, num_records:int=50)\n\n\n\n\nStream a finite number of job postings stored within the library.\n\n\nExample:\n\n\nimport json\n\njob_postings = JobPostingCollectionSample()\nfor job_posting in job_postings:\n    print(json.loads(job_posting)['title'])\n\nMeant to provide a dependency-less example of common schema job postings\nfor introduction to the library\n\nArgs:\n    num_records (int): The maximum number of records to return. Defaults to 50 (all postings available)\n\n<h2 id=\"skills_ml.job_postings.common_schema.generate_job_postings_from_s3\">generate_job_postings_from_s3</h2>\n\n```python\ngenerate_job_postings_from_s3(s3_conn, s3_prefix:str) -> Generator[Dict[str, Any], NoneType, NoneType]\n\n\n\n\nStream all job listings from s3\nArgs:\n    s3_conn: a boto s3 connection\n    s3_prefix: path to the job listings.\n\n\nYields:\n    string in json format representing the next job listing\n        Refer to sample_job_listing.json for example structure\n\n\ngenerate_job_postings_from_s3_multiple_prefixes\n\n\n\ngenerate_job_postings_from_s3_multiple_prefixes(s3_conn, s3_prefixes:str) -> Generator[Dict[str, Any], NoneType, NoneType]\n\n\n\n\nChain the generators of a list of multiple quarters\nArgs:\n    s3_conn: a boto s3 connection\n    s3_prefixes: paths to job listings\n\n\nReturn:\n    a generator that all generators are chained together into\n\n\nbatches_generator\n\n\n\nbatches_generator(iterable, batch_size)\n\n\n\n\nBatch generator\nArgs:\n    iterable: an iterable\n    batch_size: batch size\n\n\nskills_ml.job_postings.computed_properties\n\n\n\nEncapsulates the computation of some piece of data for job postings, to make aggregation\nand tabular datasets easy to produce\n\nJobPostingComputedProperty\n\n\nJobPostingComputedProperty(self, storage, partition_func=None)\n\n\n\n\nBase class for computers of job posting properties.\n\n\nUsing this class, expensive computations can be performed once, stored on S3 per job posting\nin partitions, and reused in different aggregations.\n\n\nThe base class takes care of all of the serialization and partitioning,\nleaving subclasses to implement a function for computing the property of a single posting\nand metadata describing the output of this function.\n\n\nSubclasses must implement:\n    - _compute_func_on_one to produce a callable that takes in a single\n        job posting and returns JSON-serializable output representing the computation target.\n        This function can produce objects that are kept in scope and reused,\n        so properties that require a large object (e.g. a trained classifier) to do their\n        computation work can be downloaded from S3 here without requiring the I/O work\n        to be done over and over. (See .computers.SOCClassifyProperty for illustration)\n    - property_name attribute (string) that is used when saving the computed properties\n    - property_columns attribute (list) of ComputedPropertyColumns that\n        map to the column names output by \n_compute_func_on_one\n\n\nArgs:\n    storage (skills_ml.storage.Store) A storage object in which to store the cached properties.\n    partition_func (callable, optional) A function that takes a job posting and\n        outputs a string that should be used as a partition key. Must be deterministic.\n        Defaults to the 'datePosted' value\n\n\n    The caches will be namespaced by the property name and partition function\n\n\n\nComputedPropertyColumn\n\n\n\nComputedPropertyColumn(self, name, description, compatible_aggregate_function_paths=None)\n\n\n\n\nMetadata about a specific output column of a computed property\n\n\nArgs:\n    name (string) The name of the column\n    description (string) A description of the column and how it was populated.\n    compatible_aggregate_function_paths (dict, optional): If this property is meant to be\n        used in aggregations, map string function paths to descriptions of what the\n        function is computing for this column.\n        All function paths should be compatible with pandas.agg (one argument, an iterable),\n        though multi-argument functions can be used in conjunction with functools.partial\n\n\nskills_ml.job_postings.computed_properties.aggregators\n\n\n\nAggregate job posting computed properties into tabular datasets\n\ndf_for_properties_and_keys\n\n\ndf_for_properties_and_keys(computed_properties, keys)\n\n\n\n\nAssemble a dataframe with the raw data from many computed properties and keys\n\n\nArgs:\n    computed_properties (list of JobPostingComputedProperty)\n    keys (list of strs)\n\n\nReturns: pandas.DataFrame\n\n\nexpand_array_col_to_many_cols\n\n\n\nexpand_array_col_to_many_cols(base_col, func, aggregation)\n\n\n\n\nExpand an array column created as the result of an .aggregate call into many columns\n\n\nArgs:\n    base_col (string) The name of the base column (before .aggregate)\n    func (function) The base function that was aggregated on\n    aggregation (pandas.DataFrame) The post-aggregation dataframe\n\n\nReturns: pandas.DataFrame, minus the array column and plus columns for each array value\n\n\nbase_func\n\n\n\nbase_func(aggregate_function)\n\n\n\n\nDeals with the possibility of functools.partial being applied to a given\nfunction. Allows access to the decorated 'return' attribute whether or not\nit is also a partial function\n\n\nArgs:\n    aggregate_function (callable) Either a raw function or a functools.partial object\n\n\nReturns: callable\n\n\naggregation_for_properties_and_keys\n\n\n\naggregation_for_properties_and_keys(grouping_properties, aggregate_properties, aggregate_functions, keys)\n\n\n\n\nAssemble an aggregation dataframe for given partition keys\n\n\nArgs:\n    grouping_properties (list of JobPostingComputedProperty)\n        Properties to form the primary key of the aggregation\n    aggregate_properties (list of JobPostingComputedProperty)\n        Properties to be aggregated over the primary key\n    aggregate_functions (dict) A lookup of aggregate functions\n        to be applied for each aggregate column\n    keys (list of str) The desired partition keys for the aggregation to cover\n\n\nReturns: pandas.DataFrame indexed on the grouping properties,\n    covering all data from the given keys\n\n\naggregate_properties\n\n\n\naggregate_properties(out_filename, grouping_properties, aggregate_properties, aggregate_functions, storage, aggregation_name)\n\n\n\n\nAggregate computed properties and stores the resulting CSV\n\n\nArgs:\n    out_filename (string) The desired filename (without path) for the .csv\n    grouping_properties (list of JobPostingComputedProperty)\n        Properties to form the primary key of the aggregation\n    aggregate_properties (list of JobPostingComputedProperty)\n        Properties to be aggregated over the primary key\n    aggregate_functions (dict) A lookup of aggregate functions\n        to be applied for each aggregate column\n    aggregations_path (string) The base s3 path to store aggregations\n    aggregation_name (string) The name of this particular aggregation\n\n\nReturns: nothing\n\n\nskills_ml.job_postings.computed_properties.computers\n\n\n\nVarious computers of job posting properties. Each class is generally a generic algorithm (such as skill extraction or occupation classification) paired with enough configuration to run on its own\n\nTitleCleanPhaseOne\n\n\nTitleCleanPhaseOne(self, storage, partition_func=None)\n\n\n\n\nPerform one phase of job title cleaning: lowercase/remove punctuation\n\nTitleCleanPhaseTwo\n\n\nTitleCleanPhaseTwo(self, storage, partition_func=None)\n\n\n\n\nPerform two phases of job title cleaning:\n\n\n\n\nlowercase/remove punctuation\n\n\nRemove geography information\n\n\n\n\nCBSAandStateFromGeocode\n\n\n\nCBSAandStateFromGeocode(self, cache_storage, cache_fname, *args, **kwargs)\n\n\n\n\nProduce a CBSA by geocoding the job's location and matching with a CBSA shapefile\n\n\nArgs:\n    cache_s3_path (string) An s3 path to store geocode cache results\n\n\nSOCClassifyProperty\n\n\n\nSOCClassifyProperty(self, classifier_obj, *args, **kwargs)\n\n\n\n\nClassify the SOC code from a trained classifier\n\n\nArgs:\n    classifier_obj (object, optional) An object to use as a classifier.\n        If not sent one will be downloaded from s3\n\n\nGivenSOC\n\n\n\nGivenSOC(self, storage, partition_func=None)\n\n\n\n\nAssign the SOC code given by the partner\n\nSkillCounts\n\n\nSkillCounts(self, skill_extractor, *args, **kwargs)\n\n\n\n\nAdding top skill counts from a skill extractor\n\n\nArgs: (skills_ml.algorithms.skill_extractors.base.SkillExtractorBase) A skill extractor object\n\n\nPostingIdPresent\n\n\n\nPostingIdPresent(self, storage, partition_func=None)\n\n\n\n\nRecords job posting ids. Used for counting job postings\n\nskills_ml.job_postings.corpora\n\n\nClasses for converting collections of objects (for instance, common schema job postings) into text corpora suitable for use by natural language processing algorithms.\n\n\nskills_ml.job_postings.corpora.basic\n\n\n\nCorpusCreator\n\n\n\nCorpusCreator(self, job_posting_generator=None, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], raw=False)\n\n\n\n\nA base class for objects that convert common schema\njob listings into a corpus in documnet level suitable for use by\nmachine learning algorithms or specific tasks.\n\n\nExample:\n\n\nfrom skills_ml.job_postings.common_schema import JobPostingCollectionSample\nfrom skills_ml.job_postings.corpora.basic import CorpusCreator\n\njob_postings_generator = JobPostingCollectionSample()\n\n# Default will include all the cleaned job postings\ncorpus = CorpusCreator(job_postings_generator)\n\n# For getting a the raw job postings without any cleaning\ncorpus = CorpusCreator(job_postings_generator, raw=True)\n\n\n\n\nAttributes:\njob_posting_generator (generator):  an iterable that generates JSON strings.\n                        Each string is expected to represent a job listing\n                        conforming to the common schema\n                        See sample_job_listing.json for an example of this schema\ndocument_schema_fields (list): an list of schema fields to be included\nraw (bool): a flag whether to return the raw documents or transformed documents\n\n\nYield:\n(dict): a dictinary only with selected fields as keys and corresponding raw/cleaned value\n\n\nSimpleCorpusCreator\n\n\n\nSimpleCorpusCreator(self, job_posting_generator=None, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], raw=False)\n\n\n\n\nAn object that transforms job listing documents by picking\nimportant schema fields and returns them as one large lowercased string\n\n\nDoc2VecGensimCorpusCreator\n\n\n\nDoc2VecGensimCorpusCreator(self, job_posting_generator, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], *args, **kwargs)\n\n\n\n\nCorpus for training Gensim Doc2Vec\nAn object that transforms job listing documents by picking\nimportant schema fields and yields them as one large cleaned array of words\n\n\nExample:\n\n\n\nfrom skills_ml.job_postings.common_schema import JobPostingCollectionSample\nfrom skills_ml.job_postings.corpora.basic import Doc2VecGensimCorpusCreator\n\njob_postings_generator = JobPostingCollectionSample()\n\ncorpus = Doc2VecGensimCorpusCreator(job_postings_generator)\n\nAttributes:\n    job_posting_generator (generator): a job posting generator\n    document_schema_fields (list): an list of schema fields to be included\n\n<h2 id=\"skills_ml.job_postings.corpora.basic.Word2VecGensimCorpusCreator\">Word2VecGensimCorpusCreator</h2>\n\n```python\nWord2VecGensimCorpusCreator(self, job_posting_generator, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], *args, **kwargs)\n\n\n\n\nAn object that transforms job listing documents by picking\nimportant schema fields and yields them as one large cleaned array of words\n\n\nJobCategoryCorpusCreator\n\n\n\nJobCategoryCorpusCreator(self, job_posting_generator=None, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'], raw=False)\n\n\n\n\nAn object that extract the label of each job listing document which could be onet soc code or\noccupationalCategory and yields them as a lowercased string\n\n\nRawCorpusCreator\n\n\n\nRawCorpusCreator(self, job_posting_generator, document_schema_fields=['description', 'experienceRequirements', 'qualifications', 'skills'])\n\n\n\n\nAn object that yields the joined raw string of job posting\n\n\nskills_ml.job_postings.filtering\n\n\n\nFiltering streamed job postings\n\nsoc_major_group_filter\n\n\nsoc_major_group_filter(major_groups:List) -> Callable\n\n\n\n\nReturn a function that checks the ONET Soc Code of a job posting (if it is present) against the configured major groups.\n\n\nJobPostingFilterer\n\n\n\nJobPostingFilterer(self, job_posting_generator:Generator[Dict[str, Any], NoneType, NoneType], filter_funcs:List[Callable])\n\n\n\n\nFilter common schema job postings through a number of filtering functions\n\n\nArgs:\n    job_posting_generator: An iterable of job postings (each in dict form)\n    filter_funcs: A list of filtering functions, each taking in a job posting document (as dict) and returning a boolean instructing whether or not the posting passes the filter\n\n\nskills_ml.job_postings.geography_queriers\n\n\n\nExtracting geographies from job posting datasets\n\njob_posting_search_strings\n\n\njob_posting_search_strings(job_posting)\n\n\n\n\nConvert a job posting to a geocode-ready search string\n\n\nIncludes city and state if present, or just city\n\n\nArgs:\n    job_posting (dict) A job posting in schema.org/JobPosting json form\n\n\nReturns: (string) A geocode-ready search string\n\n\nskills_ml.job_postings.geography_queriers.cbsa\n\n\n\nLook up the CBSA for a job posting from a census crosswalk (job location -> Census Place -> Census UA -> Census CBSA)\n\n\nJobCBSAQuerier\n\n\n\nJobCBSAQuerier(self)\n\n\n\n\nQueries the Core-Based Statistical Area for a job using a census crosswalk\n\n\nFirst looks up a Place or County Subdivision by the job posting's state and city.\nIf it finds a result, it will then take the Urbanized Area for that Place or County Subdivison and find CBSAs associated with it.\n\n\nQueries return all hits, so there may be multiple CBSAs for a given query.\n\n\nskills_ml.job_postings.geography_queriers.cbsa_from_geocode\n\n\n\nLook up the CBSA for a job posting against a precomputed geocoded CBSA lookup\n\nJobCBSAFromGeocodeQuerier\n\n\nJobCBSAFromGeocodeQuerier(self, cbsa_results)\n\n\n\n\nQueries the Core-Based Statistical Area for a job\n\n\nThis object delegates the CBSA-finding algorithm to a passed-in cache.\nIn practice, you can look at the \nskills_ml.algorithms.geocoders.cbsa\n\nmodule for an example of how this can be generated.\n\n\nInstead, this object focuses on the job posting-centric logic necessary,\nsuch as converting the job posting to the form needed to use the cache\nand dealing with differents kinds of cache misses.\n\n\nArgs:\n    cbsa_results (dict) A mapping of geocoding search strings to\n        (CBSA FIPS, CBSA Name) tuples\n\n\nskills_ml.job_postings.raw\n\n\n\nskills_ml.job_postings.raw.usajobs\n\n\n\nImport USAJobs postings into the Open Skills common schema\n\nskills_ml.job_postings.raw.virginia\n\n\nskills_ml.job_postings.sample\n\n\n\nSample job postings\n\nJobSampler\n\n\nJobSampler(self, job_posting_generator, major_group=False, keys=None, weights=None, random_state=None)\n\n\n\n\nJob posting sampler using reservoir sampling methods\n\n\nIt takes a job_posting generator as an input. To sample based on weights, one should sepecify a weight dictionary.\n\n\nAttributes:\n    job_posting_generator (iterator): Job posting iterator to sample from.\n    major_group (bool): A flag for using major_group as a label or not\n    keys (list|str): a key or keys(for nested dictionary) indicates the label which should exist in common schema\n                     of job posting.\n    weights (dict): a dictionary that has key-value pairs as label-weighting pairs. It expects every\n                    label in the iterator to be present as a key in the weights dictionary For example,\n                    weights = {'11': 2, '13', 1}. In this case, the label/key is the occupation major\n                    group and the value is the weight you want to sample with.\n    random_state (int): the seed used by the random number generator",
            "title": "Job Posting Dataset Processors"
        },
        {
            "location": "/skills_ml.datasets/",
            "text": "skills_ml.datasets.cbsa_shapefile\n\n\n\nUse the Census CBSA shapefile\n\ndownload_shapefile\n\n\ndownload_shapefile(cache_dir)\n\n\n\n\nDownload Tiger 2015 CBSA Shapefile\n\n\nDownloads the zip archive and unzips the contents\n\n\nArgs:\n    cache_dir (string) local path to download files to\n\n\nReturns: (string) Path to the extracted shapefile\n\n\nskills_ml.datasets.cousub_ua\n\n\n\nRetrieve County Subdivision->Urbanized Area crosswalk\n\ncousub_ua\n\n\ncousub_ua(city_cleaner)\n\n\n\n\nConstruct a County Subdivision->UA Lookup table from Census data\nReturns: dict\n{ StateCode: { CountySubdivisionName: UA Code } }\n\n\nskills_ml.datasets.job_titles\n\n\n\nProcess lists of job titles into a common format\n\nskills_ml.datasets.job_titles.elasticsearch\n\n\nIndex job title/occupation pairs in Elasticsearch.\n\n\nJobTitlesMasterIndexer\n\n\n\nJobTitlesMasterIndexer(self, job_title_generator, alias_name, **kwargs)\n\n\n\n\nArgs: job_title_generator (iterable). Each record is expected to be a dict\n    with keys\n    'Title' for the job title and\n    'Original Title' for the occupation\n\n\nskills_ml.datasets.job_titles.onet\n\n\n\nProcess ONET job titles into a common format\n\nOnet_Title\n\n\nOnet_Title(self, onet_cache)\n\n\n\n\nAn object representing job title data from different ONET files\n\n\nOriginally written by Kwame Porter Robinson\n\n\nOnetTitleExtractor\n\n\n\nOnetTitleExtractor(self, output_filename, onet_source, hash_function)\n\n\n\n\nAn object that creates a job titles CSV based on ONET data\n\n\nskills_ml.datasets.nber_county_cbsa\n\n\n\nRetrieve county->CBSA crosswalk file from the NBER\n\ncbsa_lookup\n\n\ncbsa_lookup()\n\n\n\n\nConstruct a County->CBSA Lookup table from NBER data\nReturns: dict\n    each key is a (State Code, County FIPS code) tuple\n    each value is a (CBSA FIPS code, CBSA Name) tuple\n\n\nskills_ml.datasets.negative_positive_dict\n\n\n\nnegative_positive_dict\n\n\n\nnegative_positive_dict()\n\n\n\n\nConstruct a dictionary of terms that are considered not to be in job title, including\nstates, states abv, cities\nReturns: dictionary of set\n\n\nskills_ml.datasets.onet_cache\n\n\n\nOnetCache\n\n\n\nOnetCache(self, s3_conn, s3_path, cache_dir)\n\n\n\n\nAn object that downloads and caches ONET files from S3\n\n\nOnetSiteCache\n\n\n\nOnetSiteCache(self, storage=None)\n\n\n\n\nAn object that downloads files from the ONET site\n\n\nskills_ml.datasets.onet_source\n\n\n\nDownload ONET files from their site\n\nOnetToMemoryDownloader\n\n\nOnetToMemoryDownloader(self, /, *args, **kwargs)\n\n\n\n\nDownloads newest version of ONET as of time of writing and returns it as text\n\nskills_ml.datasets.partner_updaters\n\n\nUpdate raw job postings from external partners\n\nskills_ml.datasets.partner_updaters.usa_jobs\n\n\nUpdate raw job postings from the USAJobs API\n\nskills_ml.datasets.place_ua\n\n\nRetrieve Census Place->Urbanized Area crosswalk\n\nplace_ua\n\n\nplace_ua(city_cleaner)\n\n\n\n\nConstruct a Place->UA Lookup table from Census data\nReturns: dict\n{ StateCode: { PlaceName: UA Code } }\n\n\nskills_ml.datasets.sba_city_county\n\n\n\nRetrieve county lookup tables from the SBA for each state\n\ncounty_lookup\n\n\ncounty_lookup()\n\n\n\n\nRetrieve county lookup tables if they are not already cached\n\n\nReturns: (dict) each key is a state, each value is a dict {city_name: (fips_county_code, county_name)}\n\n\nskills_ml.datasets.skill_importances\n\n\n\nProcess lists of occupation skill importances into a common format\n\nskills_ml.datasets.skill_importances.onet\n\n\nProcess ONET data to create a dataset with occupations and their skill importances\n\nOnetSkillImportanceExtractor\n\n\nOnetSkillImportanceExtractor(self, storage, output_dataset_name, hash_function=None)\n\n\n\n\nAn object that creates a skills importance CSV based on ONET data\n\n\nOriginally written by Kwame Porter Robinson\n\n\nskills_ml.datasets.skills\n\n\n\nProcess lists of skills into a common format\n\nskills_ml.datasets.skills.ceasn_from_onet\n\n\nskills_ml.datasets.skills.onet_ksat\n\n\n\nProcess ONET skill lists of various types into a common format\n\nOnetSkillListProcessor\n\n\nOnetSkillListProcessor(self, onet_source, output_filename, hash_function, ksa_types=None)\n\n\n\n\nAn object that creates a skills CSV based on ONET data\n\n\nOriginally written by Kwame Porter Robinson\n\n\nskills_ml.datasets.ua_cbsa\n\n\n\nRetrieve\n Urbanized Area->CBSA crosswalk\n\nua_cbsa\n\n\nua_cbsa()\n\n\n\n\nConstruct a UA->CBSA Lookup table from Census data\nReturns: dict\n{ UA Fips: [(CBSA FIPS, CBSA Name)] }",
            "title": "External Dataset Processors"
        }
    ]
}